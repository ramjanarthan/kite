^$^    def _ending_tester(self, value, value_type, expected):$^        result = get_value_range(value, value_type)$^        self.assertequal(result, expected)$^$^    def test_int_str_scalar(self):$^        self._ending_tester('1', int, valuerange(1, 1, 0))$^$^    def test_int_str_scalar_radius(self):$^        self._ending_tester('1~3', int, valuerange(1, 1, 3))$^$^    def test_int_str_range(self):$^        self._ending_tester('1:2', int, valuerange(1, 2, 0))$^$^    def test_int_str_range_radius(self):$^        self._ending_tester('1:2~3', int, valuerange(1, 2, 3))$^$^    def test_int_scalar(self):$^        self._ending_tester(1, int, valuerange(1, 1, 0))$^$^    def test_int_2tuple(self):$^        self._ending_tester((1, 2), int, valuerange(1, 2, 0))$^$^    def test_int_3tuple(self):$^        self._ending_tester((1, 2, 3), int, valuerange(1, 2, 3))$^$^    def test_float_str_scalar(self):$^        self._ending_tester('1.0', float, valuerange(1.0, 1.0, 0.0))$^$^    def test_float_str_scalar_radius(self):$^        self._ending_tester('1.0~3.0', float, valuerange(1.0, 1.0, 3.0))$^$^    def test_float_str_range(self):$^        self._ending_tester('1.0:2.0', float, valuerange(1.0, 2.0, 0.0))$^$^    def test_float_str_range_radius(self):$^        self._ending_tester('1.0:2.0~3.0', float, valuerange(1.0, 2.0, 3.0))$^$^    def test_float_scalar(self):$^        self._ending_tester(1.0, float, valuerange(1.0, 1.0, 0.0))$^$^    def test_float_2tuple(self):$^        self._ending_tester((1.0, 2.0), float, valuerange(1.0, 2.0, 0.0))$^$^    def test_float_3tuple(self):$^        self._ending_tester((1.0, 2.0, 3.0), float, valuerange(1.0, 2.0, 3.0))$^$^    def test_float_int_3tuple(self):$^        self._ending_tester((1, 2, 3), float, valuerange(1.0, 2.0, 3.0))$^$^$^class testpickvaluefromfixedrange(unittest.testcase):$^    def __init__(self, *args, **kwargs):$^        super(testpickvaluefromfixedrange, self).__init__(*args, **kwargs)$^        self.session = tf.session()$^        self.clock_ph = tf.placeholder(dtype=tf.float64, name='clock')$^$^    def _ending_tester(self, value_range, clock, expected):$^        with tf.session() as session:$^            tf_pick = tf_pick_value_from_range(value_range, clock=self.clock_ph)$^$^            def run_pick(_, c):$^                return session.run(tf_pick, feed_dict={self.clock_ph: c})$^$^            is_int = isinstance(value_range.start, int)$^            for pick, int_type, float_type in [(pick_value_from_range, int, float), (run_pick, np.int32, np.float32)]:$^                result = pick(value_range, clock)$^                self.assertequal(result, expected)$^                self.asserttrue(isinstance(result, int_type if is_int else float_type))$^$^    def test_int_0(self):$^        self._ending_tester(valuerange(1, 3, 0), 0.0, 1)$^$^    def test_int_half(self):$^        self._ending_tester(valuerange(1, 3, 0), 0.5, 2)$^$^    def test_int_1(self):$^        self._ending_tester(valuerange(1, 3, 0), 1.0, 3)$^$^    def test_float_0(self):$^        self._ending_tester(valuerange(1.0, 2.0, 0.0), 0.0, 1.0)$^$^    def test_float_half(self):$^        self._ending_tester(valuerange(1.0, 2.0, 0.0), 0.5, 1.5)$^$^    def test_float_1(self):$^        self._ending_tester(valuerange(1.0, 2.0, 0.0), 1.0, 2.0)$^$^$^class testpickvaluefromrandomizedrange(unittest.testcase):$^    def __init__(self, *args, **kwargs):$^        super(testpickvaluefromrandomizedrange, self).__init__(*args, **kwargs)$^        self.session = tf.session()$^        self.clock_ph = tf.placeholder(dtype=tf.float64, name='clock')$^$^    def _ending_tester(self, value_range, clock_min, clock_max, expected_min, expected_max):$^        with self.session as session:$^            tf_pick = tf_pick_value_from_range(value_range, clock=self.clock_ph)$^$^            def run_pick(_, c):$^                return session.run(tf_pick, feed_dict={self.clock_ph: c})$^$^            is_int = isinstance(value_range.start, int)$^            clock_range = np.arange(clock_min, clock_max, (clock_max - clock_min) / 100.0)$^            for pick, int_type, float_type in [(pick_value_from_range, int, float), (run_pick, np.int32, np.float32)]:$^                results = [pick(value_range, c) for c in clock_range]$^                self.assertgreater(len(set(results)), 80)$^                self.asserttrue(all(map(lambda x: expected_min <= x <= expected_max, results)))$^                self.asserttrue(all(map(lambda x: isinstance(x, int_type if is_int else float_type), results)))$^$^    def test_int_0(self):$^        self._ending_tester(valuerange(10000, 30000, 10000), 0.0, 0.1, 0, 22000)$^$^    def test_int_half(self):$^        self._ending_tester(valuerange(10000, 30000, 10000), 0.4, 0.6, 8000, 32000)$^$^    def test_int_1(self):$^        self._ending_tester(valuerange(10000, 30000, 10000), 0.8, 1.0, 16000, 40000)$^$^    def test_float_0(self):$^        self._ending_tester(valuerange(10000.0, 30000.0, 10000.0), 0.0, 0.1, 0.0, 22000.0)$^$^    def test_float_half(self):$^        self._ending_tester(valuerange(10000.0, 30000.0, 10000.0), 0.4, 0.6, 8000.0, 32000.0)$^$^    def test_float_1(self):$^        self._ending_tester(valuerange(10000.0, 30000.0, 10000.0), 0.8, 1.0, 16000.0, 40000.0)$^$^$^if __name__ == '__main__':$^    unittest.main()$^#!/usr/bin/env python$^# -*- coding: utf-8 -*-$^from __future__ import absolute_import, print_function$^$^import absl.app$^import optuna$^import sys$^import tensorflow.compat.v1 as tfv1$^$^from deepspeech_training.evaluate import evaluate$^from deepspeech_training.train import create_model$^from deepspeech_training.util.config import config, initialize_globals$^from deepspeech_training.util.flags import create_flags, flags$^from deepspeech_training.util.logging import log_error$^from deepspeech_training.util.evaluate_tools import wer_cer_batch$^from ds_ctcdecoder import scorer$^$^$^def character_based():$^    is_character_based = false$^    if flags.scorer_path:$^        scorer = scorer(flags.lm_alpha, flags.lm_beta, flags.scorer_path, config.alphabet)$^        is_character_based = scorer.is_utf8_mode()$^    return is_character_based$^$^def objective(trial):$^    flags.lm_alpha = trial.suggest_uniform('lm_alpha', 0, flags.lm_alpha_max)$^    flags.lm_beta = trial.suggest_uniform('lm_beta', 0, flags.lm_beta_max)$^$^    is_character_based = trial.study.user_attrs['is_character_based']$^$^    samples = []$^    for step, test_file in enumerate(flags.test_files.split(',')):$^        tfv1.reset_default_graph()$^$^        current_samples = evaluate([test_file], create_model)$^        samples += current_samples$^$^        # report intermediate objective value.$^        wer, cer = wer_cer_batch(current_samples)$^        trial.report(cer if is_character_based else wer, step)$^$^        # handle pruning based on the intermediate value.$^        if trial.should_prune():$^            raise optuna.exceptions.trialpruned()$^$^    wer, cer = wer_cer_batch(samples)$^    return cer if is_character_based else wer$^$^def main(_):$^    initialize_globals()$^$^    if not flags.test_files:$^        log_error('you need to specify what files to use for evaluation via '$^                  'the --test_files flag.')$^        sys.exit(1)$^$^    is_character_based = character_based()$^$^    study = optuna.create_study()$^    study.set_user_attr("is_character_based", is_character_based)$^    study.optimize(objective, n_jobs=1, n_trials=flags.n_trials)$^    print('best params: lm_alpha={} and lm_beta={} with wer={}'.format(study.best_params['lm_alpha'],$^                                                                       study.best_params['lm_beta'],$^                                                                       study.best_value))$^$^$^if __name__ == '__main__':$^    create_flags()$^    absl.app.run(main)$^#!/usr/bin/env python$^# -*- coding: utf-8 -*-$^from __future__ import absolute_import, division, print_function$^$^if __name__ == '__main__':$^    try:$^        from deepspeech_training import train as ds_train$^    except importerror:$^        print('training package is not installed. see training documentation.')$^        raise$^$^    ds_train.run_script()$^# -*- coding: utf-8 -*-$^$^from __future__ import absolute_import, print_function, unicode_literals$^$^from glob import glob$^from functools import reduce$^$^import json$^import jsone$^import os$^import sys$^import requests$^import slugid$^import yaml$^import subprocess$^$^import networkx as nx$^$^tasks_root = os.path.join(os.path.dirname(os.path.abspath(sys.argv[0])))$^taskcluster_api_baseurl = 'http://taskcluster/queue/v1/task/%(task_id)s'$^$^def string_to_dict(sid, value):$^    parts = sid.split('.')$^$^    def pack(parts):$^        if len(parts) == 1:$^            return {parts[0]: value}$^        elif len(parts):$^            return {parts[0]: pack(parts[1:])}$^        return parts$^$^    return pack(parts)$^$^def merge_dicts(*dicts):$^    if not reduce(lambda x, y: isinstance(y, dict) and x, dicts, true):$^        raise typeerror("object in *dicts not of type dict")$^    if len(dicts) < 2:$^        raise valueerror("requires 2 or more dict objects")$^$^    def merge(a, b):$^        for d in set(a.keys()).union(b.keys()):$^            if d in a and d in b:$^                if type(a[d]) == type(b[d]):$^                    if not isinstance(a[d], dict):$^                        ret = list({a[d], b[d]})$^                        if len(ret) == 1: ret = ret[0]$^                        yield (d, sorted(ret))$^                    else:$^                        yield (d, dict(merge(a[d], b[d])))$^                else:$^                    raise typeerror("conflicting key:value type assignment", type(a[d]), a[d], type(b[d]), b[d])$^            elif d in a:$^                yield (d, a[d])$^            elif d in b:$^                yield (d, b[d])$^            else:$^                raise keyerror$^$^    return reduce(lambda x, y: dict(merge(x, y)), dicts[1:], dicts[0])$^$^def taskcluster_event_context():$^    das_context = {}$^$^    # pre-filterting$^    for k in os.environ.keys():$^        if k == 'github_head_user':$^            os.environ['github_head_user_login'] = os.environ[k]$^            del os.environ['github_head_user']$^$^    for k in os.environ.keys():$^        if k == 'task_id':$^            parts = string_to_dict('taskcluster.taskgroupid', os.environ[k])$^            das_context = merge_dicts(das_context, parts)$^$^        if k.startswith('github_'):$^            parts = string_to_dict(k.lower().replace('_', '.').replace('github', 'event'), os.environ[k])$^            das_context = merge_dicts(das_context, parts)$^$^    return das_context$^$^def load_specific_contextfile(file):$^    specific_context = {}$^$^    try:$^        with open(os.path.join(tasks_root, file)) as src:$^            specific_context = yaml.safe_load(src)$^$^        if specific_context is none:$^            specific_context = {}$^    except filenotfounderror:$^        specific_context = {}$^$^    return specific_context$^$^def defaultvalues_build_context():$^    return load_specific_contextfile('.build.yml')$^$^def shared_context():$^    return load_specific_contextfile('.shared.yml')$^$^def create_task_payload(build, base_context):$^    print('build', build)$^    build_type = os.path.splitext(os.path.basename(build))[0]$^$^    build_context = defaultvalues_build_context()$^    with open(build) as src:$^        build_context['build'].update(yaml.safe_load(src)['build'])$^$^    # be able to use what has been defined in base_context$^    # e.g., the {${event.head.branch}}$^    build_context = jsone.render(build_context, base_context)$^    template_context = {$^        'taskcluster': {$^            'taskid': as_slugid(build_type)$^        },$^        'build_type': build_type$^    }$^$^    with open(os.path.join(tasks_root, build_context['build']['template_file'])) as src:$^        template = yaml.safe_load(src)$^$^    contextes = merge_dicts({}, base_context, template_context, build_context)$^    for one_context in glob(os.path.join(tasks_root, '*.cyml')):$^        with open(one_context) as src:$^            contextes = merge_dicts(contextes, yaml.safe_load(src))$^$^    return jsone.render(template, contextes)$^$^def send_task(t):$^    url = taskcluster_api_baseurl % {'task_id': t['taskid']}$^    del t['taskid']$^$^    r = requests.put(url, json=t)$^$^    print(url, r.status_code)$^    if r.status_code != requests.codes.ok:$^        print(json.dumps(t, indent=2))$^        print(r.content)$^        print(json.loads(r.content.decode())['message'])$^$^    return r.status_code == requests.codes.ok$^$^slugids = {}$^def as_slugid(name):$^    if name not in slugids:$^        slugids[name] = slugid.nice().decode()$^        print('cache miss', name, slugids[name])$^    else:$^        print('cache hit', name, slugids[name])$^    return slugids[name]$^$^def to_int(x):$^    return int(x)$^$^def functions_context():$^    return {$^        'as_slugid': as_slugid,$^        'to_int': to_int$^    }$^$^def is_dry_run():$^    return (len(sys.argv) > 1) and (sys.argv[1] == '--dry')$^$^def should_run():$^    # make a quick clone to fetch the last commit$^    try:$^        subprocess.check_call([$^            'git', 'clone', '--quiet', '-b', os.environ.get('github_head_branch'),$^            '--single-branch', os.environ.get('github_head_repo_url'),$^            '--depth=1', '/tmp/ds-clone/'$^        ], env={'git_lfs_skip_smudge': '1'})$^    except subprocess.calledprocesserror as e:$^        print("error while git cloning:", e, file=sys.stderr)$^        return false$^$^    try:$^        git_msg = subprocess.check_output([$^            'git', '--git-dir=/tmp/ds-clone/.git/',$^            'log', '--format=%b', '-n', '1',$^            os.environ.get('github_head_sha')$^        ]).decode('utf-8').strip().upper()$^    except subprocess.calledprocesserror as e:$^        print("error while git show:", e, file=sys.stderr)$^        return false$^$^    print('commit message:', git_msg)$^$^    x_deepspeech = filter(lambda x: 'x-deepspeech:' in x, git_msg.split('\n'))$^    if len(list(filter(lambda x: 'nobuild' in x, x_deepspeech))) == 1:$^        print('not running anything according to commit message')$^        return false$^$^    return true$^$^if __name__ == '__main__':$^    if not is_dry_run():$^        # we might want to not run in some cases$^        if not should_run():$^            sys.exit(0)$^$^    base_context = taskcluster_event_context()$^    base_context = merge_dicts(base_context, functions_context())$^    base_context = merge_dicts(base_context, shared_context())$^$^    root_task = base_context['taskcluster']['taskgroupid']$^$^    tasks_graph = nx.digraph()$^    tasks = {}$^$^    for build in glob(os.path.join(tasks_root, '*.yml')):$^        t = create_task_payload(build, base_context)$^$^        # we allow template to produce completely empty output$^        if not t:$^            continue$^$^        if 'dependencies' in t and len(t['dependencies']) > 0:$^            for dep in t['dependencies']:$^                tasks_graph.add_edge(t['taskid'], dep)$^        else:$^            tasks_graph.add_edge(t['taskid'], root_task)$^$^        tasks[t['taskid']] = t$^$^    for task in nx.dfs_postorder_nodes(tasks_graph):$^        # root_task is the task group and also the task id that is already$^        # running, so we don't have to schedule that$^        if task == root_task:$^            continue$^$^        t = tasks[task]$^        if is_dry_run():$^            print(json.dumps(t, indent=2))$^            continue$^$^        p = send_task(t)$^        if not p:$^            sys.exit(1)$^import os$^import platform$^import sys$^from pathlib import path$^$^from pkg_resources import parse_version$^from setuptools import find_packages, setup$^$^$^def main():$^    version_file = path(__file__).parent / 'version'$^    with open(str(version_file)) as fin:$^        version = fin.read().strip()$^$^    install_requires_base = [$^        'absl-py',$^        'attrdict',$^        'bs4',$^        'numpy',$^        'optuna',$^        'opuslib == 2.0.0',$^        'pandas',$^        'progressbar2',$^        'pyogg >= 0.6.14a1',$^        'pyxdg',$^        'resampy >= 0.2.2',$^        'requests',$^        'semver',$^        'six',$^        'sox',$^        'soundfile',$^    ]$^$^    decoder_pypi_dep = [$^        'ds_ctcdecoder == {}'.format(version)$^    ]$^$^    tensorflow_pypi_dep = [$^        'tensorflow == 1.15.4'$^    ]$^$^    horovod_pypi_dep = [$^        'horovod[tensorflow] == 0.21.3'$^    ]$^$^    # todo: fixme: this is likely not needed anymore given the way tc and$^    # github actions artifacts differs in how we can download them.$^    """$^    # due to pip craziness environment variables are the only consistent way to$^    # get options into this script when doing `pip install`.$^    ci_decoder_artifacts_root = os.environ.get('decoder_artifacts_root', '')$^    if ci_decoder_artifacts_root:$^        # we're running inside the ci environment, override the decoder$^        # package url with the one we just built.$^        decoder_pkg_url = get_ci_decoder_pkg_url(version, ci_decoder_artifacts_root)$^        install_requires = install_requires_base + [decoder_pkg_url]$^    """$^    if os.environ.get('ds_nodecoder', ''):$^        install_requires = install_requires_base$^    else:$^        install_requires = install_requires_base + decoder_pypi_dep$^$^    if os.environ.get('ds_notensorflow', ''):$^        install_requires = install_requires$^    else:$^        install_requires = install_requires + tensorflow_pypi_dep$^$^    if os.environ.get('ds_with_horovod', ''):$^        install_requires = install_requires + horovod_pypi_dep$^    else:$^        install_requires = install_requires$^$^$^    setup($^        name='deepspeech_training',$^        version=version,$^        description='training code for deepspeech',$^        url='https://github.com/mozilla/deepspeech',$^        author='deepspeech authors',$^        license='mpl-2.0',$^        # classifiers help users find your project by categorizing it.$^        #$^        # for a list of valid classifiers, see https://pypi.org/classifiers/$^        classifiers=[$^            'development status :: 3 - alpha',$^            'intended audience :: developers',$^            'topic :: multimedia :: sound/audio :: speech',$^            'license :: osi approved :: mozilla public license 2.0 (mpl 2.0)',$^            'programming language :: python :: 3',$^        ],$^        package_dir={'': 'training'},$^        packages=find_packages(where='training'),$^        python_requires='>=3.5, <4',$^        install_requires=install_requires,$^        # if there are data files included in your packages that need to be$^        # installed, specify them here.$^        package_data={$^            'deepspeech_training': [$^                'version',$^                'graph_version',$^            ],$^        },$^    )$^$^if __name__ == '__main__':$^    main()$^#!/usr/bin/env python3$^import argparse$^import functools$^import pandas$^$^from deepspeech_training.util.helpers import secs_to_hours$^from pathlib import path$^$^$^def read_csvs(csv_files):$^    # relative paths are relative to csv location$^    def absolutify(csv, path):$^        path = path(path)$^        if path.is_absolute():$^            return str(path)$^        return str(csv.parent / path)$^$^    sets = []$^    for csv in csv_files:$^        file = pandas.read_csv(csv, encoding='utf-8', na_filter=false)$^        file['wav_filename'] = file['wav_filename'].apply(functools.partial(absolutify, csv))$^        sets.append(file)$^$^    # concat all sets, drop any extra columns, re-index the final result as 0..n$^    return pandas.concat(sets, join='inner', ignore_index=true)$^$^$^def main():$^    parser = argparse.argumentparser()$^$^    parser.add_argument("-csv", "--csv-files", help="str. filenames as a comma separated list", required=true)$^    parser.add_argument("--sample-rate", type=int, default=16000, required=false, help="audio sample rate")$^    parser.add_argument("--channels", type=int, default=1, required=false, help="audio channels")$^    parser.add_argument("--bits-per-sample", type=int, default=16, required=false, help="audio bits per sample")$^    args = parser.parse_args()$^    in_files = [path(i).absolute() for i in args.csv_files.split(",")]$^$^    csv_dataframe = read_csvs(in_files)$^    total_bytes = csv_dataframe['wav_filesize'].sum()$^    total_files = len(csv_dataframe)$^    total_seconds = ((csv_dataframe['wav_filesize'] - 44) / args.sample_rate / args.channels / (args.bits_per_sample // 8)).sum()$^$^    print('total bytes:', total_bytes)$^    print('total files:', total_files)$^    print('total time:', secs_to_hours(total_seconds))$^$^if __name__ == '__main__':$^    main()$^#!/usr/bin/env python$^# -*- coding: utf-8 -*-$^from __future__ import absolute_import, division, print_function$^$^import argparse$^import numpy as np$^import wave$^$^from deepspeech import model$^$^$^def main():$^    parser = argparse.argumentparser(description='running deepspeech inference.')$^    parser.add_argument('--model', required=true,$^                        help='path to the model (protocol buffer binary file)')$^    parser.add_argument('--scorer', nargs='?',$^                        help='path to the external scorer file')$^    parser.add_argument('--audio1', required=true,$^                        help='first audio file to use in interleaved streams')$^    parser.add_argument('--audio2', required=true,$^                        help='second audio file to use in interleaved streams')$^    args = parser.parse_args()$^$^    ds = model(args.model)$^$^    if args.scorer:$^        ds.enableexternalscorer(args.scorer)$^$^    fin = wave.open(args.audio1, 'rb')$^    fs1 = fin.getframerate()$^    audio1 = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)$^    fin.close()$^$^    fin = wave.open(args.audio2, 'rb')$^    fs2 = fin.getframerate()$^    audio2 = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)$^    fin.close()$^$^    stream1 = ds.createstream()$^    stream2 = ds.createstream()$^$^    splits1 = np.array_split(audio1, 10)$^    splits2 = np.array_split(audio2, 10)$^$^    for part1, part2 in zip(splits1, splits2):$^        stream1.feedaudiocontent(part1)$^        stream2.feedaudiocontent(part2)$^$^    print(stream1.finishstream())$^    print(stream2.finishstream())$^$^if __name__ == '__main__':$^    main()$^#!/usr/bin/env python$^# -*- coding: utf-8 -*-$^from __future__ import absolute_import, division, print_function$^$^import argparse$^import numpy as np$^import shlex$^import subprocess$^import sys$^import wave$^import json$^$^from deepspeech import model, version$^from timeit import default_timer as timer$^$^try:$^    from shhlex import quote$^except importerror:$^    from pipes import quote$^$^$^def convert_samplerate(audio_path, desired_sample_rate):$^    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate {} --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(quote(audio_path), desired_sample_rate)$^    try:$^        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.pipe)$^    except subprocess.calledprocesserror as e:$^        raise runtimeerror('sox returned non-zero status: {}'.format(e.stderr))$^    except oserror as e:$^        raise oserror(e.errno, 'sox not found, use {}hz files or install it: {}'.format(desired_sample_rate, e.strerror))$^$^    return desired_sample_rate, np.frombuffer(output, np.int16)$^$^$^def metadata_to_string(metadata):$^    return ''.join(token.text for token in metadata.tokens)$^$^$^def words_from_candidate_transcript(metadata):$^    word = ""$^    word_list = []$^    word_start_time = 0$^    # loop through each character$^    for i, token in enumerate(metadata.tokens):$^        # append character to word if it's not a space$^        if token.text != " ":$^            if len(word) == 0:$^                # log the start time of the new word$^                word_start_time = token.start_time$^$^            word = word + token.text$^        # word boundary is either a space or the last character in the array$^        if token.text == " " or i == len(metadata.tokens) - 1:$^            word_duration = token.start_time - word_start_time$^$^            if word_duration < 0:$^                word_duration = 0$^$^            each_word = dict()$^            each_word["word"] = word$^            each_word["start_time"] = round(word_start_time, 4)$^            each_word["duration"] = round(word_duration, 4)$^$^            word_list.append(each_word)$^            # reset$^            word = ""$^            word_start_time = 0$^$^    return word_list$^$^$^def metadata_json_output(metadata):$^    json_result = dict()$^    json_result["transcripts"] = [{$^        "confidence": transcript.confidence,$^        "words": words_from_candidate_transcript(transcript),$^    } for transcript in metadata.transcripts]$^    return json.dumps(json_result, indent=2)$^$^$^$^class versionaction(argparse.action):$^    def __init__(self, *args, **kwargs):$^        super(versionaction, self).__init__(nargs=0, *args, **kwargs)$^$^    def __call__(self, *args, **kwargs):$^        print('deepspeech ', version())$^        exit(0)$^$^$^def main():$^    parser = argparse.argumentparser(description='running deepspeech inference.')$^    parser.add_argument('--model', required=true,$^                        help='path to the model (protocol buffer binary file)')$^    parser.add_argument('--scorer', required=false,$^                        help='path to the external scorer file')$^    parser.add_argument('--audio', required=true,$^                        help='path to the audio file to run (wav format)')$^    parser.add_argument('--beam_width', type=int,$^                        help='beam width for the ctc decoder')$^    parser.add_argument('--lm_alpha', type=float,$^                        help='language model weight (lm_alpha). if not specified, use default from the scorer package.')$^    parser.add_argument('--lm_beta', type=float,$^                        help='word insertion bonus (lm_beta). if not specified, use default from the scorer package.')$^    parser.add_argument('--version', action=versionaction,$^                        help='print version and exits')$^    parser.add_argument('--extended', required=false, action='store_true',$^                        help='output string from extended metadata')$^    parser.add_argument('--json', required=false, action='store_true',$^                        help='output json from metadata with timestamp of each word')$^    parser.add_argument('--candidate_transcripts', type=int, default=3,$^                        help='number of candidate transcripts to include in json output')$^    parser.add_argument('--hot_words', type=str,$^                        help='hot-words and their boosts.')$^    args = parser.parse_args()$^$^    print('loading model from file {}'.format(args.model), file=sys.stderr)$^    model_load_start = timer()$^    # sphinx-doc: python_ref_model_start$^    ds = model(args.model)$^    # sphinx-doc: python_ref_model_stop$^    model_load_end = timer() - model_load_start$^    print('loaded model in {:.3}s.'.format(model_load_end), file=sys.stderr)$^$^    if args.beam_width:$^        ds.setbeamwidth(args.beam_width)$^$^    desired_sample_rate = ds.samplerate()$^$^    if args.scorer:$^        print('loading scorer from files {}'.format(args.scorer), file=sys.stderr)$^        scorer_load_start = timer()$^        ds.enableexternalscorer(args.scorer)$^        scorer_load_end = timer() - scorer_load_start$^        print('loaded scorer in {:.3}s.'.format(scorer_load_end), file=sys.stderr)$^$^        if args.lm_alpha and args.lm_beta:$^            ds.setscoreralphabeta(args.lm_alpha, args.lm_beta)$^$^    if args.hot_words:$^        print('adding hot-words', file=sys.stderr)$^        for word_boost in args.hot_words.split(','):$^            word,boost = word_boost.split(':')$^            ds.addhotword(word,float(boost))$^$^    fin = wave.open(args.audio, 'rb')$^    fs_orig = fin.getframerate()$^    if fs_orig != desired_sample_rate:$^        print('warning: original sample rate ({}) is different than {}hz. resampling might produce erratic speech recognition.'.format(fs_orig, desired_sample_rate), file=sys.stderr)$^        fs_new, audio = convert_samplerate(args.audio, desired_sample_rate)$^    else:$^        audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)$^$^    audio_length = fin.getnframes() * (1/fs_orig)$^    fin.close()$^$^    print('running inference.', file=sys.stderr)$^    inference_start = timer()$^    # sphinx-doc: python_ref_inference_start$^    if args.extended:$^        print(metadata_to_string(ds.sttwithmetadata(audio, 1).transcripts[0]))$^    elif args.json:$^        print(metadata_json_output(ds.sttwithmetadata(audio, args.candidate_transcripts)))$^    else:$^        print(ds.stt(audio))$^    # sphinx-doc: python_ref_inference_stop$^    inference_end = timer() - inference_start$^    print('inference took %0.3fs for %0.3fs audio file.' % (inference_end, audio_length), file=sys.stderr)$^$^if __name__ == '__main__':$^    main()$^import os$^import platform$^$^#the api is not snake case which triggers linter errors$^#pylint: disable=invalid-name$^$^if platform.system().lower() == "windows":$^    dslib_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'lib')$^$^    # on windows, we can't rely on rpath being set to $origin/lib/ or on$^    # @loader_path/lib$^    if hasattr(os, 'add_dll_directory'):$^        # starting with python 3.8 this properly handles the problem$^        os.add_dll_directory(dslib_path)$^    else:$^        # before pythin 3.8 we need to change the path to include the proper$^        # directory for the dynamic linker$^        os.environ['path'] = dslib_path + ';' + os.environ['path']$^$^import deepspeech$^$^# rename for backwards compatibility$^from deepspeech.impl import version as version$^$^class model(object):$^    """$^    class holding a deepspeech model$^$^    :param amodelpath: path to model file to load$^    :type amodelpath: str$^    """$^    def __init__(self, model_path):$^        # make sure the attribute is there if createmodel fails$^        self._impl = none$^$^        status, impl = deepspeech.impl.createmodel(model_path)$^        if status != 0:$^            raise runtimeerror("createmodel failed with '{}' (0x{:x})".format(deepspeech.impl.errorcodetoerrormessage(status),status))$^        self._impl = impl$^$^    def __del__(self):$^        if self._impl:$^            deepspeech.impl.freemodel(self._impl)$^            self._impl = none$^$^    def beamwidth(self):$^        """$^        get beam width value used by the model. if setmodelbeamwidth was not$^        called before, will return the default value loaded from the model file.$^$^        :return: beam width value used by the model.$^        :type: int$^        """$^        return deepspeech.impl.getmodelbeamwidth(self._impl)$^$^    def setbeamwidth(self, beam_width):$^        """$^        set beam width value used by the model.$^$^        :param beam_width: the beam width used by the model. a larger beam width value generates better results at the cost of decoding time.$^        :type beam_width: int$^$^        :return: zero on success, non-zero on failure.$^        :type: int$^        """$^        return deepspeech.impl.setmodelbeamwidth(self._impl, beam_width)$^$^    def samplerate(self):$^        """$^        return the sample rate expected by the model.$^$^        :return: sample rate.$^        :type: int$^        """$^        return deepspeech.impl.getmodelsamplerate(self._impl)$^$^    def enableexternalscorer(self, scorer_path):$^        """$^        enable decoding using an external scorer.$^$^        :param scorer_path: the path to the external scorer file.$^        :type scorer_path: str$^$^        :throws: runtimeerror on error$^        """$^        status = deepspeech.impl.enableexternalscorer(self._impl, scorer_path)$^        if status != 0:$^            raise runtimeerror("enableexternalscorer failed with '{}' (0x{:x})".format(deepspeech.impl.errorcodetoerrormessage(status),status))$^$^    def disableexternalscorer(self):$^        """$^        disable decoding using an external scorer.$^$^        :return: zero on success, non-zero on failure.$^        """$^        return deepspeech.impl.disableexternalscorer(self._impl)$^$^    def addhotword(self, word, boost):$^        """$^        add a word and its boost for decoding.$^        $^        words that don't occur in the scorer (e.g. proper nouns) or strings that contain spaces won't be taken into account.$^$^        :param word: the hot-word$^        :type word: str$^$^        :param boost: positive boost value increases and negative reduces chance of a word occuring in a transcription. excessive positive boost might lead to splitting up of letters of the word following the hot-word.$^        :type boost: float$^$^        :throws: runtimeerror on error$^        """$^        status = deepspeech.impl.addhotword(self._impl, word, boost)$^        if status != 0:$^            raise runtimeerror("addhotword failed with '{}' (0x{:x})".format(deepspeech.impl.errorcodetoerrormessage(status),status))$^$^    def erasehotword(self, word):$^        """$^        remove entry for word from hot-words dict.$^$^        :param word: the hot-word$^        :type word: str$^$^        :throws: runtimeerror on error$^        """$^        status = deepspeech.impl.erasehotword(self._impl, word)$^        if status != 0:$^            raise runtimeerror("erasehotword failed with '{}' (0x{:x})".format(deepspeech.impl.errorcodetoerrormessage(status),status))$^$^    def clearhotwords(self):$^        """$^        remove all entries from hot-words dict.$^$^        :throws: runtimeerror on error$^        """$^        status = deepspeech.impl.clearhotwords(self._impl)$^        if status != 0:$^            raise runtimeerror("clearhotwords failed with '{}' (0x{:x})".format(deepspeech.impl.errorcodetoerrormessage(status),status))$^$^    def setscoreralphabeta(self, alpha, beta):$^        """$^        set hyperparameters alpha and beta of the external scorer.$^$^        :param alpha: the alpha hyperparameter of the decoder. language model weight.$^        :type alpha: float$^$^        :param beta: the beta hyperparameter of the decoder. word insertion weight.$^        :type beta: float$^$^        :return: zero on success, non-zero on failure.$^        :type: int$^        """$^        return deepspeech.impl.setscoreralphabeta(self._impl, alpha, beta)$^$^    def stt(self, audio_buffer):$^        """$^        use the deepspeech model to perform speech-to-text.$^$^        :param audio_buffer: a 16-bit, mono raw audio signal at the appropriate sample rate (matching what the model was trained on).$^        :type audio_buffer: numpy.int16 array$^$^        :return: the stt result.$^        :type: str$^        """$^        return deepspeech.impl.speechtotext(self._impl, audio_buffer)$^$^    def sttwithmetadata(self, audio_buffer, num_results=1):$^        """$^        use the deepspeech model to perform speech-to-text and return results including metadata.$^$^        :param audio_buffer: a 16-bit, mono raw audio signal at the appropriate sample rate (matching what the model was trained on).$^        :type audio_buffer: numpy.int16 array$^$^        :param num_results: maximum number of candidate transcripts to return. returned list might be smaller than this.$^        :type num_results: int$^$^        :return: metadata object containing multiple candidate transcripts. each transcript has per-token metadata including timing information.$^        :type: :func:`metadata`$^        """$^        return deepspeech.impl.speechtotextwithmetadata(self._impl, audio_buffer, num_results)$^$^    def createstream(self):$^        """$^        create a new streaming inference state. the streaming state returned by$^        this function can then be passed to :func:`feedaudiocontent()` and :func:`finishstream()`.$^$^        :return: stream object representing the newly created stream$^        :type: :func:`stream`$^$^        :throws: runtimeerror on error$^        """$^        status, ctx = deepspeech.impl.createstream(self._impl)$^        if status != 0:$^            raise runtimeerror("createstream failed with '{}' (0x{:x})".format(deepspeech.impl.errorcodetoerrormessage(status),status))$^        return stream(ctx)$^$^$^class stream(object):$^    """$^    class wrapping a deepspeech stream. the constructor cannot be called directly.$^    use :func:`model.createstream()`$^    """$^    def __init__(self, native_stream):$^        self._impl = native_stream$^$^    def __del__(self):$^        if self._impl:$^            self.freestream()$^$^    def feedaudiocontent(self, audio_buffer):$^        """$^        feed audio samples to an ongoing streaming inference.$^$^        :param audio_buffer: a 16-bit, mono raw audio signal at the appropriate sample rate (matching what the model was trained on).$^        :type audio_buffer: numpy.int16 array$^$^        :throws: runtimeerror if the stream object is not valid$^        """$^        if not self._impl:$^            raise runtimeerror("stream object is not valid. trying to feed an already finished stream?")$^        deepspeech.impl.feedaudiocontent(self._impl, audio_buffer)$^$^    def intermediatedecode(self):$^        """$^        compute the intermediate decoding of an ongoing streaming inference.$^$^        :return: the stt intermediate result.$^        :type: str$^$^        :throws: runtimeerror if the stream object is not valid$^        """$^        if not self._impl:$^            raise runtimeerror("stream object is not valid. trying to decode an already finished stream?")$^        return deepspeech.impl.intermediatedecode(self._impl)$^$^    def intermediatedecodewithmetadata(self, num_results=1):$^        """$^        compute the intermediate decoding of an ongoing streaming inference and return results including metadata.$^$^        :param num_results: maximum number of candidate transcripts to return. returned list might be smaller than this.$^        :type num_results: int$^$^        :return: metadata object containing multiple candidate transcripts. each transcript has per-token metadata including timing information.$^        :type: :func:`metadata`$^$^        :throws: runtimeerror if the stream object is not valid$^        """$^        if not self._impl:$^            raise runtimeerror("stream object is not valid. trying to decode an already finished stream?")$^        return deepspeech.impl.intermediatedecodewithmetadata(self._impl, num_results)$^$^    def finishstream(self):$^        """$^        compute the final decoding of an ongoing streaming inference and return$^        the result. signals the end of an ongoing streaming inference. the underlying$^        stream object must not be used after this method is called.$^$^        :return: the stt result.$^        :type: str$^$^        :throws: runtimeerror if the stream object is not valid$^        """$^        if not self._impl:$^            raise runtimeerror("stream object is not valid. trying to finish an already finished stream?")$^        result = deepspeech.impl.finishstream(self._impl)$^        self._impl = none$^        return result$^$^    def finishstreamwithmetadata(self, num_results=1):$^        """$^        compute the final decoding of an ongoing streaming inference and return$^        results including metadata. signals the end of an ongoing streaming$^        inference. the underlying stream object must not be used after this$^        method is called.$^$^        :param num_results: maximum number of candidate transcripts to return. returned list might be smaller than this.$^        :type num_results: int$^$^        :return: metadata object containing multiple candidate transcripts. each transcript has per-token metadata including timing information.$^        :type: :func:`metadata`$^$^        :throws: runtimeerror if the stream object is not valid$^        """$^        if not self._impl:$^            raise runtimeerror("stream object is not valid. trying to finish an already finished stream?")$^        result = deepspeech.impl.finishstreamwithmetadata(self._impl, num_results)$^        self._impl = none$^        return result$^$^    def freestream(self):$^        """$^        destroy a streaming state without decoding the computed logits. this can$^        be used if you no longer need the result of an ongoing streaming inference.$^$^        :throws: runtimeerror if the stream object is not valid$^        """$^        if not self._impl:$^            raise runtimeerror("stream object is not valid. trying to free an already finished stream?")$^        deepspeech.impl.freestream(self._impl)$^        self._impl = none$^$^$^# this is only for documentation purpose$^# metadata, candidatetranscript and tokenmetadata should be in sync with native_client/deepspeech.h$^class tokenmetadata(object):$^    """$^    stores each individual character, along with its timing information$^    """$^$^    def text(self):$^        """$^        the text for this token$^        """$^$^$^    def timestep(self):$^        """$^        position of the token in units of 20ms$^        """$^$^$^    def start_time(self):$^        """$^        position of the token in seconds$^        """$^$^$^class candidatetranscript(object):$^    """$^    stores the entire ctc output as an array of character metadata objects$^    """$^    def tokens(self):$^        """$^        list of tokens$^$^        :return: a list of :func:`tokenmetadata` elements$^        :type: list$^        """$^$^$^    def confidence(self):$^        """$^        approximated confidence value for this transcription. this is roughly the$^        sum of the acoustic model logit values for each timestep/character that$^        contributed to the creation of this transcription.$^        """$^$^$^class metadata(object):$^    def transcripts(self):$^        """$^        list of candidate transcripts$^$^        :return: a list of :func:`candidatetranscript` objects$^        :type: list$^        """$^#! /usr/bin/env python$^$^from setuptools import setup, extension$^from distutils.command.build import build$^$^import os$^import subprocess$^import sys$^$^def main():$^    try:$^        import numpy$^        try:$^            numpy_include = numpy.get_include()$^        except attributeerror:$^            numpy_include = numpy.get_numpy_include()$^    except importerror:$^        numpy_include = ''$^        assert 'numpy_include' in os.environ$^$^    def read(fname):$^        return open(os.path.join(os.path.dirname(__file__), fname)).read()$^$^    numpy_include = os.getenv('numpy_include', numpy_include)$^    numpy_min_ver = os.getenv('numpy_dep_version', '')$^$^    project_name = 'deepspeech'$^    if '--project_name' in sys.argv:$^        project_name_idx = sys.argv.index('--project_name')$^        project_name = sys.argv[project_name_idx + 1]$^        sys.argv.remove('--project_name')$^        sys.argv.pop(project_name_idx)$^$^    with open('../../training/deepspeech_training/version', 'r') as ver:$^        project_version = ver.read().strip()$^$^    class buildextfirst(build):$^        sub_commands = [('build_ext', build.has_ext_modules),$^                        ('build_py', build.has_pure_modules),$^                        ('build_clib', build.has_c_libraries),$^                        ('build_scripts', build.has_scripts)]$^$^    # properly pass arguments for linking, setuptools will perform some checks$^    def lib_dirs_split(a):$^        if os.name == 'posix':$^            return a.split('-l')[1:]$^$^        if os.name == 'nt':$^            return []$^$^        raise assertionerror('os.name == java not expected')$^$^    def libs_split(a):$^        if os.name == 'posix':$^            return a.split('-l')[1:]$^$^        if os.name == 'nt':$^            return a.split('.lib')[0:1]$^$^        raise assertionerror('os.name == java not expected')$^$^    ds_ext = extension(name='deepspeech._impl',$^                       sources=['impl.i'],$^                       include_dirs=[numpy_include, '../'],$^                       library_dirs=list(map(lambda x: x.strip(), lib_dirs_split(os.getenv('model_ldflags', '')))),$^                       libraries=list(map(lambda x: x.strip(), libs_split(os.getenv('model_libs', '')))),$^                       swig_opts=['-c++', '-keyword'])$^$^    setup(name=project_name,$^          description='a library for running inference on a deepspeech model',$^          long_description=read('readme.rst'),$^          long_description_content_type='text/x-rst; charset=utf-8',$^          author='mozilla',$^          version=project_version,$^          package_dir={'deepspeech': '.'},$^          cmdclass={'build': buildextfirst},$^          license='mpl-2.0',$^          url='https://github.com/mozilla/deepspeech',$^          project_urls={$