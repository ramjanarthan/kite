^usrbinenv python$^$^broadly speaking this script takes the audio downloaded from common voice$^for a certain language in addition to the .tsv files output by corporacreator$^and the script formats the data and transcripts to be in a state usable by$^deepspeech.py$^use python0 importcv0.py h for help$^$^import csv$^import os$^import subprocess$^import unicodedata$^from multiprocessing import pool$^$^import progressbar$^import sox$^$^from deepspeechtraining.util.downloader import simplebar$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    getimportersparser$^    getvalidatelabel$^    printimportreport$^$^from dsctcdecoder import alphabet$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^channels  0$^maxsecs  00$^params  none$^filterobj  none$^$^$^class labelfilter$^    def initself normalize alphabet validatefun$^        self.normalize  normalize$^        self.alphabet  alphabet$^        self.validatefun  validatefun$^$^    def filterself label$^        if self.normalize$^            label  unicodedata.normalizenfkd label.strip.encodeascii ignore.decodeascii ignore$^        label  self.validatefunlabel$^        if self.alphabet and label and not self.alphabet.canencodelabel$^            label  none$^        return label$^$^$^def initworkerparams$^    global filterobj   pylint disableglobalstatement$^    validatelabel  getvalidatelabelparams$^    alphabet  alphabetparams.filteralphabet if params.filteralphabet else none$^    filterobj  labelfilterparams.normalize alphabet validatelabel$^$^$^def onesamplesample$^     take an audio file and optionally convert it to 00khz wav $^    mp0filename  sample0$^    if not os.path.splitextmp0filename.lower0  .mp0$^        mp0filename  .mp0$^     storing wav files next to the mp0 ones  just with a different suffix$^    wavfilename  os.path.splitextmp0filename0  .wav$^    maybeconvertwavmp0filename wavfilename$^    filesize  0$^    frames  0$^    if os.path.existswavfilename$^        filesize  os.path.getsizewavfilename$^        frames  int$^            subprocess.checkoutput$^                soxi s wavfilename stderrsubprocess.stdout$^            $^        $^    label  filterobj.filtersample0$^    rows  $^    counter  getcounter$^    if filesize  0$^         excluding samples that failed upon conversion$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframes  samplerate  0000  00  0  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif frames  samplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendos.path.splitwavfilename0 filesize label sample0$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^$^    return counter rows$^$^$^def maybeconvertsetdataset tsvdir audiodir filterobj spaceaftereverycharacternone rowsnone excludenone$^    excludetranscripts  set$^    excludespeakers  set$^    if exclude is not none$^        for sample in exclude$^            excludetranscripts.addsample0$^            excludespeakers.addsample0$^$^    if rows is none$^        rows  $^        inputtsv  os.path.joinos.path.abspathtsvdir dataset  .tsv$^        if not os.path.isfileinputtsv$^            return rows$^        printloading tsv file  inputtsv$^         get audiofile path and transcript for each sentence in tsv$^        samples  $^        with openinputtsv encodingutf0 as inputtsvfile$^            reader  csv.dictreaderinputtsvfile delimitert$^            for row in reader$^                samples.appendos.path.joinaudiodir rowpath rowsentence rowclientid$^$^        counter  getcounter$^        numsamples  lensamples$^$^        printimporting mp0 files...$^        pool  poolinitializerinitworker initargsparams$^        bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^        for i processed in enumeratepool.imapunorderedonesample samples start0$^            counter  processed0$^            rows  processed0$^            bar.updatei$^        bar.updatenumsamples$^        pool.close$^        pool.join$^$^        importedsamples  getimportedsamplescounter$^        assert counterall  numsamples$^        assert lenrows  importedsamples$^        printimportreportcounter samplerate maxsecs$^$^    outputcsv  os.path.joinos.path.abspathaudiodir dataset  .csv$^    printsaving new deepspeechformatted csv file to  outputcsv$^    with openoutputcsv w encodingutf0 newline as outputcsvfile$^        printwriting csv file for deepspeech.py as  outputcsv$^        writer  csv.dictwriteroutputcsvfile fieldnamesfieldnames$^        writer.writeheader$^        bar  progressbar.progressbarmaxvaluelenrows widgetssimplebar$^        for filename filesize transcript speaker in barrows$^            if transcript in excludetranscripts or speaker in excludespeakers$^                continue$^            if spaceaftereverycharacter$^                writer.writerow$^                    $^                        wavfilename filename$^                        wavfilesize filesize$^                        transcript  .jointranscript$^                    $^                $^            else$^                writer.writerow$^                    $^                        wavfilename filename$^                        wavfilesize filesize$^                        transcript transcript$^                    $^                $^    return rows$^$^$^def preprocessdatatsvdir audiodir spaceaftereverycharacterfalse$^    exclude  $^    for dataset in test dev train validated other$^        setsamples  maybeconvertsetdataset tsvdir audiodir spaceaftereverycharacter$^        if dataset in test dev$^            exclude  setsamples$^        if dataset  validated$^            maybeconvertsettrainall tsvdir audiodir spaceaftereverycharacter$^                               rowssetsamples excludeexclude$^$^$^def maybeconvertwavmp0filename wavfilename$^    if not os.path.existswavfilename$^        transformer  sox.transformer$^        transformer.convertsampleratesamplerate nchannelschannels$^        try$^            transformer.buildmp0filename wavfilename$^        except sox.core.soxerror$^            pass$^$^$^def parseargs$^    parser  getimportersparserdescriptionimport commonvoice v0.0 corpora$^    parser.addargumenttsvdir helpdirectory containing tsv files$^    parser.addargument$^        audiodir$^        helpdirectory containing the audio clips  defaults to tsvdirclips$^    $^    parser.addargument$^        filteralphabet$^        helpexclude samples with characters not in provided alphabet$^    $^    parser.addargument$^        normalize$^        actionstoretrue$^        helpconverts diacritic characters to their base ones$^    $^    parser.addargument$^        spaceaftereverycharacter$^        actionstoretrue$^        helpto help transcript join by white space$^    $^    return parser.parseargs$^$^$^def main$^    audiodir  params.audiodir if params.audiodir else os.path.joinparams.tsvdir clips$^    preprocessdataparams.tsvdir audiodir params.spaceaftereverycharacter$^$^$^if name  main$^    params  parseargs$^    main$^usrbinenv python$^import codecs$^import fnmatch$^import os$^import random$^import subprocess$^import sys$^import unicodedata$^$^import librosa$^import pandas$^import soundfile    has an external dependency on libsndfile$^$^from deepspeechtraining.util.importers import validatelabeleng as validatelabel$^$^ prerequisite having the sph0pipe tool in your path$^ httpswww.ldc.upenn.edulanguageresourcestoolssphereconversiontools$^$^$^def downloadandpreprocessdatadatadir$^     assume datadir contains extracted ldc0000s00 ldc0000t00 ldc0000s00 ldc0000t00$^$^     conditionally convert fisher sph data to wav$^    maybeconvertwavdatadir ldc0000s00 fisher0000wav$^    maybeconvertwavdatadir ldc0000s00 fisher0000wav$^$^     conditionally split fisher wav data$^    all0000  splitwavandsentences$^        datadir$^        originaldatafisher0000wav$^        converteddatafisher0000splitwav$^        transdataos.path.joinldc0000t00 fe00p0tran data trans$^    $^    all0000  splitwavandsentences$^        datadir$^        originaldatafisher0000wav$^        converteddatafisher0000splitwav$^        transdataos.path.joinldc0000t00 fe00p0tran data trans$^    $^$^     the following files have incorrect transcripts that are much longer than$^     their audio source. the result is that we end up with more labels than time$^     slices which breaks ctc.$^    all0000.loc$^        all0000wavfilename.str.endswithfe000000000.0000.00.wav$^        transcript$^      correct$^    all0000.loc$^        all0000wavfilename.str.endswithfe0000000000.00000.0.wav$^        transcript$^      thats one of those$^    all0000.loc$^        all0000wavfilename.str.endswithfe0000000000.00000.00.wav$^        transcript$^      they dont want$^    all0000.loc$^        all0000wavfilename.str.endswithfe0000000000.00000.00.wav$^        transcript$^      uh my mine yeah the german shepherd pitbull mix he snores almost as loud as i do$^$^     the following file is just a short sound and not at all transcribed like provided.$^     so we just exclude it.$^    all0000  all0000$^        all0000wavfilename.str.endswithfe0000000000.0000.00.wav$^    $^$^     the following file is far too long and would ruin our training batch size.$^     so we just exclude it.$^    all0000  all0000$^        all0000wavfilename.str.endswithfe000000000.00000.00.wav$^    $^$^     the following file is too large for its transcript so we just exclude it.$^    all0000  all0000$^        all0000wavfilename.str.endswithfe0000000000.00000.00.wav$^    $^$^     conditionally split fisher data into trainvalidationtest sets$^    train0000 dev0000 test0000  splitsetsall0000$^    train0000 dev0000 test0000  splitsetsall0000$^$^     join 0000 and 0000 data$^    trainfiles  train0000.appendtrain0000$^    devfiles  dev0000.appenddev0000$^    testfiles  test0000.appendtest0000$^$^     write sets to disk as csv files$^    trainfiles.tocsvos.path.joindatadir fishertrain.csv indexfalse$^    devfiles.tocsvos.path.joindatadir fisherdev.csv indexfalse$^    testfiles.tocsvos.path.joindatadir fishertest.csv indexfalse$^$^$^def maybeconvertwavdatadir originaldata converteddata$^    sourcedir  os.path.joindatadir originaldata$^    targetdir  os.path.joindatadir converteddata$^$^     conditionally convert sph files to wav files$^    if os.path.existstargetdir$^        printskipping maybeconvertwav$^        return$^$^     create targetdir$^    os.makedirstargetdir$^$^     loop over sph files in sourcedir and convert each to 00bit pcm wav$^    for root dirnames filenames in os.walksourcedir$^        for filename in fnmatch.filterfilenames .sph$^            sphfile  os.path.joinroot filename$^            for channel in 0 0$^                wavfilename  $^                    os.path.splitextos.path.basenamesphfile0$^                     c$^                     channel$^                     .wav$^                $^                wavfile  os.path.jointargetdir wavfilename$^                printconverting  to .formatsphfile wavfile$^                subprocess.checkcall$^                    sph0pipe c channel p f rif sphfile wavfile$^                $^$^$^def parsetranscriptionstransfile$^    segments  $^    with codecs.opentransfile r utf0 as fin$^        for line in fin$^            if line.startswith or lenline  0$^                continue$^$^            tokens  line.split$^            starttime  floattokens0$^            stoptime  floattokens0$^            speaker  tokens0$^            transcript   .jointokens0$^$^             we need to do the encodedecode dance here because encode$^             returns a bytes object on python 0 and texttochararray$^             expects a string.$^            transcript  $^                unicodedata.normalizenfkd transcript$^                .encodeascii ignore$^                .decodeascii ignore$^            $^$^            segments.append$^                $^                    starttime starttime$^                    stoptime stoptime$^                    speaker speaker$^                    transcript transcript$^                $^            $^    return segments$^$^$^def splitwavandsentencesdatadir transdata originaldata converteddata$^    transdir  os.path.joindatadir transdata$^    sourcedir  os.path.joindatadir originaldata$^    targetdir  os.path.joindatadir converteddata$^    if not os.path.existstargetdir$^        os.makedirstargetdir$^$^    files  $^$^     loop over transcription files and split corresponding wav$^    for root dirnames filenames in os.walktransdir$^        for filename in fnmatch.filterfilenames .txt$^            transfile  os.path.joinroot filename$^            segments  parsetranscriptionstransfile$^$^             open wav corresponding to transcription file$^            wavfilenames  $^                os.path.splitextos.path.basenametransfile0$^                 c$^                 channel$^                 .wav$^                for channel in 0 0$^            $^            wavfiles  $^                os.path.joinsourcedir wavfilename for wavfilename in wavfilenames$^            $^$^            printsplitting  according to .formatwavfiles transfile$^$^            origaudios  $^                librosa.loadwavfile sr00000 monofalse for wavfile in wavfiles$^            $^$^             loop over segments and split wavfile for each segment$^            for segment in segments$^                 create wav segment filename$^                starttime  segmentstarttime$^                stoptime  segmentstoptime$^                newwavfilename  $^                    os.path.splitextos.path.basenametransfile0$^                     $^                     strstarttime$^                     $^                     strstoptime$^                     .wav$^                $^                newwavfile  os.path.jointargetdir newwavfilename$^$^                channel  0 if segmentspeaker  a else 0$^                splitandresamplewav$^                    origaudioschannel starttime stoptime newwavfile$^                $^$^                newwavfilesize  os.path.getsizenewwavfile$^                transcript  validatelabelsegmenttranscript$^                if transcript  none$^                    files.append$^                        os.path.abspathnewwavfile newwavfilesize transcript$^                    $^$^    return pandas.dataframe$^        datafiles columnswavfilename wavfilesize transcript$^    $^$^$^def splitaudioorigaudio starttime stoptime$^    audiodata framerate  origaudio$^    nchannels  lenaudiodata.shape$^    startindex  intstarttime  framerate$^    stopindex  intstoptime  framerate$^    return $^        audiodatastartindexstopindex$^        if 0  nchannels$^        else audiodata startindexstopindex$^    $^$^$^def splitandresamplewavorigaudio starttime stoptime newwavfile$^    framerate  origaudio0$^    chunkdata  splitaudioorigaudio starttime stoptime$^    soundfile.writenewwavfile chunkdata framerate pcm00$^$^$^def splitsetsfilelist$^    $^    randomply split the datasets into train validation and test sets where the size of the$^    validation and test sets are determined by the getsamplesize function. $^    $^    random.shufflefilelist$^    samplesize  getsamplesizelenfilelist$^$^    trainbeg  0$^    trainend  lenfilelist  0  samplesize$^$^    devbeg  trainend$^    devend  trainend  samplesize$^$^    testbeg  devend$^    testend  lenfilelist$^$^    return $^        filelisttrainbegtrainend$^        filelistdevbegdevend$^        filelisttestbegtestend$^    $^$^$^def getsamplesizepopulationsize$^    calculates the sample size for a 00 confidence and 0 margin of error$^    $^    marginoferror  0.00$^    fractionpicking  0.00$^    zscore  0.00   corresponds to confidence level 00$^    numerator  zscore  0  fractionpicking  0  fractionpicking  $^        marginoferror  0$^    $^    samplesize  0$^    for trainsize in rangepopulationsize 0 0$^        denominator  0  zscore  0  fractionpicking  0  fractionpicking  $^            marginoferror  0  trainsize$^        $^        samplesize  intnumerator  denominator$^        if 0  samplesize  trainsize  populationsize$^            break$^    return samplesize$^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^  coding utf0 $^$^import sys$^$^import tensorflow.compat.v0 as tfv0$^from google.protobuf import textformat$^$^$^def main$^     load and export as string$^    with tfv0.gfile.fastgfilesys.argv0 rb as fin$^        graphdef  tfv0.graphdef$^        graphdef.parsefromstringfin.read$^$^        with tfv0.gfile.fastgfilesys.argv0  txt w as fout$^            fout.writetextformat.messagetostringgraphdef$^$^$^if name  main$^    main$^usrbinenv python$^ vctk used in wavenet paper httpsarxiv.orgpdf0000.00000.pdf$^ licenced under open data commons attribution license odcby v0.0.$^ as per httpshomepages.inf.ed.ac.ukjyamagispage0page00page00.html$^import os$^import random$^import re$^from multiprocessing import pool$^from zipfile import zipfile$^$^import librosa$^import progressbar$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    printimportreport$^$^$^samplerate  00000$^maxsecs  00$^minsecs  0$^archivedirname  vctkcorpus$^archivename  vctkcorpus.zipsequence0isallowedy$^archiveurl  $^    httpsdatashare.is.ed.ac.ukbitstreamhandle000000000  archivename$^$^$^$^def downloadandpreprocessdatatargetdir$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^     conditionally download data$^    archivepath  maybedownloadarchivename targetdir archiveurl$^     conditionally extract common voice data$^    maybeextracttargetdir archivedirname archivepath$^     conditionally convert common voice csv files and mp0 data to deepspeech csvs and wav$^    maybeconvertsetstargetdir archivedirname$^$^$^def maybeextracttargetdir extracteddata archivepath$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.path.jointargetdir extracteddata$^    if not os.path.existsextractedpath$^        printfno directory extractedpath  extracting archive...$^        with zipfilearchivepath r as zipobj$^             extract all the contents of zip file in current directory$^            zipobj.extractalltargetdir$^    else$^        printffound directory extractedpath  not extracting it from archive.$^$^$^def maybeconvertsetstargetdir extracteddata$^    extracteddir  os.path.jointargetdir extracteddata wav00$^    txtdir  os.path.jointargetdir extracteddata txt$^$^    directory  os.path.expanduserextracteddir$^    srtd  lensortedos.listdirdirectory$^    allsamples  $^$^    for target in sortedos.listdirdirectory$^        allsamples  maybeprepareset$^            path.joinextracteddir os.path.splittarget0$^        $^$^    numsamples  lenallsamples$^    printfconverting wav files to sampleratehz...$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i  in enumeratepool.imapunorderedonesample allsamples start0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    writecsvextracteddir txtdir targetdir$^$^$^def onesamplesample$^    if isaudiofilesample$^        y sr  librosa.loadsample sr00000$^$^         trim the beginning and ending silence$^        yt index  librosa.effects.trimy   pylint disableunusedvariable$^$^        duration  librosa.getdurationyt sr$^        if duration  maxsecs or duration  minsecs$^            os.removesample$^        else$^            librosa.output.writewavsample yt sr$^$^$^def maybepreparesettargetcsv$^    samples  sortedos.listdirtargetcsv$^    newsamples  $^    for s in samples$^        newsamples.appendos.path.jointargetcsv s$^    samples  newsamples$^    return samples$^$^$^def writecsvextracteddir txtdir targetdir$^    printfwriting csv file$^    dsetabspath  extracteddir$^    dsettxtabspath  txtdir$^$^    audios  makemanifestdsetabspath$^    utterences  loadtxtsdsettxtabspath$^$^    csv  $^$^    for file in audios$^$^        st  os.statfile$^        filesize  st.stsize$^$^         seems to be one wav directory missing from txts  skip it$^        fileparts  file.splitos.sep$^        filesubdir  fileparts0$^        if filesubdir  p000$^            continue$^$^        filename  fileparts0$^        filenamenoext  filename.split.0$^$^        utterence  utterencesfilenamenoext$^        utterenceclean  re.subrazaz   utterence.lower.strip$^$^        csvline  ffilefilesizeutterencecleann$^        csv.appendcsvline$^$^    random.seed0000$^    random.shufflecsv$^$^    traindata  csv00000$^    devdata  csv0000000000$^    testdata  csv00000$^$^    with openos.path.jointargetdir vctkfull.csv w as fd$^        fd.writewavfilenamewavfilesizetranscriptn$^        for i in csv$^            fd.writei$^    with openos.path.jointargetdir vctktrain.csv w as fd$^        fd.writewavfilenamewavfilesizetranscriptn$^        for i in traindata$^            fd.writei$^    with openos.path.jointargetdir vctkdev.csv w as fd$^        fd.writewavfilenamewavfilesizetranscriptn$^        for i in devdata$^            fd.writei$^    with openos.path.jointargetdir vctktest.csv w as fd$^        fd.writewavfilenamewavfilesizetranscriptn$^        for i in testdata$^            fd.writei$^$^    printfwrote lencsv entries$^$^$^def makemanifestdirectory$^    audios  $^    directory  os.path.expanduserdirectory$^    for target in sortedos.listdirdirectory$^        d  os.path.joindirectory target$^        if not os.path.isdird$^            continue$^$^        for root  fnames in sortedos.walkd$^            for fname in fnames$^                newpath  os.path.joinroot fname$^                item  newpath$^                audios.appenditem$^    return audios$^$^$^def loadtxtsdirectory$^    utterences  dict$^    directory  os.path.expanduserdirectory$^    for target in sortedos.listdirdirectory$^        d  os.path.joindirectory target$^        if not os.path.isdird$^            continue$^$^        for root  fnames in sortedos.walkd$^            for fname in fnames$^                if fname.endswith.txt$^                    with openos.path.joinroot fname r as f$^                        fnamenoext  os.path.basenamefname.rsplit. 00$^                        utterencesfnamenoext  f.readline$^    return utterences$^$^$^audioextensions  .wav wav$^$^$^def isaudiofilefilepath$^    return any$^        os.path.basenamefilepath.endswithextension for extension in audioextensions$^    $^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^$^$^    name     ldc timit dataset$^    url      httpscatalog.ldc.upenn.eduldc00s0$^    hours    0$^    type     read  english$^    authors  garofolo john et al.$^    type     ldc membership$^    licence  ldc user agreement$^$^$^import errno$^import fnmatch$^import os$^import subprocess$^import sys$^import tarfile$^from os import path$^$^import pandas as pd$^$^$^def cleanword$^     lc all  strip punctuation which are not required$^    new  word.lower.replace. $^    new  new.replace $^    new  new.replace $^    new  new.replace $^    new  new.replace $^    new  new.replace $^    new  new.replace $^    new  new.replace $^    return new$^$^$^def preprocessdataargs$^$^     assume data is downloaded from ldc  httpscatalog.ldc.upenn.eduldc00s0$^$^     sa sentences are repeated throughout by each speaker therefore can be removed for asr as they will affect wer$^    ignoresasentences  true$^$^    if ignoresasentences$^        printusing recommended ignore sa sentences$^        print$^            ignoring sa sentences 0 x sentences which are repeated by all speakers$^        $^    else$^        printusing unrecommended setting to include sa sentences$^$^    datapath  args$^    target  path.joindatapath timit$^    print$^        checking to see if data has already been extracted in given argument s$^        target$^    $^$^    if not path.isdirtarget$^        print$^            could not find extracted data trying to find timitldc00s0.tgz in $^            datapath$^        $^        filepath  path.joindatapath timitldc00s0.tgz$^        if path.isfilefilepath$^            printfile found extracting$^            tar  tarfile.openfilepath$^            tar.extractalltarget$^            tar.close$^        else$^            printfile should be downloaded from ldc and placed at filepath$^            strerror  file not found$^            raise ioerrorerrno strerror filepath$^$^    else$^         is path therefore continue$^        printfound extracted data in  target$^$^    printpreprocessing data$^     we convert the .wav nist sphere format into msoft .wav$^     creates rif.wav as the new .wav file$^    for root dirnames filenames in os.walktarget$^        for filename in fnmatch.filterfilenames .wav$^            sphfile  os.path.joinroot filename$^            wavfile  os.path.joinroot filename0  rif.wav$^            printconverting  to .formatsphfile wavfile$^            subprocess.checkcallsox sphfile wavfile$^$^    printpreprocessing complete$^    printbuilding csvs$^$^     lists to build csv files$^    trainlistwavs trainlisttrans trainlistsize    $^    testlistwavs testlisttrans testlistsize    $^$^    for root dirnames filenames in os.walktarget$^        for filename in fnmatch.filterfilenames rif.wav$^            fullwav  os.path.joinroot filename$^            wavfilesize  path.getsizefullwav$^$^             need to remove rif.wav 0chars then add .txt$^            transfile  fullwav0  .txt$^            with opentransfile r as f$^                for line in f$^                    split  line.split$^                    start  split0$^                    end  split0$^                    tlist  split0$^                    trans  $^$^                    for t in tlist$^                        trans  trans     cleant$^$^             if ignoresasentences we only want those without sa in the name$^             or$^             if not ignoresasentences we want all to be added$^            if ignoresasentences and not sa in os.path.basenamefullwav or $^                not ignoresasentences$^            $^                if train in fullwav.lower$^                    trainlistwavs.appendfullwav$^                    trainlisttrans.appendtrans$^                    trainlistsize.appendwavfilesize$^                elif test in fullwav.lower$^                    testlistwavs.appendfullwav$^                    testlisttrans.appendtrans$^                    testlistsize.appendwavfilesize$^                else$^                    raise ioerror$^$^    a  $^        wavfilename trainlistwavs$^        wavfilesize trainlistsize$^        transcript trainlisttrans$^    $^$^    c  $^        wavfilename testlistwavs$^        wavfilesize testlistsize$^        transcript testlisttrans$^    $^$^    all  $^        wavfilename trainlistwavs  testlistwavs$^        wavfilesize trainlistsize  testlistsize$^        transcript trainlisttrans  testlisttrans$^    $^$^    dfall  pd.dataframe$^        all columnswavfilename wavfilesize transcript dtypeint$^    $^    dftrain  pd.dataframe$^        a columnswavfilename wavfilesize transcript dtypeint$^    $^    dftest  pd.dataframe$^        c columnswavfilename wavfilesize transcript dtypeint$^    $^$^    dfall.tocsv$^        target  timitall.csv sep headertrue indexfalse encodingascii$^    $^    dftrain.tocsv$^        target  timittrain.csv sep headertrue indexfalse encodingascii$^    $^    dftest.tocsv$^        target  timittest.csv sep headertrue indexfalse encodingascii$^    $^$^$^if name  main$^    preprocessdatasys.argv0$^    printcompleted$^usrbinenv python$^ ensure that you have downloaded the ldc dataset ldc00s00 and tar exists in a folder e.g.$^ .dataswbswb0ldc00s00.tgz$^ from the deepspeech directory run with .binimportswb.py .dataswb$^import codecs$^import fnmatch$^import os$^import random$^import subprocess$^import sys$^import tarfile$^import unicodedata$^import wave$^$^import librosa$^import pandas$^import requests$^import soundfile    has an external dependency on libsndfile$^$^from deepspeechtraining.util.importers import validatelabeleng as validatelabel$^$^ archivename refers to isip alignments from 000000$^archivename  switchboardwordalignments.tar.gz$^archiveurl  httpwww.openslr.orgresources0$^archivedirname  ldc00s00$^ldcdataset  swb0ldc00s00.tgz$^$^$^def downloadfilefolder url$^     httpsstackoverflow.coma00000000000000$^    localfilename  url.split0$^    fullfilename  os.path.joinfolder localfilename$^    r  requests.geturl streamtrue$^    with openfullfilename wb as f$^        for chunk in r.itercontentchunksize0000$^            if chunk   filter out keepalive new chunks$^                f.writechunk$^    return fullfilename$^$^$^def maybedownloadarchiveurl targetdir ldcdataset$^     if archive file does not exist download it...$^    archivepath  os.path.jointargetdir ldcdataset$^    ldcpath  archiveurl  ldcdataset$^    if not os.path.existstargetdir$^        printno path s  creating ...  targetdir$^        os.makedirstargetdir$^$^    if not os.path.existsarchivepath$^        printno archive s  downloading...  archivepath$^        downloadfiletargetdir ldcpath$^    else$^        printfound archive s  not downloading.  archivepath$^    return archivepath$^$^$^def downloadandpreprocessdatadatadir$^    newdatadir  os.path.joindatadir archivedirname$^    targetdir  os.path.abspathnewdatadir$^    archivepath  os.path.abspathos.path.joindatadir ldcdataset$^$^     check swb0ldc00s00.tgz then extract$^    assert os.path.isfilearchivepath$^    extracttargetdir archivepath$^$^     transcripts$^    transcriptspath  maybedownloadarchiveurl targetdir archivename$^    extracttargetdir transcriptspath$^$^     check swb0d0000swbms00transcriptions$^    expectedfolders  $^        swb0d0$^        swb0d0$^        swb0d0$^        swb0d0$^        swbms00transcriptions$^    $^    assert allos.path.isdiros.path.jointargetdir e for e in expectedfolders$^$^     conditionally convert swb sph data to wav$^    maybeconvertwavtargetdir swb0d0 swb0d0wav$^    maybeconvertwavtargetdir swb0d0 swb0d0wav$^    maybeconvertwavtargetdir swb0d0 swb0d0wav$^    maybeconvertwavtargetdir swb0d0 swb0d0wav$^$^     conditionally split wav data$^    d0  maybesplitwavandsentences$^        targetdir swbms00transcriptions swb0d0wav swb0d0splitwav$^    $^    d0  maybesplitwavandsentences$^        targetdir swbms00transcriptions swb0d0wav swb0d0splitwav$^    $^    d0  maybesplitwavandsentences$^        targetdir swbms00transcriptions swb0d0wav swb0d0splitwav$^    $^    d0  maybesplitwavandsentences$^        targetdir swbms00transcriptions swb0d0wav swb0d0splitwav$^    $^$^    swbfiles  d0.appendd0.appendd0.appendd0$^$^    trainfiles devfiles testfiles  splitsetsswbfiles$^$^     write sets to disk as csv files$^    trainfiles.tocsvos.path.jointargetdir swbtrain.csv indexfalse$^    devfiles.tocsvos.path.jointargetdir swbdev.csv indexfalse$^    testfiles.tocsvos.path.jointargetdir swbtest.csv indexfalse$^$^$^def extracttargetdir archivepath$^    with tarfile.openarchivepath as tar$^        tar.extractalltargetdir$^$^$^def maybeconvertwavdatadir originaldata converteddata$^    sourcedir  os.path.joindatadir originaldata$^    targetdir  os.path.joindatadir converteddata$^$^     conditionally convert sph files to wav files$^    if os.path.existstargetdir$^        printskipping maybeconvertwav$^        return$^$^     create targetdir$^    os.makedirstargetdir$^$^     loop over sph files in sourcedir and convert each to 00bit pcm wav$^    for root dirnames filenames in os.walksourcedir$^        for filename in fnmatch.filterfilenames .sph$^            for channel in 0 0$^                sphfile  os.path.joinroot filename$^                wavfilename  $^                    os.path.splitextos.path.basenamesphfile0$^                     $^                     channel$^                     .wav$^                $^                wavfile  os.path.jointargetdir wavfilename$^                tempwavfilename  $^                    os.path.splitextos.path.basenamesphfile0$^                     $^                     channel$^                     temp.wav$^                $^                tempwavfile  os.path.jointargetdir tempwavfilename$^                printconverting  to .formatsphfile tempwavfile$^                subprocess.checkcall$^                    $^                        sph0pipe$^                        c$^                        channel$^                        p$^                        f$^                        rif$^                        sphfile$^                        tempwavfile$^                    $^                $^                printupsampling  to .formattempwavfile wavfile$^                audiodata framerate  librosa.loadtempwavfile sr00000 monotrue$^                soundfile.writewavfile audiodata framerate pcm00$^                os.removetempwavfile$^$^$^def parsetranscriptionstransfile$^    segments  $^    with codecs.opentransfile r utf0 as fin$^        for line in fin$^            if line.startswith or lenline  0$^                continue$^$^            tokens  line.split$^            starttime  floattokens0$^            stoptime  floattokens0$^            transcript  validatelabel .jointokens0$^$^            if transcript  none$^                continue$^$^             we need to do the encodedecode dance here because encode$^             returns a bytes object on python 0 and texttochararray$^             expects a string.$^            transcript  $^                unicodedata.normalizenfkd transcript$^                .encodeascii ignore$^                .decodeascii ignore$^            $^$^            segments.append$^                $^                    starttime starttime$^                    stoptime stoptime$^                    transcript transcript$^                $^            $^    return segments$^$^$^def maybesplitwavandsentencesdatadir transdata originaldata converteddata$^    transdir  os.path.joindatadir transdata$^    sourcedir  os.path.joindatadir originaldata$^    targetdir  os.path.joindatadir converteddata$^    if os.path.existstargetdir$^        printskipping maybesplitwav$^        return$^$^    os.makedirstargetdir$^$^    files  $^$^     loop over transcription files and split corresponding wav$^    for root dirnames filenames in os.walktransdir$^        for filename in fnmatch.filterfilenames .text$^            if trans not in filename$^                continue$^            transfile  os.path.joinroot filename$^            segments  parsetranscriptionstransfile$^$^             open wav corresponding to transcription file$^            channel  0 0$^                os.path.splitextos.path.basenametransfile00  a$^            $^            wavfilename  $^                sw0$^                 os.path.splitextos.path.basenametransfile000$^                 $^                 channel$^                 .wav$^            $^            wavfile  os.path.joinsourcedir wavfilename$^$^            printsplitting  according to .formatwavfile transfile$^$^            if not os.path.existswavfile$^                printskipping. does not exist  wavfile$^                continue$^$^            origaudio  wave.openwavfile r$^$^             loop over segments and split wavfile for each segment$^            for segment in segments$^                 create wav segment filename$^                starttime  segmentstarttime$^                stoptime  segmentstoptime$^                newwavfilename  $^                    os.path.splitextos.path.basenametransfile0$^                     $^                     strstarttime$^                     $^                     strstoptime$^                     .wav$^                $^                if iswavtooshortnewwavfilename$^                    continue$^                newwavfile  os.path.jointargetdir newwavfilename$^$^                splitwavorigaudio starttime stoptime newwavfile$^$^                newwavfilesize  os.path.getsizenewwavfile$^                transcript  segmenttranscript$^                files.append$^                    os.path.abspathnewwavfile newwavfilesize transcript$^                $^$^             close origaudio$^            origaudio.close$^$^    return pandas.dataframe$^        datafiles columnswavfilename wavfilesize transcript$^    $^$^$^def iswavtooshortwavfilename$^    shortwavfilenames  $^        sw0000ams00atrans00.000000.000000.wav$^        sw0000ams00atrans000.00000000.000000.wav$^    $^    return wavfilename in shortwavfilenames$^$^$^def splitwavorigaudio starttime stoptime newwavfile$^    framerate  origaudio.getframerate$^    origaudio.setposintstarttime  framerate$^    chunkdata  origaudio.readframesintstoptime  starttime  framerate$^    chunkaudio  wave.opennewwavfile w$^    chunkaudio.setnchannelsorigaudio.getnchannels$^    chunkaudio.setsampwidthorigaudio.getsampwidth$^    chunkaudio.setframerateframerate$^    chunkaudio.writeframeschunkdata$^    chunkaudio.close$^$^$^def splitsetsfilelist$^    $^    randomply split the datasets into train validation and test sets where the size of the$^    validation and test sets are determined by the getsamplesize function. $^    $^    random.shufflefilelist$^    samplesize  getsamplesizelenfilelist$^$^    trainbeg  0$^    trainend  lenfilelist  0  samplesize$^$^    devbeg  trainend$^    devend  trainend  samplesize$^$^    testbeg  devend$^    testend  lenfilelist$^$^    return $^        filelisttrainbegtrainend$^        filelistdevbegdevend$^        filelisttestbegtestend$^    $^$^$^def getsamplesizepopulationsize$^    calculates the sample size for a 00 confidence and 0 margin of error$^    $^    marginoferror  0.00$^    fractionpicking  0.00$^    zscore  0.00   corresponds to confidence level 00$^    numerator  zscore  0  fractionpicking  0  fractionpicking  $^        marginoferror  0$^    $^    samplesize  0$^    for trainsize in rangepopulationsize 0 0$^        denominator  0  zscore  0  fractionpicking  0  fractionpicking  $^            marginoferror  0  trainsize$^        $^        samplesize  intnumerator  denominator$^        if 0  samplesize  trainsize  populationsize$^            break$^    return samplesize$^$^$^def readdataset$^    filelist$^    threadcount$^    batchsize$^    numcep$^    numcontext$^    stride0$^    offset0$^    nextindexlambda i i  0$^    limit0$^$^     optionally apply dataset size limit$^    if limit  0$^        filelist  filelist.iloclimit$^$^    filelist  filelistoffsetstride$^$^     return dataset$^    return dataset$^        txtfiles threadcount batchsize numcep numcontext nextindexnextindex$^    $^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python0$^import csv$^import os$^import re$^import subprocess$^import zipfile$^from multiprocessing import pool$^$^import progressbar$^import sox$^$^import unidecode$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    getimportersparser$^    getvalidatelabel$^    printimportreport$^$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^maxsecs  00$^archivename  00000000frfr$^archivedirname  ts  archivename$^archiveurl  $^    httpsdeepspeechstoragemirror.s0.frpar.scw.cloud  archivename  .zip$^$^$^$^def downloadandpreprocessdatatargetdir englishcompatiblefalse$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^     conditionally download data$^    archivepath  maybedownload$^        ts  archivename  .zip targetdir archiveurl$^    $^     conditionally extract archive data$^    maybeextracttargetdir archivedirname archivepath$^     conditionally convert trainingspeech data to deepspeech csvs and wav$^    maybeconvertsets$^        targetdir archivedirname englishcompatibleenglishcompatible$^    $^$^$^def maybeextracttargetdir extracteddata archivepath$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.path.jointargetdir extracteddata$^    if not os.path.existsextractedpath$^        printno directory s  extracting archive...  extractedpath$^        if not os.path.isdirextractedpath$^            os.mkdirextractedpath$^        with zipfile.zipfilearchivepath as zipf$^            zipf.extractallextractedpath$^    else$^        printfound directory s  not extracting it from archive.  archivepath$^$^$^def onesamplesample$^     take a audio file and optionally convert it to 00khz wav $^    origfilename  samplepath$^     storing wav files next to the wav ones  just with a different suffix$^    wavfilename  os.path.splitextorigfilename0  .converted.wav$^    maybeconvertwavorigfilename wavfilename$^    filesize  0$^    frames  0$^    if os.path.existswavfilename$^        filesize  os.path.getsizewavfilename$^        frames  int$^            subprocess.checkoutput$^                soxi s wavfilename stderrsubprocess.stdout$^            $^        $^    label  sampletext$^$^    rows  $^$^     keep track of how many samples are good vs. problematic$^    counter  getcounter$^    if filesize  0$^         excluding samples that failed upon conversion$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframes  samplerate  0000  00  0  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif frames  samplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendwavfilename filesize label$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^$^    return counter rows$^$^$^def maybeconvertsetstargetdir extracteddata englishcompatiblefalse$^    extracteddir  os.path.jointargetdir extracteddata$^     override existing csv with normalized one$^    targetcsvtemplate  os.path.jointargetdir ts  archivename  .csv$^    if os.path.isfiletargetcsvtemplate$^        return$^    pathtooriginalcsv  os.path.joinextracteddir data.csv$^    with openpathtooriginalcsv as csvf$^        data  $^            d$^            for d in csv.dictreadercsvf delimiter$^            if floatdduration  maxsecs$^        $^$^    for line in data$^        linepath  os.path.joinextracteddir linepath$^$^    numsamples  lendata$^    rows  $^    counter  getcounter$^$^    printimporting  wav files....formatnumsamples$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i processed in enumeratepool.imapunorderedonesample data start0$^        counter  processed0$^        rows  processed0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    with opentargetcsvtemplate.formattrain w encodingutf0 newline as traincsvfile   00$^        with opentargetcsvtemplate.formatdev w encodingutf0 newline as devcsvfile   00$^            with opentargetcsvtemplate.formattest w encodingutf0 newline as testcsvfile   00$^                trainwriter  csv.dictwritertraincsvfile fieldnamesfieldnames$^                trainwriter.writeheader$^                devwriter  csv.dictwriterdevcsvfile fieldnamesfieldnames$^                devwriter.writeheader$^                testwriter  csv.dictwritertestcsvfile fieldnamesfieldnames$^                testwriter.writeheader$^$^                for i item in enumeraterows$^                    transcript  validatelabel$^                        cleanuptranscript$^                            item0 englishcompatibleenglishcompatible$^                        $^                    $^                    if not transcript$^                        continue$^                    wavfilename  os.path.jointargetdir extracteddata item0$^                    imod  i  00$^                    if imod  0$^                        writer  testwriter$^                    elif imod  0$^                        writer  devwriter$^                    else$^                        writer  trainwriter$^                    writer.writerow$^                        dict$^                            wavfilenamewavfilename$^                            wavfilesizeos.path.getsizewavfilename$^                            transcripttranscript$^                        $^                    $^$^    importedsamples  getimportedsamplescounter$^    assert counterall  numsamples$^    assert lenrows  importedsamples$^$^    printimportreportcounter samplerate maxsecs$^$^$^def maybeconvertwavorigfilename wavfilename$^    if not os.path.existswavfilename$^        transformer  sox.transformer$^        transformer.convertsampleratesamplerate$^        try$^            transformer.buildorigfilename wavfilename$^        except sox.core.soxerror as ex$^            printsox processing error ex origfilename wavfilename$^$^$^punctuationsreg  re.compiler.$^multiplespacesreg  re.compilers0$^$^$^def cleanuptranscripttext englishcompatiblefalse$^    text  text.replace .replaceu00a0  $^    text  punctuationsreg.sub  text$^    text  multiplespacesreg.sub  text$^    if englishcompatible$^        text  unidecode.unidecodetext$^    return text.strip.lower$^$^$^def handleargs$^    parser  getimportersparserdescriptionimporter for trainingspeech dataset.$^    parser.addargumentdesttargetdir$^    parser.addargument$^        englishcompatible$^        actionstoretrue$^        destenglishcompatible$^        helpremove diactrics and other nonascii chars.$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    validatelabel  getvalidatelabelcliargs$^    downloadandpreprocessdatacliargs.targetdir cliargs.englishcompatible$^usrbinenv python$^import os$^import sys$^$^import pandas$^$^from deepspeechtraining.util.downloader import maybedownload$^$^$^def downloadandpreprocessdatadatadir$^     conditionally download data$^    ldc00s0base  ldc00s0$^    ldc00s0baseurl  httpscatalog.ldc.upenn.edudescaddenda$^    localfile  maybedownload$^        ldc00s0base  .wav datadir ldc00s0baseurl  ldc00s0base  .wav$^    $^    transfile  maybedownload$^        ldc00s0base  .txt datadir ldc00s0baseurl  ldc00s0base  .txt$^    $^    with opentransfile r as fin$^        transcript   .joinfin.read.strip.lower.split 0.replace$^            . $^        $^$^    df  pandas.dataframe$^        dataos.path.abspathlocalfile os.path.getsizelocalfile transcript$^        columnswavfilename wavfilesize transcript$^    $^    df.tocsvos.path.joindatadir ldc00s0.csv indexfalse$^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^import csv$^import os$^import sys$^import subprocess$^import tarfile$^from glob import glob$^from multiprocessing import pool$^$^import progressbar$^import sox$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    printimportreport$^$^from deepspeechtraining.util.importers import validatelabeleng as validatelabel$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^maxsecs  00$^archivedirname  cvcorpusv0$^archivename  archivedirname  .tar.gz$^archiveurl  $^    httpss0.useast0.amazonaws.comcommonvoicedatadownload  archivename$^$^$^$^def downloadandpreprocessdatatargetdir$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^     conditionally download data$^    archivepath  maybedownloadarchivename targetdir archiveurl$^     conditionally extract common voice data$^    maybeextracttargetdir archivedirname archivepath$^     conditionally convert common voice csv files and mp0 data to deepspeech csvs and wav$^    maybeconvertsetstargetdir archivedirname$^$^$^def maybeextracttargetdir extracteddata archivepath$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.jointargetdir extracteddata$^    if not os.path.existsextractedpath$^        printno directory s  extracting archive...  extractedpath$^        with tarfile.openarchivepath as tar$^            tar.extractalltargetdir$^    else$^        printfound directory s  not extracting it from archive.  extractedpath$^$^$^def maybeconvertsetstargetdir extracteddata$^    extracteddir  os.path.jointargetdir extracteddata$^    for sourcecsv in globos.path.joinextracteddir .csv$^        maybeconvertset$^            extracteddir$^            sourcecsv$^            os.path.jointargetdir os.path.splitsourcecsv0$^        $^$^$^def onesamplesample$^    mp0filename  sample0$^     storing wav files next to the mp0 ones  just with a different suffix$^    wavfilename  path.splitextmp0filename0  .wav$^    maybeconvertwavmp0filename wavfilename$^    frames  int$^        subprocess.checkoutputsoxi s wavfilename stderrsubprocess.stdout$^    $^    filesize  0$^    if os.path.existswavfilename$^        filesize  path.getsizewavfilename$^        frames  int$^            subprocess.checkoutput$^                soxi s wavfilename stderrsubprocess.stdout$^            $^        $^    label  validatelabelsample0$^    rows  $^    counter  getcounter$^    if filesize  0$^         excluding samples that failed upon conversion$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframes  samplerate  0000  00  0  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif frames  samplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendwavfilename filesize label$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^    return counter rows$^$^$^def maybeconvertsetextracteddir sourcecsv targetcsv$^    print$^    if os.path.existstargetcsv$^        printfound csv file s  not importing s.  targetcsv sourcecsv$^        return$^    printno csv file s  importing s...  targetcsv sourcecsv$^    samples  $^    with opensourcecsv as sourcecsvfile$^        reader  csv.dictreadersourcecsvfile$^        for row in reader$^            samples.appendos.path.joinextracteddir rowfilename rowtext$^$^     mutable counters for the concurrent embedded routine$^    counter  getcounter$^    numsamples  lensamples$^    rows  $^$^    printimporting mp0 files...$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i processed in enumeratepool.imapunorderedonesample samples start0$^        counter  processed0$^        rows  processed0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    printwriting s...  targetcsv$^    with opentargetcsv w encodingutf0 newline as targetcsvfile$^        writer  csv.dictwritertargetcsvfile fieldnamesfieldnames$^        writer.writeheader$^        bar  progressbar.progressbarmaxvaluelenrows widgetssimplebar$^        for filename filesize transcript in barrows$^            writer.writerow$^                $^                    wavfilename filename$^                    wavfilesize filesize$^                    transcript transcript$^                $^            $^$^    importedsamples  getimportedsamplescounter$^    assert counterall  numsamples$^    assert lenrows  importedsamples$^$^    printimportreportcounter samplerate maxsecs$^$^$^def maybeconvertwavmp0filename wavfilename$^    if not os.path.existswavfilename$^        transformer  sox.transformer$^        transformer.convertsampleratesamplerate$^        try$^            transformer.buildmp0filename wavfilename$^        except sox.core.soxerror$^            pass$^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^$^downloads and prepares parts of the spoken wikipedia corpora for deepspeech.py$^use python0 importswc.py h for help$^$^$^import argparse$^import csv$^import os$^import random$^import re$^import shutil$^import sys$^import tarfile$^import unicodedata$^import wave$^import xml.etree.elementtree as et$^from collections import counter$^from glob import glob$^from multiprocessing.pool import threadpool$^$^import progressbar$^import sox$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import validatelabeleng as validatelabel$^from dsctcdecoder import alphabet$^$^swcurl  httpswww0.informatik.unihamburg.denatspubswcswclanguage.tar$^swcarchive  swclanguage.tar$^languages  dutch english german$^fieldnames  wavfilename wavfilesize transcript$^fieldnamesext  fieldnames  article speaker$^channels  0$^samplerate  00000$^unknown  unknown$^audiopattern  audio.ogg$^wavname  audio.wav$^alignedname  aligned.swc$^$^substitutions  $^    german $^        re.compiler dollar$^        re.compiler euro$^        re.compiler pfund$^        $^            re.compilerein tausend s hundert s er $^            r0zehnhundert 0er $^        $^        re.compilerein tausend achtneun hundert r0zehnhundert$^        $^            re.compile$^                reins punkt null null null punkt null null null punkt null null null$^            $^            eine milliarde$^        $^        $^            re.compile$^                rpunkt null null null punkt null null null punkt null null null$^            $^            milliarden$^        $^        re.compilereins punkt null null null punkt null null null eine million$^        re.compilerpunkt null null null punkt null null null millionen$^        re.compilereins punkt null null null ein tausend$^        re.compilerpunkt null null null tausend$^        re.compilerpunkt null none$^    $^$^$^dontnormalize  german $^$^prefilter  str.maketransdict.fromkeys$^$^$^class sample$^    def initself wavpath start end text article speaker subsetnone$^        self.wavpath  wavpath$^        self.start  start$^        self.end  end$^        self.text  text$^        self.article  article$^        self.speaker  speaker$^        self.subset  subset$^$^$^def failmessage$^    printmessage$^    sys.exit0$^$^$^def grouplst getkey$^    groups  $^    for obj in lst$^        key  getkeyobj$^        if key in groups$^            groupskey.appendobj$^        else$^            groupskey  obj$^    return groups$^$^$^def getsamplesizepopulationsize$^    marginoferror  0.00$^    fractionpicking  0.00$^    zscore  0.00   corresponds to confidence level 00$^    numerator  zscore  0  fractionpicking  0  fractionpicking  $^        marginoferror  0$^    $^    samplesize  0$^    for trainsize in rangepopulationsize 0 0$^        denominator  0  zscore  0  fractionpicking  0  fractionpicking  $^            marginoferror  0  trainsize$^        $^        samplesize  intnumerator  denominator$^        if 0  samplesize  trainsize  populationsize$^            break$^    return samplesize$^$^$^def maybedownloadlanguagelanguage$^    langupper  language0.upper  language0$^    return maybedownload$^        swcarchive.formatlanguagelangupper$^        cliargs.basedir$^        swcurl.formatlanguagelangupper$^    $^$^$^def maybeextractdatadir extracteddata archive$^    extracted  os.path.joindatadir extracteddata$^    if os.path.isdirextracted$^        printfound directory   not extracting..formatextracted$^    else$^        printextracting ....formatarchive$^        with tarfile.openarchive as tar$^            members  tar.getmembers$^            bar  progressbar.progressbarmaxvaluelenmembers widgetssimplebar$^            for member in barmembers$^                tar.extractmembermember pathextracted$^    return extracted$^$^$^def ignorednode$^    if node is none$^        return false$^    if node.tag  ignored$^        return true$^    return ignorednode.find..$^$^$^def readtokentoken$^    texts start end   none none$^    notes  token.findalln$^    if lennotes  0$^        for note in notes$^            attributes  note.attrib$^            if start is none and start in attributes$^                start  intattributesstart$^            if end in attributes$^                tokenend  intattributesend$^                if end is none or tokenend  end$^                    end  tokenend$^            if pronunciation in attributes$^                t  attributespronunciation$^                texts.appendt$^    elif text in token.attrib$^        texts.appendtoken.attribtext$^    return start end  .jointexts$^$^$^def inalphabetalphabet c$^    return alphabet.canencodec if alphabet else true$^$^$^$^alphabets  $^$^$^def getalphabetlanguage$^    if language in alphabets$^        return alphabetslanguage$^    alphabetpath  getattrcliargs language  alphabet$^    alphabet  alphabetalphabetpath if alphabetpath else none$^    alphabetslanguage  alphabet$^    return alphabet$^$^$^def labelfilterlabel language$^    label  label.translateprefilter$^    label  validatelabellabel$^    if label is none$^        return none validation$^    substitutions  substitutionslanguage if language in substitutions else $^    for pattern replacement in substitutions$^        if replacement is none$^            if pattern.matchlabel$^                return none substitution rule$^        else$^            label  pattern.subreplacement label$^    chars  $^    dontnormalize  dontnormalizelanguage if language in dontnormalize else $^    alphabet  getalphabetlanguage$^    for c in label$^        if cliargs.normalize and c not in dontnormalize and not inalphabetalphabet c$^            c  unicodedata.normalizenfkd c.encodeascii ignore.decodeascii ignore$^        for sc in c$^            if not inalphabetalphabet sc$^                return none illegal character$^            chars.appendsc$^    label  .joinchars$^    label  validatelabellabel$^    return label validation if label is none else none$^$^$^def collectsamplesbasedir language$^    roots  $^    for root  files in os.walkbasedir$^        if alignedname in files and wavname in files$^            roots.appendroot$^    samples  $^    reasons  counter$^$^    def addsample$^        pwavpath particle pspeaker pstart pend ptext preasoncomplete$^    $^        if pstart is not none and pend is not none and ptext is not none$^            duration  pend  pstart$^            text filterreason  labelfilterptext language$^            skip  false$^            if filterreason is not none$^                skip  true$^                preason  filterreason$^            elif cliargs.excludeunknownspeakers and pspeaker  unknown$^                skip  true$^                preason  unknown speaker$^            elif cliargs.excludeunknownarticles and particle  unknown$^                skip  true$^                preason  unknown article$^            elif duration  cliargs.maxduration  0 and cliargs.ignoretoolong$^                skip  true$^                preason  exceeded duration$^            elif intduration  00  lentext$^                skip  true$^                preason  too short to decode$^            elif duration  lentext  00$^                skip  true$^                preason  length duration ratio$^            if skip$^                reasonspreason  0$^            else$^                samples.append$^                    samplepwavpath pstart pend text particle pspeaker$^                $^        elif pstart is none or pend is none$^            reasonsmissing timestamps  0$^        else$^            reasonsmissing text  0$^$^    printcollecting samples...$^    bar  progressbar.progressbarmaxvaluelenroots widgetssimplebar$^    for root in barroots$^        wavpath  os.path.joinroot wavname$^        aligned  et.parseos.path.joinroot alignedname$^        article  unknown$^        speaker  unknown$^        for prop in aligned.iterprop$^            attributes  prop.attrib$^            if key in attributes and value in attributes$^                if attributeskey  dc.identifier$^                    article  attributesvalue$^                elif attributeskey  reader.name$^                    speaker  attributesvalue$^        for sentence in aligned.iters$^            if ignoredsentence$^                continue$^            split  false$^            tokens  listmapreadtoken sentence.findallt$^            samplestart sampleend tokentexts sampletexts  none none  $^            for tokenstart tokenend tokentext in tokens$^                if cliargs.excludenumbers and anyc.isdigit for c in tokentext$^                    addsample$^                        wavpath$^                        article$^                        speaker$^                        samplestart$^                        sampleend$^                         .joinsampletexts$^                        preasonhas numbers$^                    $^                    samplestart sampleend tokentexts sampletexts  $^                        none$^                        none$^                        $^                        $^                    $^                    continue$^                if samplestart is none$^                    samplestart  tokenstart$^                if samplestart is none$^                    continue$^                tokentexts.appendtokentext$^                if tokenend is not none$^                    if $^                        tokenstart  samplestart$^                        and tokenend  samplestart  cliargs.maxduration  0$^                    $^                        addsample$^                            wavpath$^                            article$^                            speaker$^                            samplestart$^                            sampleend$^                             .joinsampletexts$^                            preasonsplit$^                        $^                        samplestart  sampleend$^                        sampletexts  $^                        split  true$^                    sampleend  tokenend$^                    sampletexts.extendtokentexts$^                    tokentexts  $^            addsample$^                wavpath$^                article$^                speaker$^                samplestart$^                sampleend$^                 .joinsampletexts$^                preasonsplit if split else complete$^            $^    printskipped samples$^    for reason n in reasons.mostcommon$^        print   .formatreason n$^    return samples$^$^$^def maybeconvertonetowaventry$^    root  files  entry$^    transformer  sox.transformer$^    transformer.convertsampleratesamplerate nchannelschannels$^    combiner  sox.combiner$^    combiner.convertsampleratesamplerate nchannelschannels$^    outputwav  os.path.joinroot wavname$^    if os.path.isfileoutputwav$^        return$^    files  sortedglobos.path.joinroot audiopattern$^    try$^        if lenfiles  0$^            transformer.buildfiles0 outputwav$^        elif lenfiles  0$^            wavfiles  $^            for i file in enumeratefiles$^                wavpath  os.path.joinroot audio.wav.formati$^                transformer.buildfile wavpath$^                wavfiles.appendwavpath$^            combiner.setinputformatfiletypewav  lenwavfiles$^            combiner.buildwavfiles outputwav concatenate$^    except sox.core.soxerror$^        return$^$^$^def maybeconverttowavbasedir$^    roots  listos.walkbasedir$^    printconverting and joining source audio files...$^    bar  progressbar.progressbarmaxvaluelenroots widgetssimplebar$^    tp  threadpool$^    for  in bartp.imapunorderedmaybeconvertonetowav roots$^        pass$^    tp.close$^    tp.join$^$^$^def assignsubsetssamples$^    samplesize  getsamplesizelensamples$^    speakers  groupsamples lambda sample sample.speaker.values$^    speakers  listsortedspeakers keylen$^    samplesets   $^    while anymaplambda s lens  samplesize samplesets and lenspeakers  0$^        for sampleset in samplesets$^            if lensampleset  samplesize and lenspeakers  0$^                sampleset.extendspeakers.pop0$^    trainset  sumspeakers $^    if lentrainset  0$^        print$^            warning unable to build dev and test sets without speaker bias as there is no speaker meta data$^        $^        random.seed00   same source data  same output$^        random.shufflesamples$^        for index sample in enumeratesamples$^            if index  samplesize$^                sample.subset  dev$^            elif index  0  samplesize$^                sample.subset  test$^            else$^                sample.subset  train$^    else$^        for subset subsetsamples in $^            train trainset$^            dev samplesets0$^            test samplesets0$^        $^            for sample in subsetsamples$^                sample.subset  subset$^    for subset subsetsamples in groupsamples lambda s s.subset.items$^        t  summaplambda s s.end  s.start subsetsamples  0000  00  00$^        print$^            subset  with  samples duration .0f h.format$^                subset lensubsetsamples t$^            $^        $^$^$^def createsampledirslanguage$^    printcreating sample directories...$^    for setname in train dev test$^        dirpath  os.path.joincliargs.basedir language    setname$^        if not os.path.isdirdirpath$^            os.mkdirdirpath$^$^$^def splitaudiofilessamples language$^    printsplitting audio files...$^    subsets  counter$^    srcwavfiles  groupsamples lambda s s.wavpath.items$^    bar  progressbar.progressbarmaxvaluelensrcwavfiles widgetssimplebar$^    for wavpath filesamples in barsrcwavfiles$^        filesamples  sortedfilesamples keylambda s s.start$^        with wave.openwavpath r as srcwavfile$^            rate  srcwavfile.getframerate$^            for sample in filesamples$^                index  subsetssample.subset$^                samplewavpath  os.path.join$^                    cliargs.basedir$^                    language    sample.subset$^                    sample000d.wav.formatindex$^                $^                sample.wavpath  samplewavpath$^                subsetssample.subset  0$^                srcwavfile.setposintsample.start  rate  0000.0$^                data  srcwavfile.readframes$^                    intsample.end  sample.start  rate  0000.0$^                $^                with wave.opensamplewavpath w as samplewavfile$^                    samplewavfile.setnchannelssrcwavfile.getnchannels$^                    samplewavfile.setsampwidthsrcwavfile.getsampwidth$^                    samplewavfile.setframeraterate$^                    samplewavfile.writeframesdata$^$^$^def writecsvssamples language$^    for subset setsamples in groupsamples lambda s s.subset.items$^        setsamples  sortedsetsamples keylambda s s.wavpath$^        basedir  os.path.abspathcliargs.basedir$^        csvpath  os.path.joinbasedir language    subset  .csv$^        printwriting ....formatcsvpath$^        with opencsvpath w encodingutf0 newline as csvfile$^            writer  csv.dictwriter$^                csvfile fieldnamesfieldnamesext if cliargs.addmeta else fieldnames$^            $^            writer.writeheader$^            bar  progressbar.progressbar$^                maxvaluelensetsamples widgetssimplebar$^            $^            for sample in barsetsamples$^                row  $^                    wavfilename os.path.relpathsample.wavpath basedir$^                    wavfilesize os.path.getsizesample.wavpath$^                    transcript sample.text$^                $^                if cliargs.addmeta$^                    rowarticle  sample.article$^                    rowspeaker  sample.speaker$^                writer.writerowrow$^$^$^def cleanuparchive language$^    if not cliargs.keeparchive$^        printremoving archive ....formatarchive$^        os.removearchive$^    languagedir  os.path.joincliargs.basedir language$^    if not cliargs.keepintermediate and os.path.isdirlanguagedir$^        printremoving intermediate files in ....formatlanguagedir$^        shutil.rmtreelanguagedir$^$^$^def preparelanguagelanguage$^    archive  maybedownloadlanguagelanguage$^    extracted  maybeextractcliargs.basedir language archive$^    maybeconverttowavextracted$^    samples  collectsamplesextracted language$^    assignsubsetssamples$^    createsampledirslanguage$^    splitaudiofilessamples language$^    writecsvssamples language$^    cleanuparchive language$^$^$^def handleargs$^    parser  argparse.argumentparserdescriptionimport spoken wikipedia corpora$^    parser.addargumentbasedir helpdirectory containing all data$^    parser.addargument$^        language defaultall helpone of all.format.joinlanguages$^    $^    parser.addargument$^        excludenumbers$^        typebool$^        defaulttrue$^        helpif sequences with nontransliterated numbers should be excluded$^    $^    parser.addargument$^        maxduration$^        typeint$^        default00000$^        helpmaximum sample duration in milliseconds$^    $^    parser.addargument$^        ignoretoolong$^        typebool$^        defaultfalse$^        helpif samples exceeding maxduration should be removed$^    $^    parser.addargument$^        normalize$^        actionstoretrue$^        helpconverts diacritic characters to their base ones$^    $^    for language in languages$^        parser.addargument$^            alphabet.formatlanguage$^            helpexclude  samples with characters not in provided alphabet file.format$^                language$^            $^        $^    parser.addargument$^        addmeta actionstoretrue helpadds article and speaker csv columns$^    $^    parser.addargument$^        excludeunknownspeakers$^        actionstoretrue$^        helpexclude unknown speakers$^    $^    parser.addargument$^        excludeunknownarticles$^        actionstoretrue$^        helpexclude unknown articles$^    $^    parser.addargument$^        keeparchive$^        typebool$^        defaulttrue$^        helpif downloaded archives should be kept$^    $^    parser.addargument$^        keepintermediate$^        typebool$^        defaultfalse$^        helpif intermediate files should be kept$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    if cliargs.language  all$^        for lang in languages$^            preparelanguagelang$^    elif cliargs.language in languages$^        preparelanguagecliargs.language$^    else$^        failwrong language id$^usrbinenv python$^$^tool for playing and augmenting single samples or samples from sample databases sdb files and deepspeech csv files$^use python0 play.py h for help$^$^$^import os$^import sys$^import random$^import argparse$^$^from deepspeechtraining.util.audio import getloadableaudiotypefromextension audiotypepcm audiotypewav$^from deepspeechtraining.util.samplecollections import samplelist labeledsample samplesfromsource$^from deepspeechtraining.util.augmentations import parseaugmentations applysampleaugmentations sampleaugmentation$^$^$^def getsamplesinplayorder$^    ext  os.path.splitextcliargs.source0.lower$^    if getloadableaudiotypefromextensionext$^        samples  samplelistcliargs.source 0 labeledfalse$^    else$^        samples  samplesfromsourcecliargs.source buffering0$^    played  0$^    index  cliargs.start$^    while true$^        if 0  cliargs.number  played$^            return$^        if cliargs.random$^            yield samplesrandom.randint0 lensamples  0$^        elif index  0$^            yield sampleslensamples  index$^        elif index  lensamples$^            printno sample with index .formatcliargs.start$^            sys.exit0$^        else$^            yield samplesindex$^        played  0$^        index  index  0  lensamples$^$^$^def playcollection$^    augmentations  parseaugmentationscliargs.augment$^    if anynot isinstancea sampleaugmentation for a in augmentations$^        printwarning some of the augmentations cannot be simulated by this command.$^    samples  getsamplesinplayorder$^    samples  applysampleaugmentationssamples$^                                         audiotypeaudiotypepcm$^                                         augmentationsaugmentations$^                                         processahead0$^                                         clockcliargs.clock$^    for sample in samples$^        if not cliargs.quiet$^            printsample .formatsample.sampleid filesys.stderr$^            if isinstancesample labeledsample$^                print  .formatsample.transcript filesys.stderr$^        if cliargs.pipe$^            sample.changeaudiotypeaudiotypewav$^            sys.stdout.buffer.writesample.audio.getvalue$^            return$^        waveobj  simpleaudio.waveobjectsample.audio$^                                          sample.audioformat.channels$^                                          sample.audioformat.width$^                                          sample.audioformat.rate$^        playobj  waveobj.play$^        playobj.waitdone$^$^$^def handleargs$^    parser  argparse.argumentparser$^        descriptiontool for playing and augmenting single samples or samples from sample databases sdb files $^        and deepspeech csv files$^    $^    parser.addargumentsource helpsample db csv or wav file to play samples from$^    parser.addargument$^        start$^        typeint$^        default0$^        helpsample index to start at negative numbers are relative to the end of the collection$^    $^    parser.addargument$^        number$^        typeint$^        default0$^        helpnumber of samples to play 0 for endless$^    $^    parser.addargument$^        random$^        actionstoretrue$^        helpif samples should be played in random order$^    $^    parser.addargument$^        augment$^        actionappend$^        helpadd an augmentation operation$^    $^    parser.addargument$^        clock$^        typefloat$^        default0.0$^        helpsimulates clock value used for augmentations during training.$^             ranges from 0.0 representing parameter start values to$^             0.0 representing parameter end values$^    $^    parser.addargument$^        pipe$^        actionstoretrue$^        helppipe first sample as wav file to stdout. forces number to 0.$^    $^    parser.addargument$^        quiet$^        actionstoretrue$^        helpno info logging to console$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    if not cliargs.pipe$^        try$^            import simpleaudio$^        except modulenotfounderror$^            printunless using the pipe flag play.py requires python package simpleaudio for playing samples$^            sys.exit0$^    try$^        playcollection$^    except keyboardinterrupt$^        print stopped$^        sys.exit0$^usrbinenv python0$^import csv$^import os$^import subprocess$^import tarfile$^import unicodedata$^from glob import glob$^from multiprocessing import pool$^$^import progressbar$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    getimportersparser$^    getvalidatelabel$^    printimportreport$^$^from dsctcdecoder import alphabet$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^maxsecs  00$^$^archivedirname  africanaccentedfrench$^archivename  africanaccentedfrench.tar.gz$^archiveurl  httpwww.openslr.orgresources00  archivename$^$^$^def downloadandpreprocessdatatargetdir$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^     conditionally download data$^    archivepath  maybedownloadarchivename targetdir archiveurl$^     conditionally extract data$^    maybeextracttargetdir archivedirname archivepath$^     produce csv files$^    maybeconvertsetstargetdir archivedirname$^$^$^def maybeextracttargetdir extracteddata archivepath$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.path.jointargetdir extracteddata$^    if not os.path.existsextractedpath$^        printno directory s  extracting archive...  extractedpath$^        if not os.path.isdirextractedpath$^            os.mkdirextractedpath$^        tar  tarfile.openarchivepath$^        tar.extractalltargetdir$^        tar.close$^    else$^        printfound directory s  not extracting it from archive.  archivepath$^$^$^def onesamplesample$^     take a audio file and optionally convert it to 00khz wav $^    wavfilename  sample0$^    filesize  0$^    frames  0$^    if os.path.existswavfilename$^        filesize  os.path.getsizewavfilename$^        frames  int$^            subprocess.checkoutput$^                soxi s wavfilename stderrsubprocess.stdout$^            $^        $^    label  labelfiltersample0$^    counter  getcounter$^    rows  $^    if filesize  0$^         excluding samples that failed upon conversion$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframes  samplerate  0000  00  0  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif frames  samplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendwavfilename filesize label$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^$^    return counter rows$^$^$^def maybeconvertsetstargetdir extracteddata$^    extracteddir  os.path.jointargetdir extracteddata$^     override existing csv with normalized one$^    targetcsvtemplate  os.path.join$^        targetdir archivedirname archivename.replace.tar.gz .csv$^    $^    if os.path.isfiletargetcsvtemplate$^        return$^$^    wavrootdir  os.path.joinextracteddir$^$^    allfiles  $^        transcriptstrainyaoundefntext.txt$^        transcriptstrainca00convtranscripts.txt$^        transcriptstrainca00readconditioned.txt$^        transcriptsdevnigerwestafricanfrtranscripts.txt$^        speechdevnigerwestafricanfrnigerwavfilenametranscript.tsv$^        transcriptsdevtestca00readconditioned.txt$^        transcriptstestca00prompts.txt$^    $^$^    transcripts  $^    for tr in allfiles$^        with openos.path.jointargetdir archivedirname tr r as trsource$^            for line in trsource.readlines$^                line  line.strip$^$^                if .tsv in tr$^                    sep  $^                else$^                    sep   $^$^                audio  os.path.basenameline.splitsep0$^$^                if not .wav in audio$^                    if .tdf in audio$^                        audio  audio.replace.tdf .wav$^                    else$^                        audio  .wav$^$^                transcript   .joinline.splitsep0$^                transcriptsaudio  transcript$^$^     get audiofile path and transcript for each sentence in tsv$^    samples  $^    globdir  os.path.joinwavrootdir .wav$^    for record in globglobdir recursivetrue$^        recordfile  os.path.basenamerecord$^        if recordfile in transcripts$^            samples.appendrecord transcriptsrecordfile$^$^     keep track of how many samples are good vs. problematic$^    counter  getcounter$^    numsamples  lensamples$^    rows  $^$^    printimporting wav files...$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i processed in enumeratepool.imapunorderedonesample samples start0$^        counter  processed0$^        rows  processed0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    with opentargetcsvtemplate.formattrain w encodingutf0 newline as traincsvfile   00$^        with opentargetcsvtemplate.formatdev w encodingutf0 newline as devcsvfile   00$^            with opentargetcsvtemplate.formattest w encodingutf0 newline as testcsvfile   00$^                trainwriter  csv.dictwritertraincsvfile fieldnamesfieldnames$^                trainwriter.writeheader$^                devwriter  csv.dictwriterdevcsvfile fieldnamesfieldnames$^                devwriter.writeheader$^                testwriter  csv.dictwritertestcsvfile fieldnamesfieldnames$^                testwriter.writeheader$^$^                for i item in enumeraterows$^                    transcript  validatelabelitem0$^                    if not transcript$^                        continue$^                    wavfilename  item0$^                    imod  i  00$^                    if imod  0$^                        writer  testwriter$^                    elif imod  0$^                        writer  devwriter$^                    else$^                        writer  trainwriter$^                    writer.writerow$^                        dict$^                            wavfilenamewavfilename$^                            wavfilesizeos.path.getsizewavfilename$^                            transcripttranscript$^                        $^                    $^$^    importedsamples  getimportedsamplescounter$^    assert counterall  numsamples$^    assert lenrows  importedsamples$^$^    printimportreportcounter samplerate maxsecs$^$^$^def handleargs$^    parser  getimportersparser$^        descriptionimporter for african accented french dataset. more information on httpwww.openslr.org00.$^    $^    parser.addargumentdesttargetdir$^    parser.addargument$^        filteralphabet$^        helpexclude samples with characters not in provided alphabet$^    $^    parser.addargument$^        normalize$^        actionstoretrue$^        helpconverts diacritic characters to their base ones$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    alphabet  alphabetcliargs.filteralphabet if cliargs.filteralphabet else none$^    validatelabel  getvalidatelabelcliargs$^$^    def labelfilterlabel$^        if cliargs.normalize$^            label  $^                unicodedata.normalizenfkd label.strip$^                .encodeascii ignore$^                .decodeascii ignore$^            $^        label  validatelabellabel$^        if alphabet and label and not alphabet.canencodelabel$^            label  none$^        return label$^$^    downloadandpreprocessdatatargetdircliargs.targetdir$^usrbinenv python$^$^tool for comparing two wav samples$^$^import sys$^import argparse$^import numpy as np$^$^from deepspeechtraining.util.audio import audiotypenp meandbfs$^from deepspeechtraining.util.samplecollections import loadsample$^$^$^def failmessage$^    printmessage filesys.stderr flushtrue$^    sys.exit0$^$^$^def comparesamples$^    sample0  loadsamplecliargs.sample0.unpack$^    sample0  loadsamplecliargs.sample0.unpack$^    if sample0.audioformat  sample0.audioformat$^        failsamples differ on audioformat  and .formatsample0.audioformat sample0.audioformat$^    if abssample0.duration  sample0.duration  0.000$^        failsamples differ on duration  and .formatsample0.duration sample0.duration$^    sample0.changeaudiotypeaudiotypenp$^    sample0.changeaudiotypeaudiotypenp$^    samples  sample0 sample0$^    largest  np.argmaxsample0.audio.shape0 sample0.audio.shape0$^    smallest  largest  0  0$^    sampleslargest.audio  sampleslargest.audiolensamplessmallest.audio$^    audiodiff  sampleslargest.audio  samplessmallest.audio$^    diffdbfs  meandbfsaudiodiff$^    differmsg  samples differ on sample data 0.0f db difference .formatdiffdbfs$^    equalmsg  samples are considered equal 0.0f db difference.formatdiffdbfs$^    if cliargs.ifdiffer$^        if diffdbfs  cliargs.threshold$^            failequalmsg$^        if not cliargs.nosuccessoutput$^            printdiffermsg filesys.stderr flushtrue$^    else$^        if diffdbfs  cliargs.threshold$^            faildiffermsg$^        if not cliargs.nosuccessoutput$^            printequalmsg filesys.stderr flushtrue$^$^$^def handleargs$^    parser  argparse.argumentparser$^        descriptiontool for checking similarity of two samples$^    $^    parser.addargumentsample0 helpfilename of sample 0 to compare$^    parser.addargumentsample0 helpfilename of sample 0 to compare$^    parser.addargumentthreshold typefloat default00.0$^                        helpdb of sample deltas above which they are considered different$^    parser.addargument$^        ifdiffer$^        actionstoretrue$^        helpif to succeed and return status code 0 on different signals and fail on equal ones inverse check.$^             this will still fail on different formats or durations.$^    $^    parser.addargument$^        nosuccessoutput$^        actionstoretrue$^        helpstay silent on success if samples are equal of  with ifdiffer  samples are not equal$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    comparesamples$^usrbinenv python$^$^import csv$^import logging$^import math$^import os$^import subprocess$^import urllib$^from pathlib import path$^$^import pandas as pd$^from sox import transformer$^$^import swifter$^from deepspeechtraining.util.importers import getimportersparser getvalidatelabel$^$^version  0.0.0$^logger  logging.getloggername$^$^$^maxsecs  00$^bitdepth  00$^nchannels  0$^samplerate  00000$^$^devpercentage  0.00$^trainpercentage  0.00$^$^$^def parseargsargs$^    parse command line parameters$^    args$^      args str command line parameters as list of strings$^    returns$^      objargparse.namespace command line parameters namespace$^    $^    parser  getimportersparserdescriptionimports gramvaani data for deep speech$^    parser.addargument$^        version$^        actionversion$^        versiongramvaaniimporter ver.formatverversion$^    $^    parser.addargument$^        v$^        verbose$^        actionstoreconst$^        requiredfalse$^        helpset loglevel to info$^        destloglevel$^        constlogging.info$^    $^    parser.addargument$^        vv$^        veryverbose$^        actionstoreconst$^        requiredfalse$^        helpset loglevel to debug$^        destloglevel$^        constlogging.debug$^    $^    parser.addargument$^        c$^        csvfilename$^        requiredtrue$^        helppath to the gramvaani csv$^        destcsvfilename$^    $^    parser.addargument$^        t$^        targetdir$^        requiredtrue$^        helpdirectory in which to save the importer gramvaani data$^        desttargetdir$^    $^    return parser.parseargsargs$^$^$^def setuplogginglevel$^    setup basic logging$^    args$^      level int minimum log level for emitting messages$^    $^    format  asctimes levelnamesnamesmessages$^    logging.basicconfig$^        levellevel streamsys.stdout formatformat datefmtymd hms$^    $^$^$^class gramvaanicsv$^    gramvaanicsv representing a gramvaani dataset.$^    args$^      csvfilename str path to the gramvaani csv$^    attributes$^        data classpandas.dataframe pandas.dataframe containing the gramvaani csv data$^    $^$^    def initself csvfilename$^        self.data  self.parsecsvcsvfilename$^$^    def parsecsvself csvfilename$^        logger.infoparsing csv file...s os.path.abspathcsvfilename$^        data  pd.readcsv$^            os.path.abspathcsvfilename$^            names$^                pieceid$^                audiourl$^                transcriptlabelled$^                transcript$^                labels$^                contentfilename$^                audiolength$^                userid$^            $^            usecolsaudiourl transcript audiolength$^            skiprows0$^            enginepython$^            encodingutf0$^            quotechar$^            quotingcsv.quoteall$^        $^        data.dropnainplacetrue$^        logger.infoparsed d lines csv file.  lendata$^        return data$^$^$^class gramvaanidownloader$^    gramvaanidownloader downloads a gramvaani dataset.$^    args$^      gramvaanicsv gramvaanicsv a gramvaanicsv representing the data to download$^      targetdir str the path to download the data to$^    attributes$^        data classpandas.dataframe pandas.dataframe containing the gramvaani csv data$^    $^$^    def initself gramvaanicsv targetdir$^        self.targetdir  targetdir$^        self.data  gramvaanicsv.data$^$^    def downloadself$^        downloads the data associated with this instance$^        return$^          mp0directory os.path the directory into which the associated mp0s were downloaded$^        $^        mp0directory  self.predownload$^        self.data.swifter.apply$^            funclambda arg self.downloadarg mp0directory axis0 rawtrue$^        $^        return mp0directory$^$^    def predownloadself$^        mp0directory  os.path.joinself.targetdir mp0$^        if not os.path.existsself.targetdir$^            logger.infocreating directory...s self.targetdir$^            os.mkdirself.targetdir$^        if not os.path.existsmp0directory$^            logger.infocreating directory...s mp0directory$^            os.mkdirmp0directory$^        return mp0directory$^$^    def downloadself audiourl transcript audiolength mp0directory$^        if audiourl  audiourl$^            return$^        mp0filename  os.path.joinmp0directory os.path.basenameaudiourl$^        if not os.path.existsmp0filename$^            logger.debugdownloading mp0 file...s audiourl$^            urllib.request.urlretrieveaudiourl mp0filename$^        else$^            logger.debugalready downloaded mp0 file...s audiourl$^$^$^class gramvaaniconverter$^    gramvaaniconverter converts the mp0s to wavs for a gramvaani dataset.$^    args$^      targetdir str the path to download the data from$^      mp0directory os.path the path containing the gramvaani mp0s$^    attributes$^        targetdir str the target directory passed as a command line argument$^        mp0directory os.path the path containing the gramvaani mp0s$^    $^$^    def initself targetdir mp0directory$^        self.targetdir  targetdir$^        self.mp0directory  pathmp0directory$^$^    def convertself$^        converts the mp0s associated with this instance to wavs$^        return$^          wavdirectory os.path the directory into which the associated wavs were downloaded$^        $^        wavdirectory  self.preconvert$^        for mp0filename in self.mp0directory.glob.mp0$^            wavfilename  os.path.join$^                wavdirectory$^                os.path.splitextos.path.basenamemp0filename0  .wav$^            $^            if not os.path.existswavfilename$^                logger.debug$^                    converting mp0 file s to wav file s$^                     mp0filename wavfilename$^                $^                transformer  transformer$^                transformer.convert$^                    sampleratesamplerate nchannelsnchannels bitdepthbitdepth$^                $^                transformer.buildstrmp0filename strwavfilename$^            else$^                logger.debug$^                    already converted mp0 file s to wav file s$^                     mp0filename wavfilename$^                $^        return wavdirectory$^$^    def preconvertself$^        wavdirectory  os.path.joinself.targetdir wav$^        if not os.path.existsself.targetdir$^            logger.infocreating directory...s self.targetdir$^            os.mkdirself.targetdir$^        if not os.path.existswavdirectory$^            logger.infocreating directory...s wavdirectory$^            os.mkdirwavdirectory$^        return wavdirectory$^$^$^class gramvaanidatasets$^    def initself targetdir wavdirectory gramvaanicsv$^        self.targetdir  targetdir$^        self.wavdirectory  wavdirectory$^        self.csvdata  gramvaanicsv.data$^        self.raw  pd.dataframecolumnswavfilename wavfilesize transcript$^        self.valid  pd.dataframe$^            columnswavfilename wavfilesize transcript$^        $^        self.train  pd.dataframe$^            columnswavfilename wavfilesize transcript$^        $^        self.dev  pd.dataframecolumnswavfilename wavfilesize transcript$^        self.test  pd.dataframecolumnswavfilename wavfilesize transcript$^$^    def createself$^        self.convertcsvdatatorawdata$^        self.raw.index  rangelenself.raw.index$^        self.valid  self.rawself.isvalidrawrows$^        self.valid  self.valid.samplefrac0.resetindexdroptrue$^        trainsize devsize testsize  self.calculatedatasetsizes$^        self.train  self.valid.loc0trainsize$^        self.dev  self.valid.loctrainsize  trainsize  devsize$^        self.test  self.valid.loc$^            trainsize  devsize  trainsize  devsize  testsize$^        $^$^    def convertcsvdatatorawdataself$^        self.rawwavfilename wavfilesize transcript  self.csvdata$^            audiourl transcript audiolength$^        .swifter.apply$^            funclambda arg self.convertcsvdatatorawdataimplarg$^            axis0$^            rawtrue$^        $^        self.raw.resetindex$^$^    def convertcsvdatatorawdataimplself audiourl transcript audiolength$^        if audiourl  audiourl$^            return pd.serieswavfilename wavfilesize transcript$^        mp0filename  os.path.basenameaudiourl$^        wavrelativefilename  os.path.join$^            wav os.path.splitextos.path.basenamemp0filename0  .wav$^        $^        wavfilesize  os.path.getsize$^            os.path.joinself.targetdir wavrelativefilename$^        $^        transcript  validatelabeltranscript$^        if none  transcript$^            transcript  $^        return pd.serieswavrelativefilename wavfilesize transcript$^$^    def isvalidrawrowsself$^        isvalidrawtranscripts  self.isvalidrawtranscripts$^        isvalidrawwavframes  self.isvalidrawwavframes$^        isvalidrawrow  $^            isvalidrawtranscript  isvalidrawwavframe$^            for isvalidrawtranscript isvalidrawwavframe in zip$^                isvalidrawtranscripts isvalidrawwavframes$^            $^        $^        series  pd.seriesisvalidrawrow$^        return series$^$^    def isvalidrawtranscriptsself$^        return pd.seriesbooltranscript for transcript in self.raw.transcript$^$^    def isvalidrawwavframesself$^        transcripts  strtranscript for transcript in self.raw.transcript$^        wavfilepaths  $^            os.path.joinself.targetdir strwavfilename$^            for wavfilename in self.raw.wavfilename$^        $^        wavframes  $^            int$^                subprocess.checkoutput$^                    soxi s wavfilepath stderrsubprocess.stdout$^                $^            $^            for wavfilepath in wavfilepaths$^        $^        isvalidrawwavframes  $^            self.iswavframevalidwavframe transcript$^            for wavframe transcript in zipwavframes transcripts$^        $^        return pd.seriesisvalidrawwavframes$^$^    def iswavframevalidself wavframe transcript$^        iswavframevalid  true$^        if intwavframe  samplerate  0000  00  0  lenstrtranscript$^            iswavframevalid  false$^        elif wavframe  samplerate  maxsecs$^            iswavframevalid  false$^        return iswavframevalid$^$^    def calculatedatasetsizesself$^        totalsize  lenself.valid$^        devsize  math.floortotalsize  devpercentage$^        trainsize  math.floortotalsize  trainpercentage$^        testsize  totalsize  trainsize  devsize$^        return trainsize devsize testsize$^$^    def saveself$^        datasets  train dev test$^        for dataset in datasets$^            self.savedataset$^$^    def saveself dataset$^        datasetpath  os.path.joinself.targetdir dataset  .csv$^        dataframe  getattrself dataset$^        dataframe.tocsv$^            datasetpath$^            indexfalse$^            encodingutf0$^            escapechar$^            quotingcsv.quoteminimal$^        $^$^$^def mainargs$^    main entry point allowing external calls$^    args$^      args str command line parameter list$^    $^    args  parseargsargs$^    validatelabel  getvalidatelabelargs$^    setuploggingargs.loglevel$^    logger.infostarting gramvaani importer...$^    logger.infostarting loading gramvaani csv...$^    csv  gramvaanicsvargs.csvfilename$^    logger.infostarting downloading gramvaani mp0s...$^    downloader  gramvaanidownloadercsv args.targetdir$^    mp0directory  downloader.download$^    logger.infostarting converting gramvaani mp0s to wavs...$^    converter  gramvaaniconverterargs.targetdir mp0directory$^    wavdirectory  converter.convert$^    datasets  gramvaanidatasetsargs.targetdir wavdirectory csv$^    datasets.create$^    datasets.save$^    logger.infofinished gramvaani importer...$^$^$^mainsys.argv0$^usrbinenv python$^import glob$^import os$^import tarfile$^$^import numpy as np$^import pandas$^$^from deepspeechtraining.util.importers import getimportersparser$^$^columnnames  wavfilename wavfilesize transcript$^$^$^def extractarchivepath targetdir$^    printextracting  into ....formatarchivepath targetdir$^    with tarfile.openarchivepath as tar$^        tar.extractalltargetdir$^$^$^def preprocessdatatgzfile targetdir$^     first extract main archive and subarchives$^    extracttgzfile targetdir$^    mainfolder  os.path.jointargetdir stcmds000000000os$^$^     folder structure is now$^      stcmds000000000os$^        .wav$^        .txt$^        .metadata$^$^    def loadsetglobpath$^        setfiles  $^        for wav in glob.globglobpath$^            wavfilename  wav$^            wavfilesize  os.path.getsizewav$^            txtfilename  os.path.splitextwavfilename0  .txt$^            with opentxtfilename r as fin$^                transcript  fin.read$^            setfiles.appendwavfilename wavfilesize transcript$^        return setfiles$^$^     load all files then deterministically split into traindevtest sets$^    allfiles  loadsetos.path.joinmainfolder .wav$^    df  pandas.dataframedataallfiles columnscolumnnames$^    df.sortvaluesbywavfilename inplacetrue$^$^    indices  np.arange0 lendf$^    np.random.seed00000$^    np.random.shuffleindices$^$^     total corpus size 000000 samples. 0000 samples gives us 00 confidence$^     level with a margin of error of under 0.$^    testindices  indices0000$^    devindices  indices000000000$^    trainindices  indices00000$^$^    trainfiles  df.iloctrainindices$^    durations  trainfileswavfilesize  00  00000  0$^    trainfiles  trainfilesdurations  00.0$^    printtrimming  samples  00 seconds.formatdurations  00.0.sum$^    destcsv  os.path.jointargetdir freestmandarintrain.csv$^    printsaving train set into ....formatdestcsv$^    trainfiles.tocsvdestcsv indexfalse$^$^    devfiles  df.ilocdevindices$^    destcsv  os.path.jointargetdir freestmandarindev.csv$^    printsaving dev set into ....formatdestcsv$^    devfiles.tocsvdestcsv indexfalse$^$^    testfiles  df.iloctestindices$^    destcsv  os.path.jointargetdir freestmandarintest.csv$^    printsaving test set into ....formatdestcsv$^    testfiles.tocsvdestcsv indexfalse$^$^$^def main$^     httpswww.openslr.org00$^    parser  getimportersparserdescriptionimport free st chinese mandarin corpus$^    parser.addargumenttgzfile helppath to stcmds000000000os.tar.gz$^    parser.addargument$^        targetdir$^        default$^        helptarget folder to extract files into and put the resulting csvs. defaults to same folder as the main archive.$^    $^    params  parser.parseargs$^$^    if not params.targetdir$^        params.targetdir  os.path.dirnameparams.tgzfile$^$^    preprocessdataparams.tgzfile params.targetdir$^$^$^if name  main$^    main$^usrbinenv python$^$^tool for building a combined sdb or csv sampleset from other sets$^use python0 datasettool.py h for help$^$^import sys$^import argparse$^import progressbar$^from pathlib import path$^$^from deepspeechtraining.util.audio import $^    audiotypepcm$^    audiotypeopus$^    audiotypewav$^    changeaudiotypes$^$^from deepspeechtraining.util.downloader import simplebar$^from deepspeechtraining.util.samplecollections import $^    csvwriter$^    directsdbwriter$^    tarwriter$^    samplesfromsources$^$^from deepspeechtraining.util.augmentations import $^    parseaugmentations$^    applysampleaugmentations$^    sampleaugmentation$^$^$^audiotypelookup  wav audiotypewav opus audiotypeopus$^$^$^def builddataset$^    audiotype  audiotypelookupcliargs.audiotype$^    augmentations  parseaugmentationscliargs.augment$^    if anynot isinstancea sampleaugmentation for a in augmentations$^        printwarning some of the specified augmentations will not get applied as this tool only supports $^              overlay codec reverb resample and volume.$^    extension  pathcliargs.target.suffix.lower$^    labeled  not cliargs.unlabeled$^    if extension  .csv$^        writer  csvwritercliargs.target absolutepathscliargs.absolutepaths labeledlabeled$^    elif extension  .sdb$^        writer  directsdbwritercliargs.target audiotypeaudiotype labeledlabeled$^    elif extension  .tar$^        writer  tarwritercliargs.target labeledlabeled gzfalse includecliargs.include$^    elif extension  .tgz or cliargs.target.lower.endswith.tar.gz$^        writer  tarwritercliargs.target labeledlabeled gztrue includecliargs.include$^    else$^        printunknown extension of target file  has to be either .csv .sdb .tar .tar.gz or .tgz$^        sys.exit0$^    with writer$^        samples  samplesfromsourcescliargs.sources labelednot cliargs.unlabeled$^        numsamples  lensamples$^        if augmentations$^            samples  applysampleaugmentationssamples audiotypeaudiotypepcm augmentationsaugmentations$^        bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^        for sample in barchangeaudiotypes$^                samples$^                audiotypeaudiotype$^                bitratecliargs.bitrate$^                processescliargs.workers$^            writer.addsample$^$^$^def handleargs$^    parser  argparse.argumentparser$^        descriptiontool for building a combined sdb or csv sampleset from other sets$^    $^    parser.addargument$^        sources$^        nargs$^        helpsource csv andor sdb files  $^        note for getting a correctly ordered target set source sdbs have to have their samples $^        already ordered from shortest to longest.$^    $^    parser.addargument$^        target$^        helpsdb csv or tar.gz file to create$^    $^    parser.addargument$^        audiotype$^        defaultopus$^        choicesaudiotypelookup.keys$^        helpaudio representation inside target sdb$^    $^    parser.addargument$^        bitrate$^        typeint$^        helpbitrate for lossy compressed sdb samples like in case of audiotype opus$^    $^    parser.addargument$^        workers typeint defaultnone helpnumber of encoding sdb workers$^    $^    parser.addargument$^        unlabeled$^        actionstoretrue$^        helpif to build an dataset with unlabeled audio only samples  $^        typically used for building noise augmentation corpora$^    $^    parser.addargument$^        absolutepaths$^        actionstoretrue$^        helpif to reference samples by their absolute paths when writing csv files$^    $^    parser.addargument$^        augment$^        actionappend$^        helpadd an augmentation operation$^    $^    parser.addargument$^        include$^        actionappend$^        helpadds a file to the root directory of .tar.gz targets$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    builddataset$^usrbinenv python$^import codecs$^import os$^import re$^import sys$^import tarfile$^import threading$^import unicodedata$^import urllib$^from glob import glob$^from multiprocessing.pool import threadpool$^from os import makedirs path$^$^import pandas$^from bs0 import beautifulsoup$^from tensorflow.python.platform import gfile$^from deepspeechtraining.util.downloader import maybedownload$^$^the number of jobs to run in parallel$^numparallel  0$^$^lambda function returns the filename of a path$^filenameof  lambda x path.splitx0$^$^$^class atomiccounterobject$^    a class that atomically increments a counter$^$^    def initself startcount0$^        initialize the counter$^        param startcount the number to start counting at$^        $^        self.lock  threading.lock$^        self.count  startcount$^$^    def incrementself amount0$^        increments the counter by the given amount$^        param amount the amount to increment by default 0$^        return       the incremented value of the counter$^        $^        self.lock.acquire$^        self.count  amount$^        v  self.value$^        self.lock.release$^        return v$^$^    def valueself$^        returns the current value of the counter not atomic$^        return self.count$^$^$^def paralleldownloadervoxforgeurl archivedir total counter$^    generate a function to download a file based on given parameters$^    this works by currying the above given arguments into a closure$^    in the form of the following function.$^$^    param voxforgeurl the base voxforge url$^    param archivedir  the location to store the downloaded file$^    param total        the total number of files to download$^    param counter      an atomic counter to keep track of  of downloaded files$^    return             a function that actually downloads a file given these params$^    $^$^    def downloadd$^        binds voxforgeurl archivedir total and counter into this scope$^        downloads the given file$^        param d a tuple consisting of index file where index is the index$^                  of the file to download and file is the name of the file to download$^        $^        i file  d$^        downloadurl  voxforgeurl    file$^        c  counter.increment$^        printdownloading file  ....formati  0 c total$^        maybedownloadfilenameofdownloadurl archivedir downloadurl$^$^    return download$^$^$^def parallelextracterdatadir numberoftest numberofdev total counter$^    generate a function to extract a tar file based on given parameters$^    this works by currying the above given arguments into a closure$^    in the form of the following function.$^$^    param datadir       the target directory to extract into$^    param numberoftest the number of files to keep as the test set$^    param numberofdev  the number of files to keep as the dev set$^    param total          the total number of files to extract$^    param counter        an atomic counter to keep track of  of extracted files$^    return               a function that actually extracts a tar file given these params$^    $^$^    def extractd$^        binds datadir numberoftest numberofdev total and counter into this scope$^        extracts the given file$^        param d a tuple consisting of index file where index is the index$^                  of the file to extract and file is the name of the file to extract$^        $^        i archive  d$^        if i  numberoftest$^            datasetdir  path.joindatadir test$^        elif i  numberoftest  numberofdev$^            datasetdir  path.joindatadir dev$^        else$^            datasetdir  path.joindatadir train$^        if not gfile.exists$^            os.path.joindatasetdir ..joinfilenameofarchive.split.0$^        $^            c  counter.increment$^            printextracting file  ....formati  0 c total$^            tar  tarfile.openarchive$^            tar.extractalldatasetdir$^            tar.close$^$^    return extract$^$^$^def downloadandpreprocessdatadatadir$^     conditionally download data to datadir$^    if not path.isdirdatadir$^        makedirsdatadir$^$^    archivedir  datadir  archive$^    if not path.isdirarchivedir$^        makedirsarchivedir$^$^    print$^        downloading voxforge data set into  if not already present....format$^            archivedir$^        $^    $^$^    voxforgeurl  httpwww.repository.voxforge0.orgdownloadsspeechcorpustrunkaudiomain00khz00bit$^    htmlpage  urllib.request.urlopenvoxforgeurl$^    soup  beautifulsouphtmlpage html.parser$^$^     list all links$^    refs  lhref for l in soup.findalla if .tgz in lhref$^$^     download files in parallel$^    print files to download.formatlenrefs$^    downloader  paralleldownloader$^        voxforgeurl archivedir lenrefs atomiccounter$^    $^    p  threadpoolnumparallel$^    p.mapdownloader enumeraterefs$^$^     conditionally extract data to datasetdir$^    if not path.isdiros.path.joindatadir test$^        makedirsos.path.joindatadir test$^    if not path.isdiros.path.joindatadir dev$^        makedirsos.path.joindatadir dev$^    if not path.isdiros.path.joindatadir train$^        makedirsos.path.joindatadir train$^$^    tarfiles  globos.path.joinarchivedir .tgz$^    numberoffiles  lentarfiles$^    numberoftest  numberoffiles  000$^    numberofdev  numberoffiles  000$^$^     extract tars in parallel$^    print$^        extracting voxforge data set into  if not already present....format$^            datadir$^        $^    $^    extracter  parallelextracter$^        datadir numberoftest numberofdev lentarfiles atomiccounter$^    $^    p.mapextracter enumeratetarfiles$^$^     generate data set$^    printgenerating voxforge data set into .formatdatadir$^    testfiles  generatedatasetdatadir test$^    devfiles  generatedatasetdatadir dev$^    trainfiles  generatedatasetdatadir train$^$^     write sets to disk as csv files$^    trainfiles.tocsvos.path.joindatadir voxforgetrain.csv indexfalse$^    devfiles.tocsvos.path.joindatadir voxforgedev.csv indexfalse$^    testfiles.tocsvos.path.joindatadir voxforgetest.csv indexfalse$^$^$^def generatedatasetdatadir dataset$^    extracteddir  path.joindatadir dataset$^    files  $^    for promtsfile in globos.path.joinextracteddir  etc prompts$^        if path.isdiros.path.joinpromtsfile00 wav$^            with codecs.openpromtsfile r utf0 as f$^                for line in f$^                    id  line.split 0.split0$^                    sentence   .joinline.split 0$^                    sentence  re.subaz   sentence.strip.lower$^                    transcript  $^                    for token in sentence.split $^                        word  token.strip$^                        if word   and word   $^                            transcript  word   $^                    transcript  $^                        unicodedata.normalizenfkd transcript.strip$^                        .encodeascii ignore$^                        .decodeascii ignore$^                    $^                    wavfile  path.joinpromtsfile00 wav  id  .wav$^                    if gfile.existswavfile$^                        wavfilesize  path.getsizewavfile$^                         remove audios that are shorter than 0.0s and longer than 00s.$^                         remove audios that are too short for transcript.$^                        if $^                            wavfilesize  00000  0.0$^                            and wavfilesize  00000  00$^                            and transcript  $^                            and wavfilesize  lentranscript  0000$^                        $^                            files.append$^                                os.path.abspathwavfile wavfilesize transcript$^                            $^$^    return pandas.dataframe$^        datafiles columnswavfilename wavfilesize transcript$^    $^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^import glob$^import json$^import os$^import tarfile$^$^import numpy as np$^import pandas$^$^from deepspeechtraining.util.importers import getimportersparser$^$^columnnames  wavfilename wavfilesize transcript$^$^$^def extractarchivepath targetdir$^    printextracting  into ....formatarchivepath targetdir$^    with tarfile.openarchivepath as tar$^        tar.extractalltargetdir$^$^$^def preprocessdatatgzfile targetdir$^     first extract main archive and subarchives$^    extracttgzfile targetdir$^    mainfolder  os.path.jointargetdir primewordsmd0000set0$^$^     folder structure is now$^      primewordsmd0000set0$^        audiofiles$^          0f000f.wav$^        set0transcript.json$^$^    transcriptspath  os.path.joinmainfolder set0transcript.json$^    with opentranscriptspath as fin$^        transcripts  json.loadfin$^$^    transcripts  entryfile entrytext for entry in transcripts$^$^    def loadsetglobpath$^        setfiles  $^        for wav in glob.globglobpath$^            try$^                wavfilename  wav$^                wavfilesize  os.path.getsizewav$^                transcriptkey  os.path.basenamewav$^                transcript  transcriptstranscriptkey$^                setfiles.appendwavfilename wavfilesize transcript$^            except keyerror$^                printwarning missing transcript for wav file ..formatwav$^        return setfiles$^$^     load all files then deterministically split into traindevtest sets$^    allfiles  loadsetos.path.joinmainfolder audiofiles   .wav$^    df  pandas.dataframedataallfiles columnscolumnnames$^    df.sortvaluesbywavfilename inplacetrue$^$^    indices  np.arange0 lendf$^    np.random.seed00000$^    np.random.shuffleindices$^$^     total corpus size 00000 samples. 0000 samples gives us 00 confidence$^     level with a margin of error of under 0.$^    testindices  indices0000$^    devindices  indices000000000$^    trainindices  indices00000$^$^    trainfiles  df.iloctrainindices$^    durations  trainfileswavfilesize  00  00000  0$^    trainfiles  trainfilesdurations  00.0$^    printtrimming  samples  00 seconds.formatdurations  00.0.sum$^    destcsv  os.path.jointargetdir primewordstrain.csv$^    printsaving train set into ....formatdestcsv$^    trainfiles.tocsvdestcsv indexfalse$^$^    devfiles  df.ilocdevindices$^    destcsv  os.path.jointargetdir primewordsdev.csv$^    printsaving dev set into ....formatdestcsv$^    devfiles.tocsvdestcsv indexfalse$^$^    testfiles  df.iloctestindices$^    destcsv  os.path.jointargetdir primewordstest.csv$^    printsaving test set into ....formatdestcsv$^    testfiles.tocsvdestcsv indexfalse$^$^$^def main$^     httpswww.openslr.org00$^    parser  getimportersparserdescriptionimport primewords chinese corpus set 0$^    parser.addargumenttgzfile helppath to primewordsmd0000set0.tar.gz$^    parser.addargument$^        targetdir$^        default$^        helptarget folder to extract files into and put the resulting csvs. defaults to same folder as the main archive.$^    $^    params  parser.parseargs$^$^    if not params.targetdir$^        params.targetdir  os.path.dirnameparams.tgzfile$^$^    preprocessdataparams.tgzfile params.targetdir$^$^$^if name  main$^    main$^usrbinenv python$^import codecs$^import fnmatch$^import os$^import subprocess$^import sys$^import tarfile$^import unicodedata$^$^import pandas$^import progressbar$^from sox import transformer$^from tensorflow.python.platform import gfile$^$^from deepspeechtraining.util.downloader import maybedownload$^$^samplerate  00000$^$^$^def downloadandpreprocessdatadatadir$^     conditionally download data to datadir$^    print$^        downloading librivox data set 00gb into  if not already present....format$^            datadir$^        $^    $^    with progressbar.progressbarmaxvalue0 widgetprogressbar.adaptiveeta as bar$^        trainclean000url  $^            httpwww.openslr.orgresources00trainclean000.tar.gz$^        $^        trainclean000url  $^            httpwww.openslr.orgresources00trainclean000.tar.gz$^        $^        trainother000url  $^            httpwww.openslr.orgresources00trainother000.tar.gz$^        $^$^        devcleanurl  httpwww.openslr.orgresources00devclean.tar.gz$^        devotherurl  httpwww.openslr.orgresources00devother.tar.gz$^$^        testcleanurl  httpwww.openslr.orgresources00testclean.tar.gz$^        testotherurl  httpwww.openslr.orgresources00testother.tar.gz$^$^        def filenameofx$^            return os.path.splitx0$^$^        trainclean000  maybedownload$^            filenameoftrainclean000url datadir trainclean000url$^        $^        bar.update0$^        trainclean000  maybedownload$^            filenameoftrainclean000url datadir trainclean000url$^        $^        bar.update0$^        trainother000  maybedownload$^            filenameoftrainother000url datadir trainother000url$^        $^        bar.update0$^$^        devclean  maybedownloadfilenameofdevcleanurl datadir devcleanurl$^        bar.update0$^        devother  maybedownloadfilenameofdevotherurl datadir devotherurl$^        bar.update0$^$^        testclean  maybedownload$^            filenameoftestcleanurl datadir testcleanurl$^        $^        bar.update0$^        testother  maybedownload$^            filenameoftestotherurl datadir testotherurl$^        $^        bar.update0$^$^     conditionally extract librispeech data$^     we extract each archive into datadir but test for existence in$^     datadirlibrispeech because the archives share that root.$^    printextracting librivox data if not already extracted...$^    with progressbar.progressbarmaxvalue0 widgetprogressbar.adaptiveeta as bar$^        librivoxdir  librispeech$^        workdir  os.path.joindatadir librivoxdir$^$^        maybeextract$^            datadir os.path.joinlibrivoxdir trainclean000 trainclean000$^        $^        bar.update0$^        maybeextract$^            datadir os.path.joinlibrivoxdir trainclean000 trainclean000$^        $^        bar.update0$^        maybeextract$^            datadir os.path.joinlibrivoxdir trainother000 trainother000$^        $^        bar.update0$^$^        maybeextractdatadir os.path.joinlibrivoxdir devclean devclean$^        bar.update0$^        maybeextractdatadir os.path.joinlibrivoxdir devother devother$^        bar.update0$^$^        maybeextractdatadir os.path.joinlibrivoxdir testclean testclean$^        bar.update0$^        maybeextractdatadir os.path.joinlibrivoxdir testother testother$^        bar.update0$^$^     convert flac data to wav from$^      datadirlibrispeechsplit00000.flac$^     to$^      datadirlibrispeechsplitwav000.wav$^    $^     and split librispeech transcriptions from$^      datadirlibrispeechsplit0000.trans.txt$^     to$^      datadirlibrispeechsplitwav000.txt$^      datadirlibrispeechsplitwav000.txt$^      datadirlibrispeechsplitwav000.txt$^      ...$^    printconverting flac to wav and splitting transcriptions...$^    with progressbar.progressbarmaxvalue0 widgetprogressbar.adaptiveeta as bar$^        train000  convertaudioandsplitsentences$^            workdir trainclean000 trainclean000wav$^        $^        bar.update0$^        train000  convertaudioandsplitsentences$^            workdir trainclean000 trainclean000wav$^        $^        bar.update0$^        train000  convertaudioandsplitsentences$^            workdir trainother000 trainother000wav$^        $^        bar.update0$^$^        devclean  convertaudioandsplitsentences$^            workdir devclean devcleanwav$^        $^        bar.update0$^        devother  convertaudioandsplitsentences$^            workdir devother devotherwav$^        $^        bar.update0$^$^        testclean  convertaudioandsplitsentences$^            workdir testclean testcleanwav$^        $^        bar.update0$^        testother  convertaudioandsplitsentences$^            workdir testother testotherwav$^        $^        bar.update0$^$^     write sets to disk as csv files$^    train000.tocsv$^        os.path.joindatadir librivoxtrainclean000.csv indexfalse$^    $^    train000.tocsv$^        os.path.joindatadir librivoxtrainclean000.csv indexfalse$^    $^    train000.tocsv$^        os.path.joindatadir librivoxtrainother000.csv indexfalse$^    $^$^    devclean.tocsvos.path.joindatadir librivoxdevclean.csv indexfalse$^    devother.tocsvos.path.joindatadir librivoxdevother.csv indexfalse$^$^    testclean.tocsvos.path.joindatadir librivoxtestclean.csv indexfalse$^    testother.tocsvos.path.joindatadir librivoxtestother.csv indexfalse$^$^$^def maybeextractdatadir extracteddata archive$^     if datadirextracteddata does not exist extract archive in datadir$^    if not gfile.existsos.path.joindatadir extracteddata$^        tar  tarfile.openarchive$^        tar.extractalldatadir$^        tar.close$^$^$^def convertaudioandsplitsentencesextracteddir dataset destdir$^    sourcedir  os.path.joinextracteddir dataset$^    targetdir  os.path.joinextracteddir destdir$^$^    if not os.path.existstargetdir$^        os.makedirstargetdir$^$^     loop over transcription files and split each one$^    $^     the format for each file 00.trans.txt is$^      000 transcription of 000.flac$^      000 transcription of 000.flac$^      ...$^    $^     each file is then split into several files$^      000.txt contains transcription of 000.flac$^      000.txt contains transcription of 000.flac$^      ...$^    $^     we also convert the corresponding flacs to wav in the same pass$^    files  $^    for root dirnames filenames in os.walksourcedir$^        for filename in fnmatch.filterfilenames .trans.txt$^            transfilename  os.path.joinroot filename$^            with codecs.opentransfilename r utf0 as fin$^                for line in fin$^                     parse each segment line$^                    firstspace  line.find $^                    seqid transcript  linefirstspace linefirstspace  0 $^$^                     we need to do the encodedecode dance here because encode$^                     returns a bytes object on python 0 and texttochararray$^                     expects a string.$^                    transcript  $^                        unicodedata.normalizenfkd transcript$^                        .encodeascii ignore$^                        .decodeascii ignore$^                    $^$^                    transcript  transcript.lower.strip$^$^                     convert corresponding flac to a wav$^                    flacfile  os.path.joinroot seqid  .flac$^                    wavfile  os.path.jointargetdir seqid  .wav$^                    if not os.path.existswavfile$^                        tfm  transformer$^                        tfm.setoutputformatratesamplerate$^                        tfm.buildflacfile wavfile$^                    wavfilesize  os.path.getsizewavfile$^$^                    files.appendos.path.abspathwavfile wavfilesize transcript$^$^    return pandas.dataframe$^        datafiles columnswavfilename wavfilesize transcript$^    $^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^$^importer for dataset published from centre de confrence pierre mendsfrance$^ministre de lconomie des finances et de la relance$^$^$^import csv$^import sys$^import os$^import progressbar$^import subprocess$^import zipfile$^from glob import glob$^from multiprocessing import pool$^$^import hashlib$^import decimal$^import math$^import unicodedata$^import re$^import sox$^import xml.etree.elementtree as et$^$^try$^    from num0words import num0words$^except importerror as ex$^    printpip install num0words$^    sys.exit0$^$^import requests$^import json$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.helpers import secstohours$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportersparser$^    getimportedsamples$^    getvalidatelabel$^    printimportreport$^$^from dsctcdecoder import alphabet$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^channels  0$^bitdepth  00$^maxsecs  00$^minsecs  0.00$^$^datasetreleasecsv  httpsdata.economie.gouv.frexploredatasettranscriptionsxmlaudiomp0mefrccpmf00000000downloadformatcsvtimezoneeuropeberlinlangfruselabelsforheadertruecsvseparator0b$^datasetreleasesha  $^    000d00a00a000c0000c0ff0f0000b000f00f0b00 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0f0a0000aa00c00000bb00b0a0e000e00dbf00e0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0e00e0f0f000000000000ac000000e0a0d0fe0f0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0bf00000cf00000ca0000e00a0bd0fa0000c00ae transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    c0000000aadc000ac00f0af00000a0bb0000b00f transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    d00e000e000000d00ce0e0000fd000d0d000e000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    de0ed0c0b0ee00ca000aae0ba0000cc00000d000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    000000c00dacfcd0000d000c00c00f0e000fc0f0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0e0b00a000000bb00f0cd00000eaba000a0d00a0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0000a00000000c0af0e0000d00bdacb000e0b0b0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00000e0000000d00ef0bd00bf0f0c0a00f00baff transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00c0be0b0ca0d0000d000da0a00e00d00a00dbac transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00000000f000a000c0ebc000000fe00a0e0cd000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0ab0c0e000e0000d0000f000e000000c0a0c00e0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0f00df000ef00dce0d0ab0e00000000a0d0c00d0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    e00bfb000000c000cb00000ef00b0b0b00000aa0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0f000ba000ee000000cf000b000c00b00ca00c0e transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    b0aa00a000000000000000000c0e0000d0c0bae0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00ddcf00c0bf000a0f0000b000c0ec00a00a000a transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    fa0b00000dd00b0a0000000a0f0ae00000b000d0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0000aef0f0e0be0f0fbf0d00b0c000c0c0e0000f transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    ce0000d0d0b0b0000ba000f00e0a00d0d000c000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    d0000ed000ac00fcf0000d0ea000000c00b00000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    ec000cd0af000f00d0bf0d0b0f00000000ff0e0c transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    000d0e0e00000e00fd000000d0b0aaecda0fd0fb transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    c00ccc0000a00b0cae0d0f0e0fbbb0ab000cb0ac transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00000a00000d00e000000ce000c000a0e00efa00 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    000aedad000a00ea0e0d00def0c00c0fd0000e00 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    db000a000f00fb0f00f000bba00c0000de0f000a transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    e0c0000f00c0c0d00a00dcb0f00a000000a00000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    e00b0bb0c0ae0000f00b0f00e0d00b000000f0ac transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    be0e00cbc00b00b00ae00c00000ef0e0a000d00e transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    000df00e0ff00fcfd00b00dab00000dc000000b0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0a000000000a0cdcb0d00a0f0dbee00be0e00000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    000d000e0ee00000000000c0c00000d000a0000b transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00f000b0b000d000eb00000c00cfff0000b0d000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0e0d0000000a0a00a00e0d0b00c0fd00b0fb0000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    00000000000cb000a00000cb0b0c000c0000a000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    0b0cbb0000000be000000f0a00000e000a00000a transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^    c0e0e000a0e0a0f0ff0dbfcefe000aa00a00d0f0 transcriptionsxmlaudiomp0mefrccpmf000000000.zip.000$^$^$^def downloadandpreprocessdatacsvurl targetdir$^    datasetsources  os.path.jointargetdir transcriptionsxmlaudiomp0mefrccpmf00000000 data.txt$^    if os.path.existsdatasetsources$^        return datasetsources$^$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^    csvref  requests.getcsvurl.text.splitrn00$^    for part in csvref$^        partfilename  requests.headpart.headers.getcontentdisposition.split 0.split0.replace $^        if not os.path.existsos.path.jointargetdir partfilename$^            partpath  maybedownloadpartfilename targetdir part$^$^    def bigsha0fname$^        s  hashlib.sha0$^        buffersize  00000$^        with openfname rb as f$^            while true$^                data  f.readbuffersize$^                if not data$^                    break$^                s.updatedata$^        return s.hexdigest$^$^    for sha0 filename in datasetreleasesha$^        printchecking  sha0.formatfilename$^        csum  bigsha0os.path.jointargetdir filename$^        if csum  sha0$^            printt ok .formatfilename sha0$^        else$^            printt error expected  computed .formatfilename sha0 csum$^        assert csum  sha0$^$^     conditionally extract data$^    maybeextracttargetdir transcriptionsxmlaudiomp0mefrccpmf00000000 transcriptionsxmlaudiomp0mefrccpmf000000000.zip transcriptionsxmlaudiomp0mefrccpmf00000000.zip$^$^     produce source text for extraction  conversion$^    return maybecreatesourcesos.path.jointargetdir transcriptionsxmlaudiomp0mefrccpmf00000000$^$^def maybeextracttargetdir extracteddata archive final$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.path.jointargetdir extracteddata$^    archivepath  os.path.jointargetdir archive$^    finalarchive  os.path.joinextractedpath final$^$^    if not os.path.existsextractedpath$^        if not os.path.existsarchivepath$^            printno archive s  building ...  archivepath$^            allzipparts  globarchivepath  .$^            allzipparts.sort$^            cmdline  cat   .format .joinallzipparts archivepath$^            printbuilding with s  cmdline$^            subprocess.checkcallcmdline shelltrue cwdtargetdir$^            assert os.path.existsarchivepath$^$^        printno directory s  extracting archive s ...  extractedpath archivepath$^        with zipfile.zipfilearchivepath as zipf$^            zipf.extractallextractedpath$^$^        with zipfile.zipfilefinalarchive as zipf$^            zipf.extractalltargetdir$^    else$^        printfound directory s  not extracting it from archive.  extractedpath$^$^def maybecreatesourcesdir$^    datasetsources  os.path.joindir data.txt$^    mp0  globos.path.joindir  .mp0$^    xml  globos.path.joindir  .xml$^$^    mp0xmlscores  $^    mp0xmlfin  $^$^    for fmp0 in mp0$^        for fxml in xml$^            bmp0  os.path.splitextos.path.basenamefmp00$^            bxml  os.path.splitextos.path.basenamefxml0$^            amp0  bmp0.split$^            axml  bxml.split$^            score  0$^            datemp0  amp00$^            datexml  axml0$^$^            if datemp0  datexml$^                continue$^$^            for i in rangeminlenamp0 lenaxml$^                if amp0i  axmli$^                    score  0$^$^            if score  0$^                mp0xmlscores.appendfmp0 fxml score$^$^     sort by score$^    mp0xmlscores.sortkeylambda x x0 reversetrue$^    for smp0 sxml score in mp0xmlscores$^        printsmp0 sxml score$^        if score not in mp0xmlfin$^            mp0xmlfinscore  $^$^        if smp0 not in mp0xmlfinscore$^            try$^                mp0.indexsmp0$^                mp0.removesmp0$^                mp0xmlfinscoresmp0  sxml$^            except valueerror as ex$^                pass$^        else$^            printhere mp0xmlfinscoresmp0 sxml filesys.stderr$^$^    with opendatasetsources w as ds$^        for score in mp0xmlfin$^            for mp0 in mp0xmlfinscore$^                xml  mp0xmlfinscoremp0$^                if os.path.getsizemp0  0 and os.path.getsizexml  0$^                    mp0  os.path.relpathmp0 dir$^                    xml  os.path.relpathxml dir$^                    ds.write0.0en.formatxml mp0 0.0e0$^                else$^                    printempty file  or .formatmp0 xml filesys.stderr$^$^    printmissing xml pairs mp0 filesys.stderr$^    return datasetsources$^$^def maybenormalizefordigitslabel$^     first try to identify numbers like 00 000 000 000$^    if   in label$^        if anys.isdigit for s in label$^            thousands  re.compilerd00sd0d$^            maybethousands  thousands.findalllabel$^            if lenmaybethousands  0$^                while true$^                    label r  re.subnrdsd0 00 label$^                    if r  0$^                        break$^$^     this might be a time or duration in the form hhmm or hhmmss$^    if  in label$^        for s in label.split $^            if anyi.isdigit for i in s$^                dateortime  re.compilerd00d0d0$^                maybedateortime  dateortime.findalls$^                if lenmaybedateortime  0$^                    maybehours    maybedateortime00$^                    maybeminutes  maybedateortime00$^                    maybeseconds  maybedateortime00$^                    if lenmaybeseconds  0$^                        label  label.replace.formatmaybehours maybeminutes maybeseconds  heures  minutes et  secondes.formatmaybehours maybeminutes maybeseconds$^                    else$^                        label  label.replace.formatmaybehours maybeminutes  heures et  minutes.formatmaybehours maybeminutes$^$^    newlabel  $^     pylint disabletoomanynestedblocks$^    for s in label.split $^        if anyi.isdigit for i in s$^            s  s.replace .  num0words requires . for floats$^            s  s.replace    clean some data num0words would choke on 0000$^$^            lastc  s0$^            if not lastc.isdigit  num0words will choke on 0.0. 00 $^                s  s0$^$^            if anyi.isalpha for i in s  so we have anyisdigit and anysialpha like 0d$^                ns  $^                for c in s$^                    nc  c$^                    if c.isdigit  convert 0 to trois$^                        try$^                            nc  num0wordsc langfr  $^                        except decimal.invalidoperation as ex$^                            printdecimal.invalidoperation .formats$^                            raise ex$^                    ns.appendnc$^                s  .joins$^            else$^                try$^                    s  num0wordss langfr$^                except decimal.invalidoperation as ex$^                    printdecimal.invalidoperation .formats$^                    raise ex$^        newlabel.appends$^    return  .joinnewlabel$^$^def maybenormalizeforspecialscharslabel$^    label  label.replace pourcents$^    label  label.replace    clean intervals like 00000000 to 0000 0000$^    label  label.replace    clean intervals like 0000 to 00 00$^    label  label.replace  plus   clean  and make it speakable$^    label  label.replace  euros   clean euro symbol and make it speakable$^    label  label.replace.     clean some strange 0.0.  00000000innovation.xml$^    label  label.replace  degr   clean some strange 0 00000000etatsgeneraux0000fre000und.xml$^    label  label.replace... .  remove ellipsis$^    label  label.replace.. .  remove broken ellipsis$^    label  label.replacem mtrecarrs  00000000deficlimat0wmv0freminefi.xml$^    label  label.replaceend   broken tag in 00000000entretienstresorpgmwmv0freminefi.xml$^    label  label.replaceuxb0c    strange cedilla in 00000000printempseconomie0wmv0freminefi.xml$^    label  label.replacec0 co 0  00000000sytemesantecopiewmv0freminefi.xml$^    return label$^$^def maybenormalizeforanglicismslabel$^    label  label.replaceb0b b to b$^    label  label.replaceb0c b to c$^    label  label.replace hashtag $^    label  label.replace at $^    return label$^$^def maybenormalizelabel$^    label  maybenormalizeforspecialscharslabel$^    label  maybenormalizeforanglicismslabel$^    label  maybenormalizefordigitslabel$^    return label$^$^def onesamplesample$^    filesize  0$^    frames  0$^$^    audiosource  sample0$^    targetdir  sample0$^    datasetbasename  sample0$^$^    starttime  sample0$^    duration  sample0$^    label  labelfilterfunsample0$^    sampleid  sample0$^$^    wavfilename  os.path.basenameaudiosource.replace.wav 00.wav.formatsampleid$^    wavfullname  os.path.jointargetdir datasetbasename wavfilename$^$^    if not os.path.existswavfullname$^        subprocess.checkoutputffmpeg i audiosource ss strstarttime t strduration c copy wavfullname stdinsubprocess.devnull stderrsubprocess.stdout$^$^    filesize  os.path.getsizewavfullname$^    frames  intsubprocess.checkoutputsoxi s wavfullname stderrsubprocess.stdout$^$^    counter  getcounter$^    rows  $^$^    if filesize  0$^         excluding samples that failed upon conversion$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframessamplerate0000000  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif framessamplerate  minsecs$^         excluding samples that are too short$^        countertooshort  0$^    elif framessamplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendos.path.joindatasetbasename wavfilename filesize label$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^$^    return counter rows$^$^def maybeimportdataxmlfile audiosource targetdir reltol0e0$^    datasetbasename  os.path.splitextos.path.splitxmlfile00$^    wavroot  os.path.jointargetdir datasetbasename$^    if not os.path.existswavroot$^        os.makedirswavroot$^$^    sourceframes  intsubprocess.checkoutputsoxi s audiosource stderrsubprocess.stdout$^    printsource audio length s  secstohourssourceframes  samplerate$^$^     get audiofile path and transcript for each sentence in tsv$^    samples  $^    tree  et.parsexmlfile$^    root  tree.getroot$^    seqid         0$^    thistime      0.0$^    thisduration  0.0$^    prevtime      0.0$^    prevduration  0.0$^    thistext      $^    for child in root$^        if child.tag  row$^            curtime      floatchild.attribtimestamp$^            curduration  floatchild.attribtimedur$^            curtext      child.text$^$^            if thistime  0.0$^                thistime  curtime$^$^            delta     curtime  prevtime  prevduration$^             reltol value is made from trialerror to try and compromise between$^              cutting enough to skip missing words$^              not too short not too long sentences$^            isclose  math.isclosecurtime thistime  thisduration reltolreltol$^            isshort  thisduration  curduration  delta  maxsecs$^$^             when the previous element is close enough and this does not$^             go over maxsecs we append content$^            if isclose and isshort$^                thisduration  curduration  delta$^                thistext      curtext$^            else$^                samples.appendaudiosource targetdir datasetbasename thistime thisduration thistext seqid$^$^                thistime      curtime$^                thisduration  curduration$^                thistext      curtext$^$^                seqid  0$^$^            prevtime      curtime$^            prevduration  curduration$^$^     keep track of how many samples are good vs. problematic$^    counter  getcounter$^    numsamples  lensamples$^    rows  $^$^    printprocessing xml data .formatxmlfile$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i processed in enumeratepool.imapunorderedonesample samples start0$^        counter  processed0$^        rows  processed0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    importedsamples  getimportedsamplescounter$^    assert counterall  numsamples$^    assert lenrows  importedsamples$^$^    printimportreportcounter samplerate maxsecs$^    printimport efficiency .0f  countertotaltime  sourceframes000$^    print$^$^    return counter rows$^$^def maybeconvertwavmp0filename wavfilename$^    if not os.path.existswavfilename$^        printconverting  to wav file .formatmp0filename wavfilename$^        transformer  sox.transformer$^        transformer.convertsampleratesamplerate nchannelschannels bitdepthbitdepth$^        try$^            transformer.buildmp0filename wavfilename$^        except sox.core.soxerror$^            pass$^$^def writegeneralcsvtargetdir rows counter$^    targetcsvtemplate  os.path.jointargetdir ccpmf.csv$^    with opentargetcsvtemplate.formattrain w as traincsvfile   00$^        with opentargetcsvtemplate.formatdev w as devcsvfile   00$^            with opentargetcsvtemplate.formattest w as testcsvfile   00$^                trainwriter  csv.dictwritertraincsvfile fieldnamesfieldnames$^                trainwriter.writeheader$^                devwriter  csv.dictwriterdevcsvfile fieldnamesfieldnames$^                devwriter.writeheader$^                testwriter  csv.dictwritertestcsvfile fieldnamesfieldnames$^                testwriter.writeheader$^$^                bar  progressbar.progressbarmaxvaluelenrows widgetssimplebar$^                for i item in enumeratebarrows$^                    imod  i  00$^                    if imod  0$^                        writer  testwriter$^                    elif imod  0$^                        writer  devwriter$^                    else$^                        writer  trainwriter$^                    writer.writerowwavfilename item0 wavfilesize item0 transcript item0$^$^    print$^    print final statistics $^    printimportreportcounter samplerate maxsecs$^    print final statistics $^    print$^$^if name  main$^    parser  getimportersparserdescriptionimport xml from conference centre for economics france$^    parser.addargumenttargetdir helpdestination directory$^    parser.addargumentfilteralphabet helpexclude samples with characters not in provided alphabet$^    parser.addargumentnormalize actionstoretrue helpconverts diacritic characters to their base ones$^$^    params  parser.parseargs$^    validatelabel  getvalidatelabelparams$^    alphabet  alphabetparams.filteralphabet if params.filteralphabet else none$^$^    def labelfilterfunlabel$^        if params.normalize$^            label  unicodedata.normalizenfkd label.strip $^                .encodeascii ignore $^                .decodeascii ignore$^        label  maybenormalizelabel$^        label  validatelabellabel$^        if alphabet and label$^            try$^                alphabet.encodelabel$^            except keyerror$^                label  none$^        return label$^$^    datasetsources  downloadandpreprocessdatacsvurldatasetreleasecsv targetdirparams.targetdir$^    sourcesrootdir  os.path.dirnamedatasetsources$^    allcounter  getcounter$^    allrows  $^    with opendatasetsources r as sources$^        for line in sources.readlines$^            d  line.split$^            thisxml  os.path.joinsourcesrootdir d0$^            thismp0  os.path.joinsourcesrootdir d0$^            thisrel  floatd0$^$^            wavfilename  os.path.joinsourcesrootdir os.path.splitextos.path.basenamethismp00  .wav$^            maybeconvertwavthismp0 wavfilename$^            counter rows  maybeimportdatathisxml wavfilename sourcesrootdir thisrel$^$^            allcounter  counter$^            allrows  rows$^    writegeneralcsvsourcesrootdir counterallcounter rowsallrows$^usrbinenv python$^$^downloads and prepares parts of the german distant speech corpus tuda for deepspeech.py$^use python0 importtuda.py h for help$^$^import argparse$^import csv$^import os$^import tarfile$^import unicodedata$^import wave$^import xml.etree.elementtree as et$^from collections import counter$^$^import progressbar$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import validatelabeleng as validatelabel$^from dsctcdecoder import alphabet$^$^tudaversion  v0$^tudapackage  germanspeechdatapackage.formattudaversion$^tudaurl  httpltdata0.informatik.unihamburg.dekalditudade.tar.gz.format$^    tudapackage$^$^tudaarchive  .tar.gz.formattudapackage$^$^channels  0$^samplewidth  0$^samplerate  00000$^$^fieldnames  wavfilename wavfilesize transcript$^$^$^def maybeextractarchive$^    extracted  os.path.joincliargs.basedir tudapackage$^    if os.path.isdirextracted$^        printfound directory   not extracting..formatextracted$^    else$^        printextracting ....formatarchive$^        with tarfile.openarchive as tar$^            members  tar.getmembers$^            bar  progressbar.progressbarmaxvaluelenmembers widgetssimplebar$^            for member in barmembers$^                tar.extractmembermember pathcliargs.basedir$^    return extracted$^$^$^def inalphabetc$^    return alphabet.canencodec if alphabet else true$^$^$^def checkandpreparesentencesentence$^    sentence  sentence.lower.replaceco0 c o zwei$^    chars  $^    for c in sentence$^        if cliargs.normalize and c not in  and not inalphabetc$^            c  unicodedata.normalizenfkd c.encodeascii ignore.decodeascii ignore$^        for sc in c$^            if not inalphabetc$^                return none$^            chars.appendsc$^    return validatelabel.joinchars$^$^$^def checkwavfilewavpath sentence   pylint disabletoomanyreturnstatements$^    try$^        with wave.openwavpath r as srcwavfile$^            rate  srcwavfile.getframerate$^            channels  srcwavfile.getnchannels$^            samplewidth  srcwavfile.getsampwidth$^            milliseconds  intsrcwavfile.getnframes  0000  rate$^        if rate  samplerate$^            return false wrong sample rate$^        if channels  channels$^            return false wrong number of channels$^        if samplewidth  samplewidth$^            return false wrong sample width$^        if milliseconds  lensentence  00$^            return false too short$^        if milliseconds  cliargs.maxduration  0$^            return false too long$^    except wave.error$^        return false invalid wav file$^    except eoferror$^        return false premature eof$^    return true ok$^$^$^def writecsvsextracted$^    samplecounter  0$^    reasons  counter$^    for subset in train dev test$^        setpath  os.path.joinextracted subset$^        setfiles  os.listdirsetpath$^        recordings  $^        for file in setfiles$^            if file.endswith.xml$^                recordingsfile0  $^        for file in setfiles$^            if file.endswith.wav and  in file$^                prefix  file.split0$^                if prefix in recordings$^                    recordingsprefix.appendfile$^        recordings  recordings.items$^        csvpath  os.path.join$^            cliargs.basedir tuda.csv.formattudaversion subset$^        $^        printwriting ....formatcsvpath$^        with opencsvpath w encodingutf0 newline as csvfile$^            writer  csv.dictwritercsvfile fieldnamesfieldnames$^            writer.writeheader$^            setdir  os.path.joinextracted subset$^            bar  progressbar.progressbarmaxvaluelenrecordings widgetssimplebar$^            for prefix wavnames in barrecordings$^                xmlpath  os.path.joinsetdir prefix  .xml$^                meta  et.parsexmlpath.getroot$^                sentence  listmeta.itercleanedsentence0.text$^                sentence  checkandpreparesentencesentence$^                if sentence is none$^                    reasonsalphabet filter  0$^                    continue$^                for wavname in wavnames$^                    samplecounter  0$^                    wavpath  os.path.joinsetpath wavname$^                    keep reason  checkwavfilewavpath sentence$^                    if keep$^                        writer.writerow$^                            $^                                wavfilename os.path.relpath$^                                    wavpath cliargs.basedir$^                                $^                                wavfilesize os.path.getsizewavpath$^                                transcript sentence.lower$^                            $^                        $^                    else$^                        reasonsreason  0$^    if lenreasons.keys  0$^        printexcluded samples$^        for reason n in reasons.mostcommon$^            print    .0f.formatreason n n  000  samplecounter$^$^$^def cleanuparchive$^    if not cliargs.keeparchive$^        printremoving archive ....formatarchive$^        os.removearchive$^$^$^def downloadandprepare$^    archive  maybedownloadtudaarchive cliargs.basedir tudaurl$^    extracted  maybeextractarchive$^    writecsvsextracted$^    cleanuparchive$^$^$^def handleargs$^    parser  argparse.argumentparserdescriptionimport german distant speech tuda$^    parser.addargumentbasedir helpdirectory containing all data$^    parser.addargument$^        maxduration$^        typeint$^        default00000$^        helpmaximum sample duration in milliseconds$^    $^    parser.addargument$^        normalize$^        actionstoretrue$^        helpconverts diacritic characters to their base ones$^    $^    parser.addargument$^        alphabet$^        helpexclude samples with characters not in provided alphabet file$^    $^    parser.addargument$^        keeparchive$^        typebool$^        defaulttrue$^        helpif downloaded archives should be kept$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    alphabet  alphabetcliargs.alphabet if cliargs.alphabet else none$^    downloadandprepare$^usrbinenv python$^import glob$^import os$^import tarfile$^import wave$^$^import pandas$^$^from deepspeechtraining.util.importers import getimportersparser$^$^columnnames  wavfilename wavfilesize transcript$^$^$^def extractarchivepath targetdir$^    printextracting  into ....formatarchivepath targetdir$^    with tarfile.openarchivepath as tar$^        tar.extractalltargetdir$^$^$^def isfiletruncatedwavfilename wavfilesize$^    with wave.openwavfilename moderb as fin$^        assert fin.getframerate  00000$^        assert fin.getsampwidth  0$^        assert fin.getnchannels  0$^$^        headerduration  fin.getnframes  fin.getframerate$^        filesizeduration  wavfilesize  00  00000  0$^$^    return headerduration  filesizeduration$^$^$^def preprocessdatafolderwitharchives targetdir$^     first extract subset archives$^    for subset in train dev test$^        extract$^            os.path.join$^                folderwitharchives magicdataset.tar.gz.formatsubset$^            $^            targetdir$^        $^$^     folder structure is now$^      magicdatatraindevtest.tar.gz$^      magicdata$^        train.wav$^        traintrans.txt$^        dev.wav$^        devtrans.txt$^        test.wav$^        testtrans.txt$^$^     the trans files are csvs with three columns one containing the wav file$^     name one containing the speaker id and one containing the transcription$^$^    def loadsetsetpath$^        transcripts  pandas.readcsv$^            os.path.joinsetpath trans.txt sept indexcol0$^        $^        globpath  os.path.joinsetpath  .wav$^        setfiles  $^        for wav in glob.globglobpath$^            try$^                wavfilename  wav$^                wavfilesize  os.path.getsizewav$^                transcriptkey  os.path.basenamewav$^                transcript  transcripts.loctranscriptkey transcription$^$^                 some files in this dataset are truncated the header duration$^                 doesnt match the file size. this causes errors at training$^                 time so check here if things are fine before including a file$^                if isfiletruncatedwavfilename wavfilesize$^                    print$^                        warning file  is corrupted header duration does $^                        not match file size. ignoring..formatwavfilename$^                    $^                    continue$^$^                setfiles.appendwavfilename wavfilesize transcript$^            except keyerror$^                printwarning missing transcript for wav file ..formatwav$^        return setfiles$^$^    for subset in train dev test$^        printloading  set samples....formatsubset$^        subsetfiles  loadsetos.path.jointargetdir subset$^        df  pandas.dataframedatasubsetfiles columnscolumnnames$^$^         trim train set to under 00s$^        if subset  train$^            durations  dfwavfilesize  00  00000  0$^            df  dfdurations  00.0$^            printtrimming  samples  00 seconds.formatdurations  00.0.sum$^$^            withnoise  dftranscript.str.containsrfilspk$^            df  dfwithnoise$^            print$^                trimming  samples with noise fil or spk.format$^                    sumwithnoise$^                $^            $^$^        destcsv  os.path.jointargetdir magicdata.csv.formatsubset$^        printsaving  set into ....formatsubset destcsv$^        df.tocsvdestcsv indexfalse$^$^$^def main$^     httpsopenslr.org00$^    parser  getimportersparserdescriptionimport magicdata corpus$^    parser.addargument$^        folderwitharchives$^        helppath to folder containing magicdatatraindevtest.tar.gz$^    $^    parser.addargument$^        targetdir$^        default$^        helptarget folder to extract files into and put the resulting csvs. defaults to a folder called magicdata next to the archives$^    $^    params  parser.parseargs$^$^    if not params.targetdir$^        params.targetdir  os.path.joinparams.folderwitharchives magicdata$^$^    preprocessdataparams.folderwitharchives params.targetdir$^$^$^if name  main$^    main$^usrbinenv python0$^ pylint disableinvalidname$^import csv$^import os$^import subprocess$^import tarfile$^import unicodedata$^from glob import glob$^from multiprocessing import pool$^$^import progressbar$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    getimportersparser$^    getvalidatelabel$^    printimportreport$^$^from dsctcdecoder import alphabet$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^maxsecs  00$^$^archivedirname  language$^archivename  language.tgz$^archiveurl  httpsdata.solak.dedatatrainingstttts  archivename$^$^$^def downloadandpreprocessdatatargetdir$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^     conditionally download data$^    archivepath  maybedownloadarchivename targetdir archiveurl$^     conditionally extract data$^    maybeextracttargetdir archivedirname archivepath$^     produce csv files$^    maybeconvertsetstargetdir archivedirname$^$^$^def maybeextracttargetdir extracteddata archivepath$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.path.jointargetdir extracteddata$^    if not os.path.existsextractedpath$^        printno directory s  extracting archive...  extractedpath$^        if not os.path.isdirextractedpath$^            os.mkdirextractedpath$^        tar  tarfile.openarchivepath$^        tar.extractallextractedpath$^        tar.close$^    else$^        printfound directory s  not extracting it from archive.  archivepath$^$^$^def onesamplesample$^     take a audio file and optionally convert it to 00khz wav $^    wavfilename  sample0$^    filesize  0$^    frames  0$^    if os.path.existswavfilename$^        tmpfilename  os.path.splitextwavfilename0.tmp.wav$^        subprocess.checkcall$^            sox wavfilename r strsamplerate c 0 b 00 tmpfilename stderrsubprocess.stdout$^        $^        os.renametmpfilename wavfilename$^        filesize  os.path.getsizewavfilename$^        frames  int$^            subprocess.checkoutput$^                soxi s wavfilename stderrsubprocess.stdout$^            $^        $^    label  labelfiltersample0$^    counter  getcounter$^    rows  $^$^    if filesize  0$^         excluding samples that failed upon conversion$^        printconversion failure wavfilename$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframes  samplerate  0000  00  0  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif frames  samplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendwavfilename filesize label$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^    return counter rows$^$^$^def maybeconvertsetstargetdir extracteddata$^    extracteddir  os.path.jointargetdir extracteddata$^     override existing csv with normalized one$^    targetcsvtemplate  os.path.join$^        targetdir archivedirname archivename.replace.tgz .csv$^    $^    if os.path.isfiletargetcsvtemplate$^        return$^$^    wavrootdir  os.path.joinextracteddir$^$^     get audiofile path and transcript for each sentence in tsv$^    samples  $^    globdir  os.path.joinwavrootdir metadata.csv$^    for record in globglobdir recursivetrue$^        if any$^            maplambda sk sk in record skiplist$^           pylint disablecellvarfromloop$^            continue$^        with openrecord r as rec$^            for re in rec.readlines$^                re  re.strip.split$^                audio  os.path.joinos.path.dirnamerecord wavs re0  .wav$^                transcript  re0$^                samples.appendaudio transcript$^$^    counter  getcounter$^    numsamples  lensamples$^    rows  $^$^    printimporting wav files...$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i processed in enumeratepool.imapunorderedonesample samples start0$^        counter  processed0$^        rows  processed0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    with opentargetcsvtemplate.formattrain w encodingutf0 newline as traincsvfile   00$^        with opentargetcsvtemplate.formatdev w encodingutf0 newline as devcsvfile   00$^            with opentargetcsvtemplate.formattest w encodingutf0 newline as testcsvfile   00$^                trainwriter  csv.dictwritertraincsvfile fieldnamesfieldnames$^                trainwriter.writeheader$^                devwriter  csv.dictwriterdevcsvfile fieldnamesfieldnames$^                devwriter.writeheader$^                testwriter  csv.dictwritertestcsvfile fieldnamesfieldnames$^                testwriter.writeheader$^$^                for i item in enumeraterows$^                    transcript  validatelabelitem0$^                    if not transcript$^                        continue$^                    wavfilename  item0$^                    imod  i  00$^                    if imod  0$^                        writer  testwriter$^                    elif imod  0$^                        writer  devwriter$^                    else$^                        writer  trainwriter$^                    writer.writerow$^                        dict$^                            wavfilenameos.path.relpathwavfilename extracteddir$^                            wavfilesizeos.path.getsizewavfilename$^                            transcripttranscript$^                        $^                    $^$^    importedsamples  getimportedsamplescounter$^    assert counterall  numsamples$^    assert lenrows  importedsamples$^$^    printimportreportcounter samplerate maxsecs$^$^$^def handleargs$^    parser  getimportersparser$^        descriptionimporter for mailabs dataset. httpswww.caito.de000000themailabsspeechdataset.$^    $^    parser.addargumentdesttargetdir$^    parser.addargument$^        filteralphabet$^        helpexclude samples with characters not in provided alphabet$^    $^    parser.addargument$^        normalize$^        actionstoretrue$^        helpconverts diacritic characters to their base ones$^    $^    parser.addargument$^        skiplist$^        typestr$^        default$^        helpdirectories  books to skip comma separated$^    $^    parser.addargument$^        language requiredtrue typestr helpdataset language to use$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    alphabet  alphabetcliargs.filteralphabet if cliargs.filteralphabet else none$^    skiplist  filternone cliargs.skiplist.split$^    validatelabel  getvalidatelabelcliargs$^$^    def labelfilterlabel$^        if cliargs.normalize$^            label  $^                unicodedata.normalizenfkd label.strip$^                .encodeascii ignore$^                .decodeascii ignore$^            $^        label  validatelabellabel$^        if alphabet and label and not alphabet.canencodelabel$^            label  none$^        return label$^$^    archivedirname  archivedirname.formatlanguagecliargs.language$^    archivename  archivename.formatlanguagecliargs.language$^    archiveurl  archiveurl.formatlanguagecliargs.language$^$^    downloadandpreprocessdatatargetdircliargs.targetdir$^usrbinenv python$^import sys$^import tarfile$^import unicodedata$^import wave$^from glob import glob$^from os import makedirs path remove rmdir$^$^import pandas$^from sox import transformer$^from tensorflow.python.platform import gfile$^$^from deepspeechtraining.util.downloader import maybedownload$^from deepspeechtraining.util.stm import parsestmfile$^$^$^def downloadandpreprocessdatadatadir$^     conditionally download data$^    teddata  tedliumrelease0.tar.gz$^    teddataurl  httpwww.openslr.orgresources00tedliumrelease0.tar.gz$^    localfile  maybedownloadteddata datadir teddataurl$^$^     conditionally extract ted data$^    teddir  tedliumrelease0$^    maybeextractdatadir teddir localfile$^$^     conditionally convert ted sph data to wav$^    maybeconvertwavdatadir teddir$^$^     conditionally split ted wav and text data into sentences$^    trainfiles devfiles testfiles  maybesplitsentencesdatadir teddir$^$^     write sets to disk as csv files$^    trainfiles.tocsvpath.joindatadir tedtrain.csv indexfalse$^    devfiles.tocsvpath.joindatadir teddev.csv indexfalse$^    testfiles.tocsvpath.joindatadir tedtest.csv indexfalse$^$^$^def maybeextractdatadir extracteddata archive$^     if datadirextracteddata does not exist extract archive in datadir$^    if not gfile.existspath.joindatadir extracteddata$^        tar  tarfile.openarchive$^        tar.extractalldatadir$^        tar.close$^$^$^def maybeconvertwavdatadir extracteddata$^     create extracteddata dir$^    extracteddir  path.joindatadir extracteddata$^$^     conditionally convert dev sph to wav$^    maybeconvertwavdatasetextracteddir dev$^$^     conditionally convert train sph to wav$^    maybeconvertwavdatasetextracteddir train$^$^     conditionally convert test sph to wav$^    maybeconvertwavdatasetextracteddir test$^$^$^def maybeconvertwavdatasetextracteddir dataset$^     create source dir$^    sourcedir  path.joinextracteddir dataset sph$^$^     create target dir$^    targetdir  path.joinextracteddir dataset wav$^$^     conditionally convert sph files to wav files$^    if not gfile.existstargetdir$^         create targetdir$^        makedirstargetdir$^$^         loop over sph files in sourcedir and convert each to wav$^        for sphfile in globpath.joinsourcedir .sph$^            transformer  transformer$^            wavfilename  path.splitextpath.basenamesphfile0  .wav$^            wavfile  path.jointargetdir wavfilename$^            transformer.buildsphfile wavfile$^            removesphfile$^$^         remove sourcedir$^        rmdirsourcedir$^$^$^def maybesplitsentencesdatadir extracteddata$^     create extracteddata dir$^    extracteddir  path.joindatadir extracteddata$^$^     conditionally split dev wav$^    devfiles  maybesplitdatasetextracteddir dev$^$^     conditionally split train wav$^    trainfiles  maybesplitdatasetextracteddir train$^$^     conditionally split test wav$^    testfiles  maybesplitdatasetextracteddir test$^$^    return trainfiles devfiles testfiles$^$^$^def maybesplitdatasetextracteddir dataset$^     create stm dir$^    stmdir  path.joinextracteddir dataset stm$^$^     create wav dir$^    wavdir  path.joinextracteddir dataset wav$^$^    files  $^$^     loop over stm files and split corresponding wav$^    for stmfile in globpath.joinstmdir .stm$^         parse stm file$^        stmsegments  parsestmfilestmfile$^$^         open wav corresponding to stmfile$^        wavfilename  path.splitextpath.basenamestmfile0  .wav$^        wavfile  path.joinwavdir wavfilename$^        origaudio  wave.openwavfile r$^$^         loop over stmsegments and split wavfile for each segment$^        for stmsegment in stmsegments$^             create wav segment filename$^            starttime  stmsegment.starttime$^            stoptime  stmsegment.stoptime$^            newwavfilename  $^                path.splitextpath.basenamestmfile0$^                 $^                 strstarttime$^                 $^                 strstoptime$^                 .wav$^            $^            newwavfile  path.joinwavdir newwavfilename$^$^             if the wav segment filename does not exist create it$^            if not gfile.existsnewwavfile$^                splitwavorigaudio starttime stoptime newwavfile$^$^            newwavfilesize  path.getsizenewwavfile$^            files.append$^                path.abspathnewwavfile newwavfilesize stmsegment.transcript$^            $^$^         close origaudio$^        origaudio.close$^$^    return pandas.dataframe$^        datafiles columnswavfilename wavfilesize transcript$^    $^$^$^def splitwavorigaudio starttime stoptime newwavfile$^    framerate  origaudio.getframerate$^    origaudio.setposintstarttime  framerate$^    chunkdata  origaudio.readframesintstoptime  starttime  framerate$^    chunkaudio  wave.opennewwavfile w$^    chunkaudio.setnchannelsorigaudio.getnchannels$^    chunkaudio.setsampwidthorigaudio.getsampwidth$^    chunkaudio.setframerateframerate$^    chunkaudio.writeframeschunkdata$^    chunkaudio.close$^$^$^if name  main$^    downloadandpreprocessdatasys.argv0$^usrbinenv python$^  coding utf0 $^$^import sys$^$^import tensorflow.compat.v0 as tfv0$^$^$^def main$^    with tfv0.gfile.fastgfilesys.argv0 rb as fin$^        graphdef  tfv0.graphdef$^        graphdef.parsefromstringfin.read$^$^        printn.joinsortedsetn.op for n in graphdef.node$^$^$^if name  main$^    main$^usrbinenv python0$^import argparse$^import csv$^import os$^import re$^import subprocess$^import unicodedata$^import zipfile$^from glob import glob$^from multiprocessing import pool$^$^import progressbar$^import sox$^$^from deepspeechtraining.util.downloader import simplebar maybedownload$^from deepspeechtraining.util.importers import $^    getcounter$^    getimportedsamples$^    getimportersparser$^    getvalidatelabel$^    printimportreport$^$^from dsctcdecoder import alphabet$^$^fieldnames  wavfilename wavfilesize transcript$^samplerate  00000$^bitdepth  00$^nchannels  0$^maxsecs  00$^$^archivedirname  lingualibre$^archivename  qqidiso0000languageenglishname.zip$^archiveurl  httpslingualibre.frdatasets  archivename$^$^$^def downloadandpreprocessdatatargetdir$^     making path absolute$^    targetdir  os.path.abspathtargetdir$^     conditionally download data$^    archivepath  maybedownloadarchivename targetdir archiveurl$^     conditionally extract data$^    maybeextracttargetdir archivedirname archivepath$^     produce csv files and convert ogg data to wav$^    maybeconvertsetstargetdir archivedirname$^$^$^def maybeextracttargetdir extracteddata archivepath$^     if targetdirextracteddata does not exist extract archive in targetdir$^    extractedpath  os.path.jointargetdir extracteddata$^    if not os.path.existsextractedpath$^        printno directory s  extracting archive...  extractedpath$^        if not os.path.isdirextractedpath$^            os.mkdirextractedpath$^        with zipfile.zipfilearchivepath as zipf$^            zipf.extractallextractedpath$^    else$^        printfound directory s  not extracting it from archive.  archivepath$^$^$^def onesamplesample$^     take a audio file and optionally convert it to 00khz wav $^    oggfilename  sample0$^     storing wav files next to the ogg ones  just with a different suffix$^    wavfilename  os.path.splitextoggfilename0  .wav$^    maybeconvertwavoggfilename wavfilename$^    filesize  0$^    frames  0$^    if os.path.existswavfilename$^        filesize  os.path.getsizewavfilename$^        frames  int$^            subprocess.checkoutput$^                soxi s wavfilename stderrsubprocess.stdout$^            $^        $^    label  labelfiltersample0$^    rows  $^    counter  getcounter$^$^    if filesize  0$^         excluding samples that failed upon conversion$^        counterfailed  0$^    elif label is none$^         excluding samples that failed on label validation$^        counterinvalidlabel  0$^    elif intframes  samplerate  0000  00  0  lenstrlabel$^         excluding samples that are too short to fit the transcript$^        countertooshort  0$^    elif frames  samplerate  maxsecs$^         excluding very long samples to keep a reasonable batchsize$^        countertoolong  0$^    else$^         this one is good  keep it for the target csv$^        rows.appendwavfilename filesize label$^        counterimportedtime  frames$^    counterall  0$^    countertotaltime  frames$^$^    return counter rows$^$^$^def maybeconvertsetstargetdir extracteddata$^    extracteddir  os.path.jointargetdir extracteddata$^     override existing csv with normalized one$^    targetcsvtemplate  os.path.join$^        targetdir archivedirname    archivename.replace.zip .csv$^    $^    if os.path.isfiletargetcsvtemplate$^        return$^$^    oggrootdir  os.path.joinextracteddir archivename.replace.zip $^$^     get audiofile path and transcript for each sentence in tsv$^    samples  $^    globdir  os.path.joinoggrootdir .ogg$^    for record in globglobdir recursivetrue$^        recordfile  record.replaceoggrootdir  os.path.sep $^        if recordfilterrecordfile$^            samples.append$^                $^                    os.path.joinoggrootdir recordfile$^                    os.path.splitextos.path.basenamerecordfile0$^                $^            $^$^    counter  getcounter$^    numsamples  lensamples$^    rows  $^$^    printimporting ogg files...$^    pool  pool$^    bar  progressbar.progressbarmaxvaluenumsamples widgetssimplebar$^    for i processed in enumeratepool.imapunorderedonesample samples start0$^        counter  processed0$^        rows  processed0$^        bar.updatei$^    bar.updatenumsamples$^    pool.close$^    pool.join$^$^    with opentargetcsvtemplate.formattrain w encodingutf0 newline as traincsvfile   00$^        with opentargetcsvtemplate.formatdev w encodingutf0 newline as devcsvfile   00$^            with opentargetcsvtemplate.formattest w encodingutf0 newline as testcsvfile   00$^                trainwriter  csv.dictwritertraincsvfile fieldnamesfieldnames$^                trainwriter.writeheader$^                devwriter  csv.dictwriterdevcsvfile fieldnamesfieldnames$^                devwriter.writeheader$^                testwriter  csv.dictwritertestcsvfile fieldnamesfieldnames$^                testwriter.writeheader$^$^                for i item in enumeraterows$^                    transcript  validatelabelitem0$^                    if not transcript$^                        continue$^                    wavfilename  os.path.join$^                        oggrootdir item0.replace.ogg .wav$^                    $^                    imod  i  00$^                    if imod  0$^                        writer  testwriter$^                    elif imod  0$^                        writer  devwriter$^                    else$^                        writer  trainwriter$^                    writer.writerow$^                        dict$^                            wavfilenamewavfilename$^                            wavfilesizeos.path.getsizewavfilename$^                            transcripttranscript$^                        $^                    $^$^    importedsamples  getimportedsamplescounter$^    assert counterall  numsamples$^    assert lenrows  importedsamples$^$^    printimportreportcounter samplerate maxsecs$^$^$^def maybeconvertwavoggfilename wavfilename$^    if not os.path.existswavfilename$^        transformer  sox.transformer$^        transformer.convertsampleratesamplerate nchannelsnchannels bitdepthbitdepth$^        try$^            transformer.buildoggfilename wavfilename$^        except sox.core.soxerror as ex$^            printsox processing error ex oggfilename wavfilename$^$^$^def handleargs$^    parser  getimportersparser$^        descriptionimporter for lingualibre dataset. check httpslingualibre.frwikihelpdownloadfromlingualibre for details.$^    $^    parser.addargumentdesttargetdir$^    parser.addargument$^        qid typeint requiredtrue helplingualibre language qid$^    $^    parser.addargument$^        iso0000 typestr requiredtrue helpiso0000 language code$^    $^    parser.addargument$^        englishname typestr requiredtrue helpenglish name of the language$^    $^    parser.addargument$^        filteralphabet$^        helpexclude samples with characters not in provided alphabet$^    $^    parser.addargument$^        normalize$^        actionstoretrue$^        helpconverts diacritic characters to their base ones$^    $^    parser.addargument$^        bogusrecords$^        typeargparse.filetyper$^        requiredfalse$^        helptext file listing wellknown bogus record to skip from importing from httpslingualibre.frwikilingualibremisleadingitems$^    $^    return parser.parseargs$^$^$^if name  main$^    cliargs  handleargs$^    alphabet  alphabetcliargs.filteralphabet if cliargs.filteralphabet else none$^    validatelabel  getvalidatelabelcliargs$^$^    bogusregexes  $^    if cliargs.bogusrecords$^        for line in cliargs.bogusrecords$^            bogusregexes.appendre.compileline.strip$^$^    def recordfilterpath$^        if anyregex.matchpath for regex in bogusregexes$^            printreject path$^            return false$^        return true$^$^    def labelfilterlabel$^        if cliargs.normalize$^            label  $^                unicodedata.normalizenfkd label.strip$^                .encodeascii ignore$^                .decodeascii ignore$^            $^        label  validatelabellabel$^        if alphabet and label and not alphabet.canencodelabel$^            label  none$^        return label$^$^    archivename  archivename.format$^        qidcliargs.qid$^        iso0000cliargs.iso0000$^        languageenglishnamecliargs.englishname$^    $^    archiveurl  archiveurl.format$^        qidcliargs.qid$^        iso0000cliargs.iso0000$^        languageenglishnamecliargs.englishname$^    $^    downloadandpreprocessdatatargetdircliargs.targetdir$^usrbinenv python$^import glob$^import os$^import tarfile$^$^import pandas$^$^from deepspeechtraining.util.importers import getimportersparser$^$^columnnames  wavfilename wavfilesize transcript$^$^$^def extractarchivepath targetdir$^    printextracting  into ....formatarchivepath targetdir$^    with tarfile.openarchivepath as tar$^        tar.extractalltargetdir$^$^$^def preprocessdatatgzfile targetdir$^     first extract main archive and subarchives$^    extracttgzfile targetdir$^    mainfolder  os.path.jointargetdir dataaishell$^$^    wavarchivesfolder  os.path.joinmainfolder wav$^    for targz in glob.globos.path.joinwavarchivesfolder .tar.gz$^        extracttargz mainfolder$^$^     folder structure is now$^      dataaishell$^        trains.wav$^        devs.wav$^        tests.wav$^        wavs.tar.gz$^        transcriptaishelltranscriptv0.0.txt$^$^     transcripts file has one line per wav file where each line consists of$^     the wav file name without extension followed by a single space followed$^     by the transcript.$^$^     since the transcripts themselves can contain spaces we split on space but$^     only once then build a mapping from file name to transcript$^    transcriptspath  os.path.join$^        mainfolder transcript aishelltranscriptv0.0.txt$^    $^    with opentranscriptspath as fin$^        transcripts  dictline.split  maxsplit0 for line in fin$^$^    def loadsetglobpath$^        setfiles  $^        for wav in glob.globglobpath$^            try$^                wavfilename  wav$^                wavfilesize  os.path.getsizewav$^                transcriptkey  os.path.splitextos.path.basenamewav0$^                transcript  transcriptstranscriptkey.stripn$^                setfiles.appendwavfilename wavfilesize transcript$^            except keyerror$^                printwarning missing transcript for wav file ..formatwav$^        return setfiles$^$^    for subset in train dev test$^        printloading  set samples....formatsubset$^        subsetfiles  loadsetos.path.joinmainfolder subset s .wav$^        df  pandas.dataframedatasubsetfiles columnscolumnnames$^$^         trim train set to under 00s by removing the last couple hundred samples$^        if subset  train$^            durations  dfwavfilesize  00  00000  0$^            df  dfdurations  00.0$^            printtrimming  samples  00 seconds.formatdurations  00.0.sum$^$^        destcsv  os.path.jointargetdir aishell.csv.formatsubset$^        printsaving  set into ....formatsubset destcsv$^        df.tocsvdestcsv indexfalse$^$^$^def main$^     httpwww.openslr.org00$^    parser  getimportersparserdescriptionimport aishell corpus$^    parser.addargumentaishelltgzfile helppath to dataaishell.tgz$^    parser.addargument$^        targetdir$^        default$^        helptarget folder to extract files into and put the resulting csvs. defaults to same folder as the main archive.$^    $^    params  parser.parseargs$^$^    if not params.targetdir$^        params.targetdir  os.path.dirnameparams.aishelltgzfile$^$^    preprocessdataparams.aishelltgzfile params.targetdir$^$^$^if name  main$^    main$^usrbinenv python$^import glob$^import os$^import tarfile$^$^import pandas$^$^from deepspeechtraining.util.importers import getimportersparser$^$^columnnames  wavfilename wavfilesize transcript$^$^$^def extractarchivepath targetdir$^    printextracting  into ....formatarchivepath targetdir$^    with tarfile.openarchivepath as tar$^        tar.extractalltargetdir$^$^$^def preprocessdatatgzfile targetdir$^     first extract main archive and subarchives$^    extracttgzfile targetdir$^    mainfolder  os.path.jointargetdir aidatatang000zh$^$^    for targz in glob.globos.path.joinmainfolder corpus  .tar.gz$^        extracttargz os.path.dirnametargz$^$^     folder structure is now$^      aidatatang000zh$^        transcriptaidatatang000zhtranscript.txt$^        corpustrain.tar.gz$^        corpustrain.wavtxttrnmetadata$^        corpusdev.tar.gz$^        corpusdev.wavtxttrnmetadata$^        corpustest.tar.gz$^        corpustest.wavtxttrnmetadata$^$^     transcripts file has one line per wav file where each line consists of$^     the wav file name without extension followed by a single space followed$^     by the transcript.$^$^     since the transcripts themselves can contain spaces we split on space but$^     only once then build a mapping from file name to transcript$^    transcriptspath  os.path.join$^        mainfolder transcript aidatatang000zhtranscript.txt$^    $^    with opentranscriptspath as fin$^        transcripts  dictline.split  maxsplit0 for line in fin$^$^    def loadsetglobpath$^        setfiles  $^        for wav in glob.globglobpath$^            try$^                wavfilename  wav$^                wavfilesize  os.path.getsizewav$^                transcriptkey  os.path.splitextos.path.basenamewav0$^                transcript  transcriptstranscriptkey.stripn$^                setfiles.appendwavfilename wavfilesize transcript$^            except keyerror$^                printwarning missing transcript for wav file ..formatwav$^        return setfiles$^$^    for subset in train dev test$^        printloading  set samples....formatsubset$^        subsetfiles  loadset$^            os.path.joinmainfolder corpus subset  .wav$^        $^        df  pandas.dataframedatasubsetfiles columnscolumnnames$^$^         trim train set to under 00s by removing the last couple hundred samples$^        if subset  train$^            durations  dfwavfilesize  00  00000  0$^            df  dfdurations  00.0$^            printtrimming  samples  00 seconds.formatdurations  00.0.sum$^$^        destcsv  os.path.jointargetdir aidatatang.csv.formatsubset$^        printsaving  set into ....formatsubset destcsv$^        df.tocsvdestcsv indexfalse$^$^$^def main$^     httpswww.openslr.org00$^    parser  getimportersparserdescriptionimport aidatatang000zh corpus$^    parser.addargumenttgzfile helppath to aidatatang000zh.tgz$^    parser.addargument$^        targetdir$^        default$^        helptarget folder to extract files into and put the resulting csvs. defaults to same folder as the main archive.$^    $^    params  parser.parseargs$^$^    if not params.targetdir$^        params.targetdir  os.path.dirnameparams.tgzfile$^$^    preprocessdataparams.tgzfile params.targetdir$^$^$^if name  main$^    main$^from future import printfunction$^$^import progressbar$^import sys$^$^from .flags import flags$^$^$^ logging functions$^ $^$^def prefixprintprefix message$^    printprefix  n  prefix.joinmessage.splitn$^$^$^def logdebugmessage$^    if flags.loglevel  0$^        prefixprintd  message$^$^$^def loginfomessage$^    if flags.loglevel  0$^        prefixprinti  message$^$^$^def logwarnmessage$^    if flags.loglevel  0$^        prefixprintw  message$^$^$^def logerrormessage$^    if flags.loglevel  0$^        prefixprinte  message$^$^$^def createprogressbarargs kwargs$^     progress bars in stdout by default$^    if fd not in kwargs$^        kwargsfd  sys.stdout$^$^    if flags.showprogressbar$^        return progressbar.progressbarargs kwargs$^$^    return progressbar.nullbarargs kwargs$^$^$^def logprogressmessage$^    if not flags.showprogressbar$^        loginfomessage$^  coding utf0 $^from future import absoluteimport division printfunction$^$^from collections import counter$^from functools import partial$^$^import numpy as np$^import tensorflow as tf$^$^from tensorflow.python.ops import genaudioops as contribaudio$^$^from .config import config$^from .text import texttochararray$^from .flags import flags$^from .augmentations import applysampleaugmentations applygraphaugmentations$^from .audio import readframesfromfile vadsplit pcmtonp defaultformat$^from .samplecollections import samplesfromsources$^from .helpers import rememberexception megabyte$^$^$^def audiotofeaturesaudio samplerate transcriptnone clock0.0 trainphasefalse augmentationsnone sampleidnone$^    if trainphase$^         we need the lambdas to make tensorflow happy.$^         pylint disableunnecessarylambda$^        tf.condtf.math.notequalsamplerate flags.audiosamplerate$^                lambda tf.printwarning sample rate of sample sampleid  samplerate  $^                                 does not match flags.audiosamplerate. this can lead to incorrect results.$^                lambda tf.noop$^                namematchingsamplerate$^$^    if trainphase and augmentations$^        audio  applygraphaugmentationssignal audio augmentations transcripttranscript clockclock$^$^    spectrogram  contribaudio.audiospectrogramaudio$^                                                  windowsizeconfig.audiowindowsamples$^                                                  strideconfig.audiostepsamples$^                                                  magnitudesquaredtrue$^$^    if trainphase and augmentations$^        spectrogram  applygraphaugmentationsspectrogram spectrogram augmentations transcripttranscript clockclock$^$^    features  contribaudio.mfccspectrogramspectrogram$^                                  sampleratesamplerate$^                                  dctcoefficientcountconfig.ninput$^                                  upperfrequencylimitflags.audiosamplerate  0$^    features  tf.reshapefeatures 0 config.ninput$^$^    if trainphase and augmentations$^        features  applygraphaugmentationsfeatures features augmentations transcripttranscript clockclock$^$^    return features tf.shapeinputfeatures0$^$^$^def audiofiletofeatureswavfilename clock0.0 trainphasefalse augmentationsnone$^    samples  tf.io.readfilewavfilename$^    decoded  contribaudio.decodewavsamples desiredchannels0$^    return audiotofeaturesdecoded.audio$^                             decoded.samplerate$^                             clockclock$^                             trainphasetrainphase$^                             augmentationsaugmentations$^                             sampleidwavfilename$^$^$^def entrytofeaturessampleid audio samplerate transcript clock trainphasefalse augmentationsnone$^     httpsbugs.python.orgissue00000$^    sparsetranscript  tf.sparsetensortranscript$^    features featureslen  audiotofeaturesaudio$^                                               samplerate$^                                               transcriptsparsetranscript$^                                               clockclock$^                                               trainphasetrainphase$^                                               augmentationsaugmentations$^                                               sampleidsampleid$^    return sampleid features featureslen sparsetranscript$^$^$^def tosparsetuplesequence$^    rcreates a sparse representention of sequence.$^        returns a tuple with indices values shape$^    $^    indices  np.asarraylistzip0lensequence rangelensequence dtypenp.int00$^    shape  np.asarray0 lensequence dtypenp.int00$^    return indices sequence shape$^$^$^def createdatasetsources$^                   batchsize$^                   epochs0$^                   augmentationsnone$^                   cachepathnone$^                   trainphasefalse$^                   reversefalse$^                   limit0$^                   exceptionboxnone$^                   processaheadnone$^                   buffering0  megabyte$^                   splitdatasetfalse$^    epochcounter  counter   survives restarts of the dataset and its generator$^$^    def generatevalues$^        epoch  epochcounterepoch$^        if trainphase$^            epochcounterepoch  0$^        samples  samplesfromsourcessources bufferingbuffering labeledtrue reversereverse$^        numsamples  lensamples$^        if limit  0$^            numsamples  minlimit numsamples$^        samples  applysampleaugmentationssamples$^                                             augmentations$^                                             bufferingbuffering$^                                             processahead0  batchsize if processahead is none else processahead$^                                             clockepoch  epochs$^                                             finalclockepoch  0  epochs$^        for sampleindex sample in enumeratesamples$^            if sampleindex  numsamples$^                break$^            clock  epoch  numsamples  sampleindex  epochs  numsamples if trainphase and epochs  0 else 0.0$^            transcript  texttochararraysample.transcript config.alphabet contextsample.sampleid$^            transcript  tosparsetupletranscript$^            yield sample.sampleid sample.audio sample.audioformat.rate transcript clock$^$^     batching a dataset of 0d sparsetensors creates 0d batches which fail$^     when passed to tf.nn.ctcloss so we reshape them to remove the extra$^     dimension here.$^    def sparsereshapesparse$^        shape  sparse.denseshape$^        return tf.sparse.reshapesparse shape0 shape0$^$^    def batchfnsampleids features featureslen transcripts$^        features  tf.data.dataset.zipfeatures featureslen$^        features  features.paddedbatchbatchsize paddedshapesnone config.ninput $^        transcripts  transcripts.batchbatchsize.mapsparsereshape$^        sampleids  sampleids.batchbatchsize$^        return tf.data.dataset.zipsampleids features transcripts$^$^    processfn  partialentrytofeatures trainphasetrainphase augmentationsaugmentations$^$^    dataset  tf.data.dataset.fromgeneratorrememberexceptiongeneratevalues exceptionbox$^                                             outputtypestf.string tf.float00 tf.int00$^                                                           tf.int00 tf.int00 tf.int00 tf.float00$^    if splitdataset$^         using horovod iterator.getnext is not aware of different devices.$^         a.shardn i will contain all elements of a whose index mod n  i.$^        import horovod.tensorflow as hvd$^        dataset  dataset.shardhvd.size hvd.rank$^    dataset  dataset.mapprocessfn numparallelcallstf.data.experimental.autotune$^    if cachepath$^        dataset  dataset.cachecachepath$^    dataset  dataset.windowbatchsize dropremaindertrainphase.flatmapbatchfn$^    if splitdataset$^        todo is there a way to get a proper value$^        dataset  dataset.prefetch0$^    else$^        dataset  dataset.prefetchconfig.numdevices$^    return dataset$^$^def splitaudiofileaudiopath$^                     audioformatdefaultformat$^                     batchsize0$^                     aggressiveness0$^                     outlierdurationms00000$^                     outlierbatchsize0$^                     exceptionboxnone$^    def generatevalues$^        frames  readframesfromfileaudiopath$^        segments  vadsplitframes aggressivenessaggressiveness$^        for segment in segments$^            segmentbuffer timestart timeend  segment$^            samples  pcmtonpsegmentbuffer audioformat$^            yield timestart timeend samples$^$^    def tomfccstimestart timeend samples$^        features featureslen  audiotofeaturessamples audioformat.rate$^        return timestart timeend features featureslen$^$^    def createbatchsetbs criteria$^        return tf.data.dataset$^                .fromgeneratorrememberexceptiongeneratevalues exceptionbox$^                                outputtypestf.int00 tf.int00 tf.float00$^                .maptomfccs numparallelcallstf.data.experimental.autotune$^                .filtercriteria$^                .paddedbatchbs paddedshapes  none config.ninput $^$^    nds  createbatchsetbatchsize$^                           lambda start end f fl end  start  intoutlierdurationms$^    ods  createbatchsetoutlierbatchsize$^                           lambda start end f fl end  start  intoutlierdurationms$^    dataset  nds.concatenateods$^    dataset  dataset.prefetchconfig.numdevices$^    return dataset$^from future import absoluteimport division printfunction$^$^import os$^import sys$^import tensorflow.compat.v0 as tfv0$^$^from attrdict import attrdict$^from xdg import basedirectory as xdg$^from dsctcdecoder import alphabet utf0alphabet$^$^from .flags import flags$^from .gpu import getavailablegpus$^from .logging import logerror logwarn$^from .helpers import parsefilesize$^from .augmentations import parseaugmentations normalizesamplerate$^from .io import pathexistsremote$^$^class configsingleton$^    config  none$^$^    def getattrself name$^        if not configsingleton.config$^            raise runtimeerrorglobal configuration not yet initialized.$^        if not hasattrconfigsingleton.config name$^            raise runtimeerrorconfiguration option  not found in config..formatname$^        return configsingleton.configname$^$^$^config  configsingleton  pylint disableinvalidname$^$^def initializeglobals$^    c  attrdict$^$^     augmentations$^    c.augmentations  parseaugmentationsflags.augment$^    if c.augmentations and flags.featurecache and flags.cacheforepochs  0$^        logwarndue to current featurecache settings the exact same sample augmentations of the first $^                 epoch will be repeated on all following epochs. this could lead to unintended overfitting. $^                 you could use cacheforepochs nepochs to invalidate the cache after a given number of epochs.$^$^    if flags.normalizesamplerate$^        c.augmentations  normalizesamplerateflags.audiosamplerate  caugmentations$^$^     caching$^    if flags.cacheforepochs  0$^        logwarncacheforepochs  0 is recreating the feature cache on every epoch but will never use it.$^$^     readbuffer$^    flags.readbuffer  parsefilesizeflags.readbuffer$^$^     set default dropout rates$^    if flags.dropoutrate0  0$^        flags.dropoutrate0  flags.dropoutrate$^    if flags.dropoutrate0  0$^        flags.dropoutrate0  flags.dropoutrate$^    if flags.dropoutrate0  0$^        flags.dropoutrate0  flags.dropoutrate$^$^     set default checkpoint dir$^    if not flags.checkpointdir$^        flags.checkpointdir  xdg.savedatapathos.path.joindeepspeech checkpoints$^$^    if flags.loadtrain not in last best init auto$^        flags.loadtrain  auto$^$^    if flags.loadevaluate not in last best auto$^        flags.loadevaluate  auto$^$^     set default summary dir$^    if not flags.summarydir$^        flags.summarydir  xdg.savedatapathos.path.joindeepspeech summaries$^$^     standard session configuration thatll be used for all new sessions.$^    c.sessionconfig  tfv0.configprotoallowsoftplacementtrue logdeviceplacementflags.logplacement$^                                        interopparallelismthreadsflags.interopparallelismthreads$^                                        intraopparallelismthreadsflags.intraopparallelismthreads$^                                        gpuoptionstfv0.gpuoptionsallowgrowthflags.useallowgrowth$^$^     cpu device$^    c.cpudevice  cpu0$^$^    if flags.horovod$^        try$^            import horovod.tensorflow as hvd$^        except importerror as e$^            print$^                error importing horovod. did you installed deepspeech with dnohorovod $^                if you do not want to use horovod use from deepspeechtraining import train$^            raise e$^$^        hvd.init$^$^         pin gpu to be used to process local rank one gpu per process$^        c.sessionconfig.gpuoptions.visibledevicelist  strhvd.localrank$^        c.numdevices  hvd.size$^        c.ismasterprocess  true if hvd.rank  0 else false$^    else$^      available gpu devices$^        c.availabledevices  getavailablegpusc.sessionconfig$^$^         if there is no gpu available we fall back to cpu based operation$^        if not c.availabledevices$^            c.availabledevices  c.cpudevice$^$^        c.numdevices  lenc.availabledevices$^$^         if there are no horovod processes the only one should handled like horovod master$^        c.ismasterprocess  true$^$^    if flags.bytesoutputmode$^        c.alphabet  utf0alphabet$^    else$^        c.alphabet  alphabetos.path.abspathflags.alphabetconfigpath$^$^     geometric constants$^     $^$^     for an explanation of the meaning of the geometric constants please refer to$^     docgeometry.md$^$^     number of mfcc features$^    c.ninput  00  todo determine this programmatically from the sample rate$^$^     the number of frames in the context$^    c.ncontext  0  todo determine the optimal value using a validation data set$^$^     number of units in hidden layers$^    c.nhidden  flags.nhidden$^$^    c.nhidden0  c.nhidden$^$^    c.nhidden0  c.nhidden$^$^    c.nhidden0  c.nhidden$^$^     lstm cell state dimension$^    c.ncelldim  c.nhidden$^$^     the number of units in the third layer which feeds in to the lstm$^    c.nhidden0  c.ncelldim$^$^     units in the sixth layer  number of characters in the target language plus one$^    c.nhidden0  c.alphabet.getsize  0  0 for ctc blank label$^$^     size of audio window in samples$^    if flags.featurewinlen  flags.audiosamplerate  0000  0$^        logerrorfeaturewinlen value  in milliseconds  multiplied $^                  by audiosamplerate value  must be an integer value. adjust $^                  your featurewinlen value or resample your audio accordingly.$^                  .formatflags.featurewinlen flags.featurewinlen  0000 flags.audiosamplerate$^        sys.exit0$^$^    c.audiowindowsamples  flags.audiosamplerate  flags.featurewinlen  0000$^$^     stride for feature computations in samples$^    if flags.featurewinstep  flags.audiosamplerate  0000  0$^        logerrorfeaturewinstep value  in milliseconds  multiplied $^                  by audiosamplerate value  must be an integer value. adjust $^                  your featurewinstep value or resample your audio accordingly.$^                  .formatflags.featurewinstep flags.featurewinstep  0000 flags.audiosamplerate$^        sys.exit0$^$^    c.audiostepsamples  flags.audiosamplerate  flags.featurewinstep  0000$^$^    if flags.oneshotinfer$^        if not pathexistsremoteflags.oneshotinfer$^            logerrorpath specified in oneshotinfer is not a valid file.$^            sys.exit0$^$^    if flags.traincudnn and flags.loadcudnn$^        logerrortrying to use traincudnn but loadcudnn $^                  was also specified. the loadcudnn flag is only $^                  needed when converting a cudnn rnn checkpoint to $^                  a cpucapable graph. if your system is capable of $^                  using cudnn rnn you can just specify the cudnn rnn $^                  checkpoint normally with savecheckpointdir.$^        sys.exit0$^$^     if separate save and load flags were not specified default to load and save$^     from the same dir.$^    if not flags.savecheckpointdir$^        flags.savecheckpointdir  flags.checkpointdir$^$^    if not flags.loadcheckpointdir$^        flags.loadcheckpointdir  flags.checkpointdir$^$^    configsingleton.config  c  pylint disableprotectedaccess$^import requests$^import progressbar$^$^from os import path makedirs$^from .io import openremote pathexistsremote isremotepath$^$^simplebar  progress  progressbar.bar   progressbar.percentage  completed$^$^def maybedownloadarchivename targetdir archiveurl$^     if archive file does not exist download it...$^    archivepath  path.jointargetdir archivename$^$^    if not isremotepathtargetdir and not path.existstargetdir$^        printno path s  creating ...  targetdir$^        makedirstargetdir$^$^    if not pathexistsremotearchivepath$^        printno archive s  downloading...  archivepath$^        req  requests.getarchiveurl streamtrue$^        totalsize  intreq.headers.getcontentlength 0$^        done  0$^        with openremotearchivepath wb as f$^            bar  progressbar.progressbarmaxvaluetotalsize if totalsize  0 else progressbar.unknownlength widgetssimplebar$^$^            for data in req.itercontent00000000$^                done  lendata$^                f.writedata$^                bar.updatedone$^    else$^        printfound archive s  not downloading.  archivepath$^    return archivepath$^  coding utf0 $^import os$^import io$^import csv$^import json$^import tarfile$^$^from pathlib import path$^from functools import partial$^$^from .helpers import kilobyte megabyte gigabyte interleaved lenmap$^from .audio import $^    sample$^    audiotypepcm$^    audiotypeopus$^    serializableaudiotypes$^    getloadableaudiotypefromextension$^    writewav$^$^from .io import openremote isremotepath$^$^bigendian  big$^intsize  0$^bigintsize  0  intsize$^magic  bsampledb$^$^buffersize  0  megabyte$^reversebuffersize  00  kilobyte$^cachesize  0  gigabyte$^$^schemakey  schema$^contentkey  content$^mimetypekey  mimetype$^mimetypetext  textplain$^contenttypespeech  speech$^contenttypetranscript  transcript$^$^$^class labeledsamplesample$^    inmemory labeled audio sample representing an utterance.$^    derived from util.audio.sample and used by sample collection readers and writers.$^    def initself audiotype rawdata transcript audioformatnone sampleidnone$^        $^        parameters$^        $^        audiotype  str$^            see util.audio.sample.init .$^        rawdata  binary$^            see util.audio.sample.init .$^        transcript  str$^            transcript of the samples utterance$^        audioformat  tuple$^            see util.audio.sample.init .$^        sampleid  str$^            tracking id  should indicate samples origin as precisely as possible.$^            it is typically assigned by collection readers.$^        $^        super.initaudiotype rawdata audioformataudioformat sampleidsampleid$^        self.transcript  transcript$^$^$^class packedsample$^    $^    a wrapper that we can carry around in an iterator and pass to a child process in order to$^    have the child process do the loadingunpacking of the sample allowing for parallel file$^    io.$^    $^    def initself filename audiotype label$^        self.filename  filename$^        self.audiotype  audiotype$^        self.label  label$^$^    def unpackself$^        with openremoteself.filename rb as audiofile$^            data  audiofile.read$^        if self.label is none$^            s  sampleself.audiotype data sampleidself.filename$^        s  labeledsampleself.audiotype data self.label sampleidself.filename$^        return s$^$^$^def unpackmaybesample$^    $^    loads the supplied sample from disk or the network if the audio isnt loaded in to memory already.$^    $^    if hasattrsample unpack$^        realizedsample  sample.unpack$^    else$^        realizedsample  sample$^    return realizedsample$^$^$^def loadsamplefilename labelnone$^    $^    loads audiofile as a labeled or unlabeled sample$^$^    parameters$^    $^    filename  str$^        filename of the audiofile to load as sample$^    label  str$^        label transcript of the sample.$^        if none returned result.unpack will return util.audio.sample instance$^        otherwise returned result.unpack  util.samplecollections.labeledsample instance$^$^    returns$^    $^    util.samplecollections.packedsample a wrapper object on which calling unpack will return$^        util.audio.sample instance if label is none else util.samplecollections.labeledsample instance$^    $^    ext  os.path.splitextfilename0.lower$^    audiotype  getloadableaudiotypefromextensionext$^    if audiotype is none$^        raise valueerrorunknown audio type extension .formatext$^    return packedsamplefilename audiotype label$^$^$^class directsdbwriter$^    sample collection writer for creating a sample db sdb file$^    def initself$^                 sdbfilename$^                 bufferingbuffersize$^                 audiotypeaudiotypeopus$^                 bitratenone$^                 idprefixnone$^                 labeledtrue$^        $^        parameters$^        $^        sdbfilename  str$^            path to the sdb file to write$^        buffering  int$^            writebuffer size to use while writing the sdb file$^        audiotype  str$^            see util.audio.sample.init .$^        bitrate  int$^            bitrate for samplecompression in case of lossy audiotype e.g. audiotypeopus$^        idprefix  str$^            prefix for ids of written samples  defaults to sdbfilename$^        labeled  bool or none$^            if true writes labeled samples util.samplecollections.labeledsample only.$^            if false ignores transcripts if available and writes unlabeled util.audio.sample instances.$^        $^        self.sdbfilename  sdbfilename$^        self.idprefix  sdbfilename if idprefix is none else idprefix$^        self.labeled  labeled$^        if audiotype not in serializableaudiotypes$^            raise valueerroraudio type  not supported.formataudiotype$^        self.audiotype  audiotype$^        self.bitrate  bitrate$^        self.sdbfile  openremotesdbfilename wb bufferingbuffering$^        self.offsets  $^        self.numsamples  0$^$^        self.sdbfile.writemagic$^$^        schemaentries  contentkey contenttypespeech mimetypekey audiotype$^        if self.labeled$^            schemaentries.appendcontentkey contenttypetranscript mimetypekey mimetypetext$^        metadata  schemakey schemaentries$^        metadata  json.dumpsmetadata.encode$^        self.writebigintlenmetadata$^        self.sdbfile.writemetadata$^$^        self.offsetsamples  self.sdbfile.tell$^        self.sdbfile.seek0  bigintsize 0$^$^    def writeintself n$^        return self.sdbfile.writen.tobytesintsize bigendian$^$^    def writebigintself n$^        return self.sdbfile.writen.tobytesbigintsize bigendian$^$^    def enterself$^        return self$^$^    def addself sample$^        def tobytesn$^            return n.tobytesintsize bigendian$^        sample.changeaudiotypeself.audiotype bitrateself.bitrate$^        opus  sample.audio.getbuffer$^        opuslen  tobyteslenopus$^        if self.labeled$^            transcript  sample.transcript.encode$^            transcriptlen  tobyteslentranscript$^            entrylen  tobyteslenopuslen  lenopus  lentranscriptlen  lentranscript$^            buffer  b.joinentrylen opuslen opus transcriptlen transcript$^        else$^            entrylen  tobyteslenopuslen  lenopus$^            buffer  b.joinentrylen opuslen opus$^        self.offsets.appendself.sdbfile.tell$^        self.sdbfile.writebuffer$^        sample.sampleid  .formatself.idprefix self.numsamples$^        self.numsamples  0$^        return sample.sampleid$^$^    def closeself$^        if self.sdbfile is none$^            return$^        offsetindex  self.sdbfile.tell$^        self.sdbfile.seekself.offsetsamples$^        self.writebigintoffsetindex  self.offsetsamples  bigintsize$^        self.writebigintself.numsamples$^$^        self.sdbfile.seekoffsetindex  bigintsize$^        self.writebigintself.numsamples$^        for offset in self.offsets$^            self.writebigintoffset$^        offsetend  self.sdbfile.tell$^        self.sdbfile.seekoffsetindex$^        self.writebigintoffsetend  offsetindex  bigintsize$^        self.sdbfile.close$^        self.sdbfile  none$^$^    def lenself$^        return lenself.offsets$^$^    def exitself exctype excval exctb$^        self.close$^$^$^class sdb   pylint disabletoomanyinstanceattributes$^    sample collection reader for reading a sample db sdb file$^    def initself$^                 sdbfilename$^                 bufferingbuffersize$^                 idprefixnone$^                 labeledtrue$^                 reversefalse$^        $^        parameters$^        $^        sdbfilename  str$^            path to the sdb file to read samples from$^        buffering  int$^            readahead buffer size to use while reading the sdb file in normal order. fixed to 00kb if in reversemode.$^        idprefix  str$^            prefix for ids of read samples  defaults to sdbfilename$^        labeled  bool or none$^            if true reads util.samplecollections.labeledsample instances. fails if sdb file provides no transcripts.$^            if false ignores transcripts if available and reads unlabeled util.audio.sample instances.$^            if none automatically determines if sdb schema has transcripts$^            reading util.samplecollections.labeledsample instances or not reading util.audio.sample instances.$^        $^        self.sdbfilename  sdbfilename$^        self.idprefix  sdbfilename if idprefix is none else idprefix$^        self.sdbfile  openremotesdbfilename rb bufferingreversebuffersize if reverse else buffering$^        self.offsets  $^        if self.sdbfile.readlenmagic  magic$^            raise runtimeerrorno sample database$^        metachunklen  self.readbigint$^        self.meta  json.loadsself.sdbfile.readmetachunklen.decode$^        if schemakey not in self.meta$^            raise runtimeerrormissing schema$^        self.schema  self.metaschemakey$^$^        speechcolumns  self.findcolumnscontentcontenttypespeech mimetypeserializableaudiotypes$^        if not speechcolumns$^            raise runtimeerrorno speech data missing in schema$^        self.speechindex  speechcolumns0$^        self.audiotype  self.schemaself.speechindexmimetypekey$^$^        self.transcriptindex  none$^        if labeled is not false$^            transcriptcolumns  self.findcolumnscontentcontenttypetranscript mimetypemimetypetext$^            if transcriptcolumns$^                self.transcriptindex  transcriptcolumns0$^            else$^                if labeled is true$^                    raise runtimeerrorno transcript data missing in schema$^$^        samplechunklen  self.readbigint$^        self.sdbfile.seeksamplechunklen  bigintsize 0$^        numsamples  self.readbigint$^        for  in rangenumsamples$^            self.offsets.appendself.readbigint$^        if reverse$^            self.offsets.reverse$^$^    def readintself$^        return int.frombytesself.sdbfile.readintsize bigendian$^$^    def readbigintself$^        return int.frombytesself.sdbfile.readbigintsize bigendian$^$^    def findcolumnsself contentnone mimetypenone$^        criteria  $^        if content is not none$^            criteria.appendcontentkey content$^        if mimetype is not none$^            criteria.appendmimetypekey mimetype$^        if lencriteria  0$^            raise valueerrorat least one of content or mimetype has to be provided$^        matches  $^        for index column in enumerateself.schema$^            matched  0$^            for field value in criteria$^                if columnfield  value or isinstancevalue list and columnfield in value$^                    matched  0$^            if matched  lencriteria$^                matches.appendindex$^        return matches$^$^    def readrowself rowindex columns$^        columns  listcolumns$^        columndata  none  lencolumns$^        found  0$^        if not 0  rowindex  lenself.offsets$^            raise valueerrorwrong sample index   has to be between 0 and $^                             .formatrowindex lenself.offsets  0$^        self.sdbfile.seekself.offsetsrowindex  intsize$^        for index in rangelenself.schema$^            chunklen  self.readint$^            if index in columns$^                columndatacolumns.indexindex  self.sdbfile.readchunklen$^                found  0$^                if found  lencolumns$^                    return tuplecolumndata$^            else$^                self.sdbfile.seekchunklen 0$^        return tuplecolumndata$^$^    def getitemself i$^        sampleid  .formatself.idprefix i$^        if self.transcriptindex is none$^            audiodata  self.readrowi self.speechindex$^            return sampleself.audiotype audiodata sampleidsampleid$^        audiodata transcript  self.readrowi self.speechindex self.transcriptindex$^        transcript  transcript.decode$^        return labeledsampleself.audiotype audiodata transcript sampleidsampleid$^$^    def iterself$^        for i in rangelenself.offsets$^            yield selfi$^$^    def lenself$^        return lenself.offsets$^$^    def closeself$^        if self.sdbfile is not none$^            self.sdbfile.close$^$^    def delself$^        self.close$^$^$^class csvwriter   pylint disabletoomanyinstanceattributes$^    sample collection writer for writing a csv dataset and all its referenced wav samples$^    def initself$^                 csvfilename$^                 absolutepathsfalse$^                 labeledtrue$^        $^        parameters$^        $^        csvfilename  str$^            path to the csv file to write.$^            will create a directory csvfilename without extension next to it and fail if it already exists.$^        absolutepaths  bool$^            if paths in csv file should be absolute instead of relative to the csv files parent directory.$^        labeled  bool or none$^            if true writes labeled samples util.samplecollections.labeledsample only.$^            if false ignores transcripts if available and writes unlabeled util.audio.sample instances.$^        $^        currently only works with local files not gs or hdfs...$^        $^        self.csvfilename  pathcsvfilename$^        self.csvbasedir  self.csvfilename.parent.resolve.absolute$^        self.setname  self.csvfilename.stem$^        self.csvdir  self.csvbasedir  self.setname$^        if self.csvdir.exists$^            raise runtimeerror already existing.formatself.csvdir$^        os.mkdirstrself.csvdir$^        self.absolutepaths  absolutepaths$^        fieldnames  wavfilename wavfilesize$^        self.labeled  labeled$^        if labeled$^            fieldnames.appendtranscript$^        self.csvfile  openremotecsvfilename w encodingutf0 newline$^        self.csvwriter  csv.dictwriterself.csvfile fieldnamesfieldnames$^        self.csvwriter.writeheader$^        self.counter  0$^$^    def enterself$^        return self$^$^    def addself sample$^        samplefilename  self.csvdir  sample000d.wav.formatself.counter$^        self.counter  0$^        sample.changeaudiotypeaudiotypepcm$^        writewavstrsamplefilename sample.audio audioformatsample.audioformat$^        sample.sampleid  strsamplefilename.relativetoself.csvbasedir$^        row  $^            wavfilename strsamplefilename.absolute if self.absolutepaths else sample.sampleid$^            wavfilesize samplefilename.stat.stsize$^        $^        if self.labeled$^            rowtranscript  sample.transcript$^        self.csvwriter.writerowrow$^        return sample.sampleid$^$^    def closeself$^        if self.csvfile$^            self.csvfile.close$^$^    def lenself$^        return self.counter$^$^    def exitself exctype excval exctb$^        self.close$^$^$^class tarwriter   pylint disabletoomanyinstanceattributes$^    sample collection writer for writing a csv dataset and all its referenced wav samples to a tar file.$^    def initself$^                 tarfilename$^                 gzfalse$^                 labeledtrue$^                 includenone$^        $^        parameters$^        $^        tarfilename  str$^            path to the tar file to write.$^        gz  bool$^            if to compress tar file with gzip.$^        labeled  bool or none$^            if true writes labeled samples util.samplecollections.labeledsample only.$^            if false ignores transcripts if available and writes unlabeled util.audio.sample instances.$^        include  str$^            list of files to include into tar root.$^$^        currently only works with local files not gs or hdfs...$^        $^        self.tar  tarfile.opentarfilename wgz if gz else w$^        samplesdir  tarfile.tarinfosamples$^        samplesdir.type  tarfile.dirtype$^        self.tar.addfilesamplesdir$^        if include$^            for includepath in include$^                self.tar.addincludepath recursivefalse arcnamepathincludepath.name$^        fieldnames  wavfilename wavfilesize$^        self.labeled  labeled$^        if labeled$^            fieldnames.appendtranscript$^        self.csvfile  io.stringio$^        self.csvwriter  csv.dictwriterself.csvfile fieldnamesfieldnames$^        self.csvwriter.writeheader$^        self.counter  0$^$^    def enterself$^        return self$^$^    def addself sample$^        samplefilename  samplessample000d.wav.formatself.counter$^        self.counter  0$^        sample.changeaudiotypeaudiotypepcm$^        samplefile  io.bytesio$^        writewavsamplefile sample.audio audioformatsample.audioformat$^        samplesize  samplefile.tell$^        samplefile.seek0$^        sampletar  tarfile.tarinfosamplefilename$^        sampletar.size  samplesize$^        self.tar.addfilesampletar samplefile$^        row  $^            wavfilename samplefilename$^            wavfilesize samplesize$^        $^        if self.labeled$^            rowtranscript  sample.transcript$^        self.csvwriter.writerowrow$^        return samplefilename$^$^    def closeself$^        if self.csvfile and self.tar$^            csvtar  tarfile.tarinfosamples.csv$^            csvtar.size  self.csvfile.tell$^            self.csvfile.seek0$^            self.tar.addfilecsvtar io.bytesioself.csvfile.read.encodeutf0$^        if self.tar$^            self.tar.close$^$^    def lenself$^        return self.counter$^$^    def exitself exctype excval exctb$^        self.close$^$^$^class samplelist$^    sample collection base class with samples loaded from a list of inmemory paths.$^    def initself samples labeledtrue reversefalse$^        $^        parameters$^        $^        samples  iterable of tuples of the form samplefilename filesize  transcript$^            filesize is used for ordering the samples transcript has to be provided if labeledtrue$^        labeled  bool or none$^            if true reads labeledsample instances.$^            if false ignores transcripts if available and reads unlabeled util.audio.sample instances.$^        reverse  bool$^            if the order of the samples should be reversed$^        $^        self.labeled  labeled$^        self.samples  listsamples$^        self.samples.sortkeylambda r r0 reversereverse$^$^    def getitemself i$^        samplespec  self.samplesi$^        return loadsamplesamplespec0 labelsamplespec0 if self.labeled else none$^$^    def lenself$^        return lenself.samples$^$^$^class csvsamplelist$^    sample collection reader for reading a deepspeech csv file$^    automatically orders samples by csv column wavfilesize if available.$^    def initself csvfilename labelednone reversefalse$^        $^        parameters$^        $^        csvfilename  str$^            path to the csv file containing sample audio paths and transcripts$^        labeled  bool or none$^            if true reads labeledsample instances. fails if csv file has no transcript column.$^            if false ignores transcripts if available and reads unlabeled util.audio.sample instances.$^            if none automatically determines if csv file has a transcript column$^            reading util.samplecollections.labeledsample instances or not reading util.audio.sample instances.$^        reverse  bool$^            if the order of the samples should be reversed$^        $^        rows  $^        with openremotecsvfilename r encodingutf0 as csvfile$^            reader  csv.dictreadercsvfile$^            if transcript in reader.fieldnames$^                if labeled is none$^                    labeled  true$^            elif labeled$^                raise runtimeerrorno transcript data missing csv column$^            for row in reader$^                wavfilename  pathrowwavfilename$^                if not wavfilename.isabsolute and not isremotepathrowwavfilename$^                    wavfilename  pathcsvfilename.parent  wavfilename$^                    wavfilename  strwavfilename$^                else$^                     pathlib otherwise removes a  from filenames like hdfs$^                    wavfilename  rowwavfilename$^                wavfilesize  introwwavfilesize if wavfilesize in row else 0$^                if labeled$^                    rows.appendwavfilename wavfilesize rowtranscript$^                else$^                    rows.appendwavfilename wavfilesize$^        supercsv self.initrows labeledlabeled reversereverse$^$^$^def samplesfromsourcesamplesource bufferingbuffersize labelednone reversefalse$^    $^    loads samples from a sample source file.$^$^    parameters$^    $^    samplesource  str$^        path to the sample source file sdb or csv$^    buffering  int$^        readbuffer size to use while reading files$^    labeled  bool or none$^        if true reads labeledsample instances. fails if source provides no transcripts.$^        if false ignores transcripts if available and reads unlabeled util.audio.sample instances.$^        if none automatically determines if source provides transcripts$^        reading util.samplecollections.labeledsample instances or not reading util.audio.sample instances.$^    reverse  bool$^        if the order of the samples should be reversed$^$^    returns$^    $^    iterable of util.samplecollections.labeledsample or util.audio.sample instances supporting len.$^    $^    ext  os.path.splitextsamplesource0.lower$^    if ext  .sdb$^        return sdbsamplesource bufferingbuffering labeledlabeled reversereverse$^    if ext  .csv$^        return csvsamplesource labeledlabeled reversereverse$^    raise valueerrorunknown file type .formatext$^$^$^def samplesfromsourcessamplesources bufferingbuffersize labelednone reversefalse$^    $^    loads and combines samples from a list of source files. sources are combined in an interleaving way to$^    keep default sample order from shortest to longest.$^$^    note that when using distributed training it is much faster to call this function with single pre$^    sorted sample source because this allows for parallelization of the file io. if this function is$^    called with multiple sources the samples have to be unpacked on a single parent process to allow$^    for reading their durations.$^$^    parameters$^    $^    samplesources  list of str$^        paths to sample source files sdbs or csvs$^    buffering  int$^        readbuffer size to use while reading files$^    labeled  bool or none$^        if true reads labeledsample instances. fails if not all sources provide transcripts.$^        if false ignores transcripts if available and always reads unlabeled util.audio.sample instances.$^        if none reads util.samplecollections.labeledsample instances from sources with transcripts and$^        util.audio.sample instances from sources with no transcripts.$^    reverse  bool$^        if the order of the samples should be reversed$^$^    returns$^    $^    iterable of util.samplecollections.packedsample if a single collection is provided wrapping$^        labeledsample labeledtrue or util.audio.sample labeledfalse supporting len$^    or labeledsample  util.audio.sample directly if multiple collections are provided$^    $^    samplesources  listsamplesources$^    if lensamplesources  0$^        raise valueerrorno files$^    if lensamplesources  0$^        return samplesfromsourcesamplesources0 bufferingbuffering labeledlabeled reversereverse$^$^     if we wish to interleave based on duration we have to unpack the audio. note that this unpacking should$^     be done lazily onn the fly so that it respects the limitingpool logic used in the feeding code.$^    cols  lenmap$^        unpackmaybe samplesfromsourcesource bufferingbuffering labeledlabeled reversereverse$^        for source in samplesources$^$^    return interleavedcols keylambda s s.duration reversereverse$^from future import absoluteimport division printfunction$^$^import os$^import absl.flags$^$^flags  absl.flags.flags$^$^ sphinxdoc trainingrefflagsstart$^def createflags$^     importer$^     $^$^    f  absl.flags$^$^    f.definestringtrainfiles  comma separated list of files specifying the dataset used for training. multiple files will get merged. if empty training will not be run.$^    f.definestringdevfiles  comma separated list of files specifying the datasets used for validation. multiple files will get reported separately. if empty validation will not be run.$^    f.definestringtestfiles  comma separated list of files specifying the datasets used for testing. multiple files will get reported separately. if empty the model will not be tested.$^    f.definestringmetricsfiles  comma separated list of files specifying the datasets used for tracking of metrics after validation step. currently the only metric is the ctc loss but without affecting the tracking of best validation loss. multiple files will get reported separately. if empty metrics will not be computed.$^$^    f.definestringreadbuffer 0mb buffersize for reading samples from datasets supports filesize suffixes kb mb gb tb$^    f.definestringfeaturecache  cache mfcc features to disk to speed up future training runs on the same data. this flag specifies the path where cached features extracted from trainfiles will be saved. if empty or if online augmentation flags are enabled caching will be disabled.$^    f.defineintegercacheforepochs 0 after how many epochs the feature cache is invalidated again  0 for never$^$^    f.defineintegerfeaturewinlen 00 feature extraction audio window length in milliseconds$^    f.defineintegerfeaturewinstep 00 feature extraction window step length in milliseconds$^    f.defineintegeraudiosamplerate 00000 sample rate value expected by model$^    f.definebooleannormalizesamplerate true normalize sample rate of all trainfiles to audiosamplerate$^$^     data augmentation$^     $^$^    f.definemultistringaugment none specifies an augmentation of the training samples. format is augment operationparam0value0 ...$^$^     global constants$^     $^$^    f.defineintegerepochs 00 how many epochs complete runs through the train files to train for$^$^    f.definefloatdropoutrate 0.00 dropout rate for feedforward layers$^    f.definefloatdropoutrate0 0.0 dropout rate for layer 0  defaults to dropoutrate$^    f.definefloatdropoutrate0 0.0 dropout rate for layer 0  defaults to dropoutrate$^    f.definefloatdropoutrate0 0.0 dropout rate for layer 0  defaults to 0.0$^    f.definefloatdropoutrate0 0.0 dropout rate for layer 0  defaults to 0.0$^    f.definefloatdropoutrate0 0.0 dropout rate for layer 0  defaults to dropoutrate$^$^    f.definefloatreluclip 00.0 relu clipping value for nonrecurrent layers$^$^     adam optimizerhttparxiv.orgabs0000.0000 parameters$^$^    f.definefloatbeta0 0.0 beta 0 parameter of adam optimizer$^    f.definefloatbeta0 0.000 beta 0 parameter of adam optimizer$^    f.definefloatepsilon 0e0 epsilon parameter of adam optimizer$^    f.definefloatlearningrate 0.000 learning rate of adam optimizer$^$^     batch sizes$^$^    f.defineintegertrainbatchsize 0 number of elements in a training batch$^    f.defineintegerdevbatchsize 0 number of elements in a validation batch$^    f.defineintegertestbatchsize 0 number of elements in a test batch$^$^    f.defineintegerexportbatchsize 0 number of elements per batch on the exported graph$^$^     performance$^$^    f.defineintegerinteropparallelismthreads 0 number of interop parallelism threads  see tf.configproto for more details. use of this flag is unsupported$^    f.defineintegerintraopparallelismthreads 0 number of intraop parallelism threads  see tf.configproto for more details. use of this flag is unsupported$^    f.definebooleanuseallowgrowth false use allow growth flag which will allocate only required amount of gpu memory and prevent full allocation of available gpu memory$^    f.definebooleanloadcudnn false specifying this flag allows one to convert a cudnn rnn checkpoint to a checkpoint capable of running on a cpu graph.$^    f.definebooleantraincudnn false use cudnn rnn backend for training on gpu. note that checkpoints created with this flag can only be used with cudnn rnn i.e. fine tuning on a cpu device will not work$^    f.definebooleanautomaticmixedprecision false whether to allow automatic mixed precision training. use of this flag is unsupported. checkpoints created with automatic mixed precision training will not be usable without mixed precision.$^$^    f.definebooleanhorovod false use horovod for training on multiple gpus$^$^     sample limits$^$^    f.defineintegerlimittrain 0 maximum number of elements to use from train set  0 means no limit$^    f.defineintegerlimitdev 0 maximum number of elements to use from validation set  0 means no limit$^    f.defineintegerlimittest 0 maximum number of elements to use from test set  0 means no limit$^$^     sample order$^$^    f.definebooleanreversetrain false if to reverse sample order of the train set$^    f.definebooleanreversedev false if to reverse sample order of the dev set$^    f.definebooleanreversetest false if to reverse sample order of the test set$^$^     checkpointing$^$^    f.definestringcheckpointdir  directory from which checkpoints are loaded and to which they are saved  defaults to directory deepspeechcheckpoints within users data home specified by the xdg base directory specification$^    f.definestringloadcheckpointdir  directory in which checkpoints are stored  defaults to directory deepspeechcheckpoints within users data home specified by the xdg base directory specification$^    f.definestringsavecheckpointdir  directory to which checkpoints are saved  defaults to directory deepspeechcheckpoints within users data home specified by the xdg base directory specification$^    f.defineintegercheckpointsecs 000 checkpoint saving interval in seconds$^    f.defineintegermaxtokeep 0 number of checkpoint files to keep  default value is 0$^    f.definestringloadtrain auto what checkpoint to load before starting the training process. last for loading most recent epoch checkpoint best for loading best validation loss checkpoint init for initializing a new checkpoint auto for trying several options.$^    f.definestringloadevaluate auto what checkpoint to load for evaluation tasks test epochs model export single file inference etc. last for loading most recent epoch checkpoint best for loading best validation loss checkpoint auto for trying several options.$^$^     transfer learning$^$^    f.defineintegerdropsourcelayers 0 single integer for how many layers to drop from source model to drop just output  0 drop penultimate and output 0 etc$^$^     exporting$^$^    f.definestringexportdir  directory in which exported models are stored  if omitted the model wont get exported$^    f.definebooleanremoveexport false whether to remove old exported models$^    f.definebooleanexporttflite false export a graph ready for tf lite engine$^    f.defineintegernsteps 00 how many timesteps to process at once by the export graph higher values mean more latency$^    f.definebooleanexportzip false export a tflite model and package with lm and info.json$^    f.definestringexportfilename outputgraph name for the exported model file name$^    f.defineintegerexportbeamwidth 000 default beam width to embed into exported graph$^$^     model metadata$^$^    f.definestringexportauthorid author author of the exported model. github user or organization name used to uniquely identify the author of this model$^    f.definestringexportmodelname model name of the exported model. must not contain forward slashes.$^    f.definestringexportmodelversion 0.0.0 semantic version of the exported model. see httpssemver.org. this is fully controlled by you as author of the model and has no required connection with deepspeech versions$^$^    def strvalequalshelpname valdesc$^        f.definestringname .formatvaldesc valdesc$^$^    strvalequalshelpexportcontactinfo public contact information of the author. can be an email address or a link to a contact form issue tracker or discussion forum. must provide a way to reach the model authors$^    strvalequalshelpexportlicense spdx identifier of the license of the exported model. see httpsspdx.orglicenses. if the license does not have an spdx identifier use the license name.$^    strvalequalshelpexportlanguage language the model was trained on  ietf bcp 00 language tag including at least language script and region subtags. e.g. enlatnuk or delatnde or cmnhanscn. include as much info as you can without loss of precision. for example if a model is trained on scottish english include the variant subtag enlatngbscotland.$^    strvalequalshelpexportmindsversion minimum deepspeech version inclusive the exported model is compatible with$^    strvalequalshelpexportmaxdsversion maximum deepspeech version inclusive the exported model is compatible with$^    strvalequalshelpexportdescription freeform description of the model being exported. markdown accepted. you can also leave this flag unchanged and edit the generated .md file directly. useful things to describe are demographic and acoustic characteristics of the data used to train the model any architectural changes names of public datasets that were used when applicable hyperparameters used for training evaluation results on standard benchmark datasets etc.$^$^     reporting$^$^    f.defineintegerloglevel 0 log level for console logs  0 debug 0 info 0 warn 0 error$^    f.definebooleanshowprogressbar true show progress for training validation and testing processes. log level should be  0.$^$^    f.definebooleanlogplacement false whether to log device placement of the operators to the console$^    f.defineintegerreportcount 0 number of phrases for each of best wer median wer and worst wer to print out during a wer report$^$^    f.definestringsummarydir  target directory for tensorboard summaries  defaults to directory deepspeechsummaries within users data home specified by the xdg base directory specification$^$^    f.definestringtestoutputfile  path to a file to save all srcdecodeddistanceloss tuples generated during a test epoch$^$^     geometry$^$^    f.defineintegernhidden 0000 layer width to use when initialising layers$^    f.definebooleanlayernorm false wether to use layernormalization after each fullyconnected layer except the last one$^$^     initialization$^$^    f.defineintegerrandomseed 0000 default random seed that is used to initialize variables$^$^     early stopping$^$^    f.definebooleanearlystop false enable early stopping mechanism over validation dataset. if validation is not being run early stopping is disabled.$^    f.defineintegeresepochs 00 number of epochs with no improvement after which training will be stopped. loss is not stored in the checkpoint so when checkpoint is revived it starts the loss calculation from start at that point$^    f.definefloatesmindelta 0.00 minimum change in loss to qualify as an improvement. this value will also be used in reduce learning rate on plateau$^$^     reduce learning rate on plateau$^$^    f.definebooleanreducelronplateau false enable reducing the learning rate if a plateau is reached. this is the case if the validation loss did not improve for some epochs.$^    f.defineintegerplateauepochs 00 number of epochs to consider for rlrop. has to be smaller than esepochs from early stopping$^    f.definefloatplateaureduction 0.0 multiplicative factor to apply to the current learning rate if a plateau has occurred.$^    f.definebooleanforceinitializelearningrate false force reinitialization of learning rate which was previously reduced.$^$^     decoder$^$^    f.definebooleanbytesoutputmode false enable bytes output mode mode. when this is used the model outputs utf0 byte values directly rather than using an alphabet mapping. the alphabetconfigpath option will be ignored. see the training documentation for more details.$^    f.definestringalphabetconfigpath dataalphabet.txt path to the configuration file specifying the alphabet used by the network. see the comment in dataalphabet.txt for a description of the format.$^    f.definestringscorerpath  path to the external scorer file.$^    f.definealiasscorer scorerpath$^    f.defineintegerbeamwidth 0000 beam width used in the ctc decoder when building candidate transcriptions$^    f.definefloatlmalpha 0.000000000000000 the alpha hyperparameter of the ctc decoder. language model weight.$^    f.definefloatlmbeta 0.0000000000000000 the beta hyperparameter of the ctc decoder. word insertion weight.$^    f.definefloatcutoffprob 0.0 only consider characters until this probability mass is reached. 0.0  disabled.$^    f.defineintegercutofftopn 000 only process this number of characters sorted by probability mass for each time step. if bigger than alphabet size disabled.$^$^     inference mode$^$^    f.definestringoneshotinfer  oneshot inference mode specify a wav file and the script will load the checkpoint and perform inference on it.$^$^     optimizer mode$^$^    f.definefloatlmalphamax 0 the maximum of the alpha hyperparameter of the ctc decoder explored during hyperparameter optimization. language model weight.$^    f.definefloatlmbetamax 0 the maximum beta hyperparameter of the ctc decoder explored during hyperparameter optimization. word insertion weight.$^    f.defineintegerntrials 0000 the number of trials to run during hyperparameter optimization.$^$^     register validators for paths which require a file to be specified$^$^    f.registervalidatoralphabetconfigpath$^                         os.path.isfile$^                         messagethe file pointed to by alphabetconfigpath must exist and be readable.$^$^    f.registervalidatoroneshotinfer$^                         lambda value not value or os.path.isfilevalue$^                         messagethe file pointed to by oneshotinfer must exist and be readable.$^$^ sphinxdoc trainingrefflagsend$^$^a set of io utils that allow us to open files on remote storage as if they were present locally and access$^into hdfs storage using tensorflows c filestream api.$^currently only includes wrappers for googles gcs but this can easily be expanded for aws s0 buckets.$^$^import os$^from tensorflow.io import gfile$^$^$^def isremotepathpath$^    $^    returns true iff the path is one of the remote formats that this$^    module supports$^    $^    return path.startswithgs or path.startswithhdfs$^$^$^def pathexistsremotepath$^    $^    wrapper that allows existance check of local and remote paths like$^    gs...$^    $^    if isremotepathpath$^        return gfile.existspath$^    return os.path.existspath$^$^$^def copyremotesrc dst overwritefalse$^    $^    allows us to copy a file from local to remote or vice versa$^    $^    return gfile.copysrc dst overwrite$^$^$^def openremotepath moder buffering0 encodingnone newlinenone closefdtrue openernone$^    $^    wrapper around open method that can handle remote paths like gs...$^    off google cloud using tensorflows io helpers.$^$^    buffering encoding newline closefd and opener are ignored for remote files$^$^    this enables us to do$^    with openremotegs..... modew as f$^        do something with the file f whether or not we have local access to it$^    $^    if isremotepathpath$^        return gfile.gfilepath modemode$^    return openpath mode bufferingbuffering encodingencoding newlinenewline closefdclosefd openeropener$^$^$^def isdirremotepath$^    $^    wrapper to check if remote and local paths are directories$^    $^    if isremotepathpath$^        return gfile.isdirpath$^    return os.path.isdirpath$^$^$^def listdirremotepath$^    $^    wrapper to list paths in local dirs alternative to using a glob i suppose$^    $^    if isremotepathpath$^        return gfile.listdirpath$^    return os.listdirpath$^$^$^def globremotefilename$^    $^    wrapper that provides globs on local and remote paths like gs...$^    $^    return gfile.globfilename$^$^$^def removeremotefilename$^    $^    wrapper that can remove local and remote files like gs...$^    $^     conditional import$^    return gfile.removefilename$^import argparse$^import importlib$^import os$^import re$^import sys$^$^from .helpers import secstohours$^from collections import counter$^$^def getcounter$^    return counterall 0 failed 0 invalidlabel 0 tooshort 0 toolong 0 importedtime 0 totaltime 0$^$^def getimportedsamplescounter$^    return counterall  counterfailed  countertooshort  countertoolong  counterinvalidlabel$^$^def printimportreportcounter samplerate maxsecs$^    printimported d samples.  getimportedsamplescounter$^    if counterfailed  0$^        printskipped d samples that failed upon conversion.  counterfailed$^    if counterinvalidlabel  0$^        printskipped d samples that failed on transcript validation.  counterinvalidlabel$^    if countertooshort  0$^        printskipped d samples that were too short to match the transcript.  countertooshort$^    if countertoolong  0$^        printskipped d samples that were longer than d seconds.  countertoolong maxsecs$^    printfinal amount of imported audio s from s.  secstohourscounterimportedtime  samplerate secstohourscountertotaltime  samplerate$^$^def getimportersparserdescription$^    parser  argparse.argumentparserdescriptiondescription$^    parser.addargumentvalidatelabellocale helppath to a python file defining a validatelabel function for your locale. warning this will add this files directory into pythonpath.$^    return parser$^$^def getvalidatelabelargs$^    $^    expects an argparse.namespace argument to search for validatelabellocale parameter.$^    if found this will modify pythons library search path and add the directory of the$^    file pointed by the validatelabellocale argument.$^$^    param args the importers cli argument object$^    type args argparse.namespace$^$^    return the usersupplied validatelabel function$^    type function$^    $^     python 0.0 does not support passing a pathlib.path to os.path. methods$^    if validatelabellocale not in args or args.validatelabellocale is none$^        printwarning no validatelabellocale specified your might end with inconsistent dataset.$^        return validatelabeleng$^    validatelabellocale  strargs.validatelabellocale$^    if not os.path.existsos.path.abspathvalidatelabellocale$^        printerror inexistent validatelabellocale specified. please check.$^        return none$^    moduledir  os.path.abspathos.path.dirnamevalidatelabellocale$^    sys.path.insert0 moduledir$^    fname  os.path.basenamevalidatelabellocale.replace.py $^    localemodule  importlib.importmodulefname packagenone$^    return localemodule.validatelabel$^$^ validate and normalize transcriptions. returns a cleaned version of the label$^ or none if its invalid.$^def validatelabelenglabel$^     for now we can only handle az $^    if re.searchr00 label is not none$^        return none$^$^    label  label.replace  $^    label  label.replace  $^    label  re.sub 0   label$^    label  label.replace. $^    label  label.replace $^    label  label.replace $^    label  label.replace $^    label  label.replace $^    label  label.replace $^    label  label.replace $^    label  label.strip$^    label  label.lower$^$^    return label if label else none$^import sys$^import tensorflow as tf$^import tensorflow.compat.v0 as tfv0$^$^from .flags import flags$^from .logging import loginfo logerror logwarn$^$^$^def loadcheckpointsession checkpointpath allowdroplayers allowlrinittrue$^     load the checkpoint and put all variables into loading list$^     we will exclude variables we do not wish to load and then$^     we will initialize them instead$^    ckpt  tfv0.train.loadcheckpointcheckpointpath$^    varsinckpt  frozensetckpt.getvariabletoshapemap.keys$^    loadvars  settfv0.globalvariables$^    initvars  set$^$^     we explicitly allow the learning rate variable to be missing for backwards$^     compatibility with older checkpoints.$^    lrvar  setv for v in loadvars if v.op.name  learningrate$^    if lrvar and learningrate not in varsinckpt or$^                    flags.forceinitializelearningrate and allowlrinit$^        assert lenlrvar  0$^        loadvars  lrvar$^        initvars  lrvar$^$^    if flags.loadcudnn$^         initialize training from a cudnn rnn checkpoint$^         identify the variables which we cannot load and set them$^         for initialization$^        missingvars  set$^        for v in loadvars$^            if v.op.name not in varsinckpt$^                logwarncudnn variable not found s  v.op.name$^                missingvars.addv$^                initvars.addv$^$^        loadvars  initvars$^$^         check that the only missing variables i.e. those to be initialised$^         are the adam moment tensors if they arent then we have an issue$^        missingvarnames  v.op.name for v in missingvars$^        if anyadam not in v for v in missingvarnames$^            logerrortried to load a cudnn rnn checkpoint but there were $^                      more missing variables than just the adam moment $^                      tensors. missing variables .formatmissingvarnames$^            sys.exit0$^$^    if allowdroplayers and flags.dropsourcelayers  0$^         this transfer learning approach requires supplying$^         the layers which we exclude from the source model.$^         say we want to exclude all layers except for the first one$^         then we are dropping five layers total so dropsourcelayers0$^         if we want to use all layers from the source model except$^         the last one we use this dropsourcelayers0$^        if flags.dropsourcelayers  0$^            logwarnthe checkpoint only has 0 layers but you are trying to drop $^                     all of them or more than all of them. continuing and $^                     dropping only 0 layers.$^            flags.dropsourcelayers  0$^$^        droppedlayers  0 0 lstm 0 00  intflags.dropsourcelayers$^         initialize all variables needed for ds but not loaded from ckpt$^        for v in loadvars$^            if anylayer in v.op.name for layer in droppedlayers$^                initvars.addv$^        loadvars  initvars$^$^    for v in sortedloadvars keylambda v v.op.name$^        loginfoloading variable from checkpoint s  v.op.name$^        v.loadckpt.gettensorv.op.name sessionsession$^$^    for v in sortedinitvars keylambda v v.op.name$^        loginfoinitializing variable s  v.op.name$^        session.runv.initializer$^$^$^def checkpointpathornonecheckpointfilename$^    checkpoint  tfv0.train.getcheckpointstateflags.loadcheckpointdir checkpointfilename$^    if not checkpoint$^        return none$^    return checkpoint.modelcheckpointpath$^$^$^def initializeallvariablessession$^    initvars  tfv0.globalvariables$^    for v in initvars$^        session.runv.initializer$^$^$^def loadorinitimplsession methodorder allowdroplayers allowlrinittrue$^    for method in methodorder$^         load best validating checkpoint saved in checkpoint file bestdevcheckpoint$^        if method  best$^            ckptpath  checkpointpathornonebestdevcheckpoint$^            if ckptpath$^                loginfoloading best validating checkpoint from .formatckptpath$^                return loadcheckpointsession ckptpath allowdroplayers allowlrinitallowlrinit$^            loginfocould not find best validating checkpoint.$^$^         load most recent checkpoint saved in checkpoint file checkpoint$^        elif method  last$^            ckptpath  checkpointpathornonecheckpoint$^            if ckptpath$^                loginfoloading most recent checkpoint from .formatckptpath$^                return loadcheckpointsession ckptpath allowdroplayers allowlrinitallowlrinit$^            loginfocould not find most recent checkpoint.$^$^         initialize all variables$^        elif method  init$^            loginfoinitializing all variables.$^            return initializeallvariablessession$^$^        else$^            logerrorunknown initialization method .formatmethod$^            sys.exit0$^$^    logerrorall initialization methods failed ..formatmethodorder$^    sys.exit0$^$^$^def reloadbestcheckpointsession$^    loadorinitimplsession best allowdroplayersfalse allowlrinitfalse$^$^$^def loadorinitgraphfortrainingsession$^    $^    load variables from checkpoint or initialize variables. by default this will$^    try to load the best validating checkpoint then try the last checkpoint$^    and finally initialize the weights from scratch. this can be overriden with$^    the loadtrain flag. see its documentation for more info.$^    $^    if flags.loadtrain  auto$^        methods  best last init$^    else$^        methods  flags.loadtrain$^    loadorinitimplsession methods allowdroplayerstrue$^$^$^def loadgraphforevaluationsession$^    $^    load variables from checkpoint. initialization is not allowed. by default$^    this will try to load the best validating checkpoint then try the last$^    checkpoint. this can be overriden with the loadevaluate flag. see its$^    documentation for more info.$^    $^    if flags.loadevaluate  auto$^        methods  best last$^    else$^        methods  flags.loadevaluate$^    loadorinitimplsession methods allowdroplayersfalse$^from future import absoluteimport division printfunction$^$^import numpy as np$^import struct$^$^def texttochararraytranscript alphabet context$^    r$^    given a transcript string map characters to$^    integers and return a numpy array representing the processed string.$^    use a string in context for adding text to raised exceptions.$^    $^    if not alphabet.canencodetranscript$^         provide the row context especially wavfilename for alphabet errors$^        raise valueerror$^            alphabet cannot encode transcript  while processing sample  $^            check that your alphabet contains all characters in the training corpus. $^            missing characters are .$^            .formattranscript context listch for ch in transcript if not alphabet.canencodesinglech$^$^    encoded  alphabet.encodetranscript$^    if lenencoded  0$^        raise valueerrorwhile processing  found an empty transcript $^                         you must include a transcript for all training data.$^                         .formatcontext$^    return encoded$^$^$^ the following code is from httphetland.orgcodingpythonlevenshtein.py$^$^ this is a straightforward implementation of a wellknown algorithm and thus$^ probably shouldnt be covered by copyright to begin with. but in case it is$^ the author magnus lie hetland has to the extent possible under law$^ dedicated all copyright and related and neighboring rights to this software$^ to the public domain worldwide by distributing it under the cc0 license$^ version 0.0. this software is distributed without any warranty. for more$^ information see httpcreativecommons.orgpublicdomainzero0.0$^$^def levenshteina b$^    calculates the levenshtein distance between a and b.$^    n m  lena lenb$^    if n  m$^         make sure n  m to use ominnm space$^        a b  b a$^        n m  m n$^$^    current  listrangen0$^    for i in range0 m0$^        previous current  current i0n$^        for j in range0 n0$^            add delete  previousj0 currentj00$^            change  previousj0$^            if aj0  bi0$^                change  change  0$^            currentj  minadd delete change$^$^    return currentn$^$^usage$^ from within the training directory call this script as a module$^$^        python0 m deepspeechtraining.util.checkcharacters infile$^ e.g.   python0 m deepspeechtraining.util.checkcharacters csv homedatafrench.csv$^ e.g.   python0 m deepspeechtraining.util.checkcharacters csv ..train.csv..test.csv$^ e.g.   python0 m deepspeechtraining.util.checkcharacters alpha csv ..train.csv$^$^point this script to your transcripts and it returns$^to the terminal the unique set of characters in those$^files combined.$^$^these files are assumed to be csv with the transcript being the third field.$^$^the script simply reads all the text from all the files$^storing a set of unique characters that were seen$^along the way.$^$^import argparse$^import csv$^import os$^import sys$^import unicodedata$^from .io import openremote$^$^def main$^    parser  argparse.argumentparser$^$^    parser.addargumentcsv csvfiles helpstr. filenames as a comma separated list requiredtrue$^    parser.addargumentalpha alphabetformat helpbool. print in format for alphabet.txt actionstoretrue$^    parser.addargumentunicode disableunicodevariants helpbool. disable check for unicode consistency use with alphabetformat actionstoretrue$^    args  parser.parseargs$^    infiles  args.csvfiles.split$^$^    print reading in the following transcript files $^    print  .formatinfiles$^$^    alltext  set$^    for infile in infiles$^        with openremoteinfile r as csvfile$^            reader  csv.readercsvfile$^            try$^                nextreader none   skip the file header i.e. transcript$^                for row in reader$^                    if not args.disableunicodevariants$^                        unicodetranscript  unicodedata.normalizenfkc row0$^                        if row0  unicodetranscript$^                            printyour input file infile contains at least one transript with unicode chars on more than one codepoint . consider using nfkc normalization unicodedata.normalizenfkc str..formatrow0$^                            sys.exit0$^                    alltext  setrow0$^            except indexerror$^                printyour input file infile is not formatted properly. check if there are 0 columns with the 0rd containing the transcript$^                sys.exit0$^            finally$^                csvfile.close$^$^    print the following unique characters were found in your transcripts $^    if args.alphabetformat$^        for char in listalltext$^            printchar$^        print  you can copypaste these into dataalphabet.txt $^    else$^        printlistalltext$^$^if name  main$^    main$^import collections$^import ctypes$^import io$^import math$^import numpy as np$^import os$^import pyogg$^import tempfile$^import wave$^$^from .helpers import limitingpool$^from collections import namedtuple$^from .io import openremote removeremote copyremote isremotepath$^$^audioformat  namedtupleaudioformat rate channels width$^$^defaultrate  00000$^defaultchannels  0$^defaultwidth  0$^defaultformat  audioformatdefaultrate defaultchannels defaultwidth$^$^audiotypenp  applicationvnd.mozilla.np$^audiotypepcm  applicationvnd.mozilla.pcm$^audiotypewav  audiowav$^audiotypeopus  applicationvnd.mozilla.opus$^audiotypeoggopus  applicationvnd.deepspeech.oggopus$^$^serializableaudiotypes  audiotypewav audiotypeopus audiotypeoggopus$^$^opuspcmlensize  0$^opusratesize  0$^opuschannelssize  0$^opuswidthsize  0$^opuschunklensize  0$^$^$^class sample$^    $^    represents inmemory audio data of a certain convertible representation.$^$^    attributes$^    $^    audiotype  str$^        see init.$^    audioformat  util.audio.audioformat$^        see init.$^    audio  binary$^        audio data represented as indicated by audiotype$^    duration  float$^        audio duration of the sample in seconds$^    $^    def initself audiotype rawdata audioformatnone sampleidnone$^        $^        parameters$^        $^        audiotype  str$^            audio data representation type$^            supported types$^                 util.audio.audiotypeopus memory file representation bytesio of opus encoded audio$^                    wrapped by a custom container format used in sdbs$^                 util.audio.audiotypewav memory file representation bytesio of a wave file$^                 util.audio.audiotypepcm binary representation bytearray of pcm encoded audio data wave file without header$^                 util.audio.audiotypenp numpy representation of audio data np.float00  typically used for gpu feeding$^        rawdata  binary$^            audio data in the form of the provided representation type see audiotype.$^            for types util.audio.audiotypeopus or util.audio.audiotypewav data can also be passed as a bytearray.$^        audioformat  util.audio.audioformat$^            required in case of audiotype  util.audio.audiotypepcm or util.audio.audiotypenp$^            as this information cannot be derived from raw audio data.$^        sampleid  str$^            tracking id  should indicate samples origin as precisely as possible$^        $^        self.audiotype  audiotype$^        self.audioformat  audioformat$^        self.sampleid  sampleid$^        if audiotype in serializableaudiotypes$^            self.audio  rawdata if isinstancerawdata io.bytesio else io.bytesiorawdata$^            self.duration  readdurationaudiotype self.audio$^            if not self.audioformat$^                self.audioformat  readformataudiotype self.audio$^        else$^            self.audio  rawdata$^            if self.audioformat is none$^                raise valueerrorfor audio type  parameter audioformat is mandatory.formatself.audiotype$^            if audiotype  audiotypepcm$^                self.duration  getpcmdurationlenself.audio self.audioformat$^            elif audiotype  audiotypenp$^                self.duration  getnpdurationlenself.audio self.audioformat$^            else$^                raise valueerrorunsupported audio type .formatself.audiotype$^$^    def changeaudiotypeself newaudiotype bitratenone$^        $^        inplace conversion of audio data into a different representation.$^$^        parameters$^        $^        newaudiotype  str$^            new audiotype  see init.$^        bitrate  int$^            bitrate to use in case of converting to a lossy audiotype.$^        $^        if self.audiotype  newaudiotype$^            return$^        if newaudiotype  audiotypepcm and self.audiotype in serializableaudiotypes$^            self.audioformat audio  readaudioself.audiotype self.audio$^            self.audio.close$^            self.audio  audio$^        elif newaudiotype  audiotypepcm and self.audiotype  audiotypenp$^            self.audio  nptopcmself.audio self.audioformat$^        elif newaudiotype  audiotypenp$^            self.changeaudiotypeaudiotypepcm$^            self.audio  pcmtonpself.audio self.audioformat$^        elif newaudiotype in serializableaudiotypes$^            self.changeaudiotypeaudiotypepcm$^            audiobytes  io.bytesio$^            writeaudionewaudiotype audiobytes self.audio audioformatself.audioformat bitratebitrate$^            audiobytes.seek0$^            self.audio  audiobytes$^        else$^            raise runtimeerrorchanging audio representation type from  to  not supported$^                               .formatself.audiotype newaudiotype$^        self.audiotype  newaudiotype$^$^$^def unpackandchangeaudiotypesampleandaudiotype$^    packedsample audiotype bitrate  sampleandaudiotype$^    if hasattrpackedsample unpack$^        sample  packedsample.unpack$^    else$^        sample  packedsample$^    sample.changeaudiotypeaudiotype bitratebitrate$^    return sample$^$^$^def changeaudiotypespackedsamples audiotypeaudiotypepcm bitratenone processesnone processaheadnone$^    with limitingpoolprocessesprocesses processaheadprocessahead as pool$^        yield from pool.imapunpackandchangeaudiotype maplambda s s audiotype bitrate packedsamples$^$^$^def getloadableaudiotypefromextensionext$^    return $^        .wav audiotypewav$^        .opus audiotypeoggopus$^    .getext none$^$^$^def readaudioformatfromwavfilewavfile$^    return audioformatwavfile.getframerate wavfile.getnchannels wavfile.getsampwidth$^$^$^def getnumsamplespcmbuffersize audioformatdefaultformat$^    return pcmbuffersize  audioformat.channels  audioformat.width$^$^$^def getpcmdurationpcmbuffersize audioformatdefaultformat$^    calculates duration in seconds of a binary pcm buffer typically read from a wav file$^    return getnumsamplespcmbuffersize audioformat  audioformat.rate$^$^$^def getnpdurationnplen audioformatdefaultformat$^    calculates duration in seconds of numpy audio data$^    return nplen  audioformat.rate$^$^$^def convertaudiosrcaudiopath dstaudiopath filetypenone audioformatdefaultformat$^    import sox$^    transformer  sox.transformer$^    transformer.setoutputformatfiletypefiletype$^                                  rateaudioformat.rate$^                                  channelsaudioformat.channels$^                                  bitsaudioformat.width  0$^    transformer.buildsrcaudiopath dstaudiopath$^$^$^class audiofile$^    $^    audio data file wrapper that ensures that the file is loaded with the correct sample rate channels$^    and width and converts the file on the fly otherwise.$^    $^    def initself audiopath aspathfalse audioformatdefaultformat$^        self.audiopath  audiopath$^        self.audioformat  audioformat$^        self.aspath  aspath$^        self.openfile  none$^        self.openwav  none$^        self.tmpfilepath  none$^        self.tmpsrcfilepath  none$^$^    def enterself$^        if self.audiopath.endswith.wav$^            self.openfile  openremoteself.audiopath rb$^            self.openwav  wave.openself.openfile$^            if readaudioformatfromwavfileself.openwav  self.audioformat$^                if self.aspath$^                    self.openwav.close$^                    self.openfile.close$^                    return self.audiopath$^                return self.openwav$^            self.openwav.close$^            self.openfile.close$^$^         if the format isnt right copy the file to local tmp dir and do the conversion on disk$^        if isremotepathself.audiopath$^             self.tmpsrcfilepath  tempfile.mkstempsuffix.wav$^            copyremoteself.audiopath self.tmpsrcfilepath true$^            self.audiopath  self.tmpsrcfilepath$^$^         self.tmpfilepath  tempfile.mkstempsuffix.wav$^        convertaudioself.audiopath self.tmpfilepath filetypewav audioformatself.audioformat$^        if self.aspath$^            return self.tmpfilepath$^        self.openwav  wave.openself.tmpfilepath rb$^        return self.openwav$^$^    def exitself args$^        if not self.aspath$^            self.openwav.close$^            if self.openfile$^                self.openfile.close$^        if self.tmpfilepath is not none$^            os.removeself.tmpfilepath$^        if self.tmpsrcfilepath is not none$^            os.removeself.tmpsrcfilepath$^$^$^def readframeswavfile framedurationms00 yieldremainderfalse$^    audioformat  readaudioformatfromwavfilewavfile$^    framesize  intaudioformat.rate  framedurationms  0000.0$^    while true$^        try$^            data  wavfile.readframesframesize$^            if not yieldremainder and getpcmdurationlendata audioformat  0000  framedurationms$^                break$^            yield data$^        except eoferror$^            break$^$^$^def readframesfromfileaudiopath audioformatdefaultformat framedurationms00 yieldremainderfalse$^    with audiofileaudiopath audioformataudioformat as wavfile$^        for frame in readframeswavfile framedurationmsframedurationms yieldremainderyieldremainder$^            yield frame$^$^$^def vadsplitaudioframes$^              audioformatdefaultformat$^              numpaddingframes00$^              threshold0.0$^              aggressiveness0$^    from webrtcvad import vad   pylint disableimportoutsidetoplevel$^    if audioformat.channels  0$^        raise valueerrorvadsplitting requires mono samples$^    if audioformat.width  0$^        raise valueerrorvadsplitting requires 00 bit samples$^    if audioformat.rate not in 0000 00000 00000 00000$^        raise valueerrorvadsplitting only supported for sample rates 0000 00000 00000 or 00000$^    if aggressiveness not in 0 0 0 0$^        raise valueerrorvadsplitting aggressiveness mode has to be one of 0 0 0 or 0$^    ringbuffer  collections.dequemaxlennumpaddingframes$^    triggered  false$^    vad  vadintaggressiveness$^    voicedframes  $^    framedurationms  0$^    frameindex  0$^    for frameindex frame in enumerateaudioframes$^        framedurationms  getpcmdurationlenframe audioformat  0000$^        if intframedurationms not in 00 00 00$^            raise valueerrorvadsplitting only supported for frame durations 00 00 or 00 ms$^        isspeech  vad.isspeechframe audioformat.rate$^        if not triggered$^            ringbuffer.appendframe isspeech$^            numvoiced  lenf for f speech in ringbuffer if speech$^            if numvoiced  threshold  ringbuffer.maxlen$^                triggered  true$^                for f s in ringbuffer$^                    voicedframes.appendf$^                ringbuffer.clear$^        else$^            voicedframes.appendframe$^            ringbuffer.appendframe isspeech$^            numunvoiced  lenf for f speech in ringbuffer if not speech$^            if numunvoiced  threshold  ringbuffer.maxlen$^                triggered  false$^                yield b.joinvoicedframes $^                      framedurationms  max0 frameindex  lenvoicedframes $^                      framedurationms  frameindex$^                ringbuffer.clear$^                voicedframes  $^    if lenvoicedframes  0$^        yield b.joinvoicedframes $^              framedurationms  frameindex  lenvoicedframes $^              framedurationms  frameindex  0$^$^$^def packnumbern numbytes$^    return n.tobytesnumbytes big signedfalse$^$^$^def unpacknumberdata$^    return int.frombytesdata big signedfalse$^$^$^def getopusframesizerate$^    return 00  rate  0000$^$^$^def writeopusopusfile audiodata audioformatdefaultformat bitratenone$^    framesize  getopusframesizeaudioformat.rate$^    import opuslib   pylint disableimportoutsidetoplevel$^    encoder  opuslib.encoderaudioformat.rate audioformat.channels audio$^    if bitrate is not none$^        encoder.bitrate  bitrate$^    chunksize  framesize  audioformat.channels  audioformat.width$^    opusfile.writepacknumberlenaudiodata opuspcmlensize$^    opusfile.writepacknumberaudioformat.rate opusratesize$^    opusfile.writepacknumberaudioformat.channels opuschannelssize$^    opusfile.writepacknumberaudioformat.width opuswidthsize$^    for i in range0 lenaudiodata chunksize$^        chunk  audiodataii  chunksize$^         preventing nondeterministic encoding results from uninitialized remainder of the encoder buffer$^        if lenchunk  chunksize$^            chunk  chunk  b0  chunksize  lenchunk$^        encoded  encoder.encodechunk framesize$^        opusfile.writepacknumberlenencoded opuschunklensize$^        opusfile.writeencoded$^$^$^def readopusheaderopusfile$^    opusfile.seek0$^    pcmbuffersize  unpacknumberopusfile.readopuspcmlensize$^    rate  unpacknumberopusfile.readopusratesize$^    channels  unpacknumberopusfile.readopuschannelssize$^    width  unpacknumberopusfile.readopuswidthsize$^    return pcmbuffersize audioformatrate channels width$^$^$^def readopusopusfile$^    pcmbuffersize audioformat  readopusheaderopusfile$^    framesize  getopusframesizeaudioformat.rate$^    import opuslib   pylint disableimportoutsidetoplevel$^    decoder  opuslib.decoderaudioformat.rate audioformat.channels$^    audiodata  bytearray$^    while lenaudiodata  pcmbuffersize$^        chunklen  unpacknumberopusfile.readopuschunklensize$^        chunk  opusfile.readchunklen$^        decoded  decoder.decodechunk framesize$^        audiodata.extenddecoded$^    audiodata  audiodatapcmbuffersize$^    return audioformat bytesaudiodata$^$^$^def readoggopusoggfile$^    error  ctypes.cint$^    oggfilebuffer  oggfile.getbuffer$^    ubytearray  ctypes.cubyte  lenoggfilebuffer$^    opusfile  pyogg.opus.opopenmemory$^        ubytearray.frombufferoggfilebuffer$^        lenoggfilebuffer$^        ctypes.pointererror$^    $^$^    if error.value  0$^        raise valueerror$^            oggopus buffer could not be read.$^             error code .formaterror.value$^        $^$^    channelcount  pyogg.opus.opchannelcountopusfile 0$^    samplerate  00000  opus files are always 00khz$^    samplewidth  0  always 00bit$^    audioformat  audioformatsamplerate channelcount samplewidth$^$^     allocate sufficient memory to store the entire pcm$^    pcmsize  pyogg.opus.oppcmtotalopusfile 0$^    buf  pyogg.opus.opusint00pcmsizechannelcount$^    buf  buf$^$^     create a pointer to the newly allocated memory.  it$^     seems we can only do pointer arithmetic on void$^     pointers.  see$^     httpsmattgwwalker.wordpress.com00000000pointermanipulationinpython$^    bufptr  ctypes.cast$^        ctypes.pointerbuf$^        ctypes.cvoidp$^    $^    assert bufptr.value is not none  for mypy$^    bufptrzero  bufptr.value$^$^     bytes per sample$^    bytespersample  ctypes.sizeofpyogg.opus.opusint00$^$^     read through the entire file copying the pcm into the$^     buffer$^    samples  0$^    while true$^         calculate remaining buffer size$^        remainingbuffer  $^            lenbuf  int$^             bufptr.value  bufptrzero  bytespersample$^        $^$^         convert buffer pointer to the desired type$^        ptr  ctypes.cast$^            bufptr$^            ctypes.pointerpyogg.opus.opusint00$^        $^$^         read the next section of pcm$^        ns  pyogg.opus.opread$^            opusfile$^            ptr$^            remainingbuffer$^            pyogg.ogg.cintp$^        $^$^         check for errors$^        if ns  0$^            raise valueerror$^                error while reading oggopus buffer. $^                error code .formatns$^            $^$^         increment the pointer$^        bufptr.value  $^            ns$^             bytespersample$^             channelcount$^        $^        assert bufptr.value is not none  for mypy$^$^        samples  ns$^$^         check if weve finished$^        if ns  0$^            break$^$^     close the open file$^    pyogg.opus.opfreeopusfile$^$^     cast buffer to a onedimensional array of chars$^     raw pcm data from audio file.$^    charbuffer  ctypes.cbyte  bytespersample  channelcount  pcmsize$^    audiodata  charbuffer.frombufferbuf$^$^    return audioformat audiodata$^$^$^def writewavwavfile pcmdata audioformatdefaultformat$^     wavfile is already a filepointer here$^    with wave.openwavfile wb as wavfilewriter$^        wavfilewriter.setframerateaudioformat.rate$^        wavfilewriter.setnchannelsaudioformat.channels$^        wavfilewriter.setsampwidthaudioformat.width$^        wavfilewriter.writeframespcmdata$^$^$^def readwavwavfile$^    wavfile.seek0$^    with wave.openwavfile rb as wavfilereader$^        audioformat  readaudioformatfromwavfilewavfilereader$^        pcmdata  wavfilereader.readframeswavfilereader.getnframes$^        return audioformat pcmdata$^$^$^def readaudioaudiotype audiofile$^    if audiotype  audiotypewav$^        return readwavaudiofile$^    if audiotype  audiotypeopus$^        return readopusaudiofile$^    if audiotype  audiotypeoggopus$^        return readoggopusaudiofile$^    raise valueerrorunsupported audio type .formataudiotype$^$^$^def writeaudioaudiotype audiofile pcmdata audioformatdefaultformat bitratenone$^    if audiotype  audiotypewav$^        return writewavaudiofile pcmdata audioformataudioformat$^    if audiotype  audiotypeopus$^        return writeopusaudiofile pcmdata audioformataudioformat bitratebitrate$^    raise valueerrorunsupported audio type .formataudiotype$^$^$^def readwavdurationwavfile$^    wavfile.seek0$^    with wave.openwavfile rb as wavfilereader$^        return wavfilereader.getnframes  wavfilereader.getframerate$^$^$^def readopusdurationopusfile$^    pcmbuffersize audioformat  readopusheaderopusfile$^    return getpcmdurationpcmbuffersize audioformat$^$^$^def readoggopusdurationoggfile$^    error  ctypes.cint$^    oggfilebuffer  oggfile.getbuffer$^    ubytearray  ctypes.cubyte  lenoggfilebuffer$^    opusfile  pyogg.opus.opopenmemory$^        ubytearray.frombufferoggfilebuffer$^        lenoggfilebuffer$^        ctypes.pointererror$^    $^$^    if error.value  0$^        raise valueerror$^            oggopus buffer could not be read.$^             error code .formaterror.value$^        $^$^    pcmbuffersize  pyogg.opus.oppcmtotalopusfile 0$^    channelcount  pyogg.opus.opchannelcountopusfile 0$^    samplerate  00000  opus files are always 00khz$^    samplewidth  0  always 00bit$^    audioformat  audioformatsamplerate channelcount samplewidth$^    pyogg.opus.opfreeopusfile$^    return getpcmdurationpcmbuffersize audioformat$^$^$^def readdurationaudiotype audiofile$^    if audiotype  audiotypewav$^        return readwavdurationaudiofile$^    if audiotype  audiotypeopus$^        return readopusdurationaudiofile$^    if audiotype  audiotypeoggopus$^        return readoggopusdurationaudiofile$^    raise valueerrorunsupported audio type .formataudiotype$^$^$^def readwavformatwavfile$^    wavfile.seek0$^    with wave.openwavfile rb as wavfilereader$^        return readaudioformatfromwavfilewavfilereader$^$^$^def readopusformatopusfile$^     audioformat  readopusheaderopusfile$^    return audioformat$^$^$^def readoggopusformatoggfile$^    error  ctypes.cint$^    oggfilebuffer  oggfile.getbuffer$^    ubytearray  ctypes.cubyte  lenoggfilebuffer$^    opusfile  pyogg.opus.opopenmemory$^        ubytearray.frombufferoggfilebuffer$^        lenoggfilebuffer$^        ctypes.pointererror$^    $^$^    if error.value  0$^        raise valueerror$^            oggopus buffer could not be read.$^             error code .formaterror.value$^        $^$^    channelcount  pyogg.opus.opchannelcountopusfile 0$^    pyogg.opus.opfreeopusfile$^$^    samplerate  00000  opus files are always 00khz$^    samplewidth  0  always 00bit$^    return audioformatsamplerate channelcount samplewidth$^$^$^def readformataudiotype audiofile$^    if audiotype  audiotypewav$^        return readwavformataudiofile$^    if audiotype  audiotypeopus$^        return readopusformataudiofile$^    if audiotype  audiotypeoggopus$^        return readoggopusformataudiofile$^    raise valueerrorunsupported audio type .formataudiotype$^$^$^def getdtypeaudioformat$^    if audioformat.width not in 0 0 0$^        raise valueerrorunsupported sample width .formataudioformat.width$^    return none np.int0 np.int00 none np.int00audioformat.width$^$^$^def pcmtonppcmdata audioformatdefaultformat$^    $^    converts pcm data e.g. read from a wavfile into a mono numpy column vector$^    with values in the range 0.0 0.0.$^    $^     handles both mono and stero audio$^    dtype  getdtypeaudioformat$^    samples  np.frombufferpcmdata dtypedtype$^$^     read interleaved channels$^    nchannels  audioformat.channels$^    samples  samples.reshapeintlensamplesnchannels nchannels$^    $^     convert to 0.00.0 range$^    samples  samples.astypenp.float00  np.iinfodtype.max$^$^     average multichannel clips into mono and turn into column vector$^    return np.expanddimsnp.meansamples axis0 axis0$^$^$^def nptopcmnpdata audioformatdefaultformat$^    dtype  getdtypeaudioformat$^    npdata  npdata.squeeze$^    npdata  npdata  np.iinfodtype.max$^    npdata  npdata.astypedtype$^    return npdata.tobytes$^$^$^def rmstodbfsrms$^    return 00.0  math.log00max0e00 rms  0.0000$^$^$^def maxdbfssampledata$^     peak dbfs based on the maximum energy sample. will prevent overdrive if used for normalization.$^    return rmstodbfsmaxabsnp.minsampledata absnp.maxsampledata$^$^$^def meandbfssampledata$^    return rmstodbfsmath.sqrtnp.meannp.squaresampledata dtypenp.float00$^$^$^def gaindbtoratiogaindb$^    return math.pow00.0 gaindb  00.0$^$^$^def normalizeaudiosampledata dbfs0.0000$^    return np.maximumnp.minimumsampledata  gaindbtoratiodbfs  maxdbfssampledata 0.0 0.0$^import codecs$^import unicodedata$^$^class stmsegmentobject$^    r$^    representation of an individual segment in an stm file.$^    $^    def initself stmline$^        tokens  stmline.split$^        self.filename     tokens0$^        self.channel      tokens0$^        self.speakerid   tokens0$^        self.starttime   floattokens0$^        self.stoptime    floattokens0$^        self.labels       tokens0$^        self.transcript   $^        for token in tokens0$^          self.transcript  token   $^         we need to do the encodedecode dance here because encode$^         returns a bytes object on python 0 and texttochararray$^         expects a string.$^        self.transcript  unicodedata.normalizenfkd self.transcript.strip  $^                                      .encodeascii ignore                    $^                                      .decodeascii ignore$^$^    property$^    def filenameself$^        return self.filename$^$^    property$^    def channelself$^        return self.channel$^$^    property$^    def speakeridself$^        return self.speakerid$^$^    property$^    def starttimeself$^        return self.starttime$^$^    property$^    def stoptimeself$^        return self.stoptime$^$^    property$^    def labelsself$^        return self.labels$^$^    property$^    def transcriptself$^        return self.transcript$^$^def parsestmfilestmfile$^    r$^    parses an stm file at stmfile into a list of classstmsegment.$^    $^    stmsegments  $^    with codecs.openstmfile encodingutf0 as stmlines$^        for stmline in stmlines$^            stmsegment  stmsegmentstmline$^            if not ignoretimesegmentinscoring  stmsegment.transcript$^                stmsegments.appendstmsegment$^    return stmsegments$^from tensorflow.python.client import devicelib$^$^$^def getavailablegpusconfig$^    r$^    returns the number of gpus available on this system.$^    $^    localdeviceprotos  devicelib.listlocaldevicessessionconfigconfig$^    return x.name for x in localdeviceprotos if x.devicetype  gpu$^usrbinenv python$^  coding utf0 $^from future import absoluteimport division printfunction$^$^import json$^from multiprocessing.dummy import pool$^$^import numpy as np$^from attrdict import attrdict$^$^from .flags import flags$^from .text import levenshtein$^from .io import openremote$^$^def pmapfun iterable$^    pool  pool$^    results  pool.mapfun iterable$^    pool.close$^    return results$^$^$^def wercerbatchsamples$^    r$^    the wer is defined as the editlevenshtein distance on word level divided by$^    the amount of words in the original text.$^    in case of the original having more words n than the result and both$^    being totally different all n words resulting in 0 edit operation each$^    the wer will always be 0 n  n  0.$^    $^    wer  sums.worddistance for s in samples  sums.wordlength for s in samples$^    cer  sums.chardistance for s in samples  sums.charlength for s in samples$^$^    wer  minwer 0.0$^    cer  mincer 0.0$^$^    return wer cer$^$^$^def processdecoderesultitem$^    wavfilename groundtruth prediction loss  item$^    chardistance  levenshteingroundtruth prediction$^    charlength  lengroundtruth$^    worddistance  levenshteingroundtruth.split prediction.split$^    wordlength  lengroundtruth.split$^    return attrdict$^        wavfilename wavfilename$^        src groundtruth$^        res prediction$^        loss loss$^        chardistance chardistance$^        charlength charlength$^        worddistance worddistance$^        wordlength wordlength$^        cer chardistance  charlength$^        wer worddistance  wordlength$^    $^$^$^def calculateandprintreportwavfilenames labels decodings losses datasetname$^    r$^    this routine will calculate and print a wer report.$^    itll compute the mean wer and create sample objects of the reportcount top lowest$^    loss items from the provided wer results tuple only items with wer0 and ordered by their wer.$^    $^    samples  pmapprocessdecoderesult zipwavfilenames labels decodings losses$^$^     getting the wer and cer from the accumulated edit distances and lengths$^    sampleswer samplescer  wercerbatchsamples$^$^     reversed because the worst wer with the best loss is to identify systemic issues where the acoustic model is confident$^     yet the result is completely off the mark. this can point to transcription errors and stuff like that.$^    samples.sortkeylambda s s.loss reversetrue$^$^     then order by ascending wercer$^    if flags.bytesoutputmode$^        samples.sortkeylambda s s.cer$^    else$^        samples.sortkeylambda s s.wer$^$^     print the report$^    printreportsamples losses sampleswer samplescer datasetname$^$^    return samples$^$^$^def printreportsamples losses wer cer datasetname$^     print a report summary and samples of best median and worst results $^$^     print summary$^    meanloss  np.meanlosses$^    printtest on s  wer f cer f loss f  datasetname wer cer meanloss$^    print  00$^$^    bestsamples  samplesflags.reportcount$^    worstsamples  samplesflags.reportcount$^    medianindex  intlensamples  0$^    medianleft  intflags.reportcount  0$^    medianright  flags.reportcount  medianleft$^    mediansamples  samplesmedianindex  medianleftmedianindex  medianright$^$^    def printsinglesamplesample$^        printwer f cer f loss f  sample.wer sample.cer sample.loss$^        print  wav files  sample.wavfilename$^        print  src s  sample.src$^        print  res s  sample.res$^        print  00$^$^    printbest wer n    00$^    for s in bestsamples$^        printsinglesamples$^$^    printmedian wer n    00$^    for s in mediansamples$^        printsinglesamples$^$^    printworst wer n    00$^    for s in worstsamples$^        printsinglesamples$^$^$^def savesamplesjsonsamples outputpath$^     save decoded tuples as json converting numpy floats to python floats.$^$^        we set ensureasciitrue to prevent json from escaping nonascii chars$^        in the texts.$^    $^    with openremoteoutputpath w as fout$^        json.dumpsamples fout defaultfloat ensureasciifalse indent0$^import os$^import sys$^import time$^import heapq$^import semver$^import random$^$^from multiprocessing import pool$^from collections import namedtuple$^$^kilo  0000$^kilobyte  0  kilo$^megabyte  kilo  kilobyte$^gigabyte  kilo  megabyte$^terabyte  kilo  gigabyte$^sizeprefixlookup  k kilobyte m megabyte g gigabyte t terabyte$^$^valuerange  namedtuplevaluerange start end r$^$^$^def parsefilesizefilesize$^    filesize  filesize.lower.strip$^    if lenfilesize  0$^        return 0$^    n  intkeeponlydigitsfilesize$^    if filesize0  b$^        filesize  filesize0$^    e  filesize0$^    return sizeprefixlookupe  n if e in sizeprefixlookup else n$^$^$^def keeponlydigitstxt$^    return .joinfilterstr.isdigit txt$^$^$^def secstohourssecs$^    hours remainder  divmodsecs 0000$^    minutes seconds  divmodremainder 00$^    return d00d00d  hours minutes seconds$^$^$^def checkctcdecoderversion$^    dsversions  openos.path.joinos.path.dirnamefile ..version.read.strip$^$^    try$^         pylint disableimportoutsidetoplevel$^        from dsctcdecoder import version as decoderversion$^    except importerror as e$^        if e.msg.findversion  0$^            printdeepspeech version dsversion requires ctc decoder to expose version. $^                  please upgrade the dsctcdecoder package to version dsversion.formatdsversiondsversions$^            sys.exit0$^        raise e$^$^    rv  semver.comparedsversions decoderversion$^    if rv  0$^        printdeepspeech version  and ctc decoder version  do not match. $^              please ensure matching versions are in use..formatdsversions decoderversion$^        sys.exit0$^$^    return rv$^$^$^class interleaved$^    collection that lazily combines sorted collections in an interleaving fashion.$^    during iteration the next smallest element from all the sorted collections is always picked.$^    the collections must support iter and len.$^    def initself iterables keylambda obj obj reversefalse$^        self.iterables  iterables$^        self.key  key$^        self.reverse  reverse$^        self.len  summaplen iterables$^$^    def iterself$^        return heapq.mergeself.iterables keyself.key reverseself.reverse$^$^    def lenself$^        return self.len$^$^$^class lenmap$^    $^    wrapper around python map output object that preserves the original collection length$^    by implementing len.$^    $^    def initself fn iterable$^        try$^            self.length  leniterable$^        except typeerror$^            self.length  none$^        self.mapobj  mapfn iterable$^$^    def iterself$^        self.mapobj  self.mapobj.iter$^        return self$^$^    def nextself$^        return self.mapobj.next$^$^    def getitemself key$^        return self.mapobj.getitemkey$^$^    def lenself$^        return self.length$^$^$^class limitingpool$^    limits unbound aheadprocessing of multiprocessing.pools imap method$^    before items get consumed by the iteration caller.$^    this prevents oom issues in situations where items represent larger memory allocations.$^    def initself processesnone initializernone initargsnone processaheadnone sleepingfor0.0$^        self.processahead  os.cpucount if processahead is none else processahead$^        self.sleepingfor  sleepingfor$^        self.processed  0$^        self.pool  poolprocessesprocesses initializerinitializer initargsinitargs$^$^    def enterself$^        return self$^$^    def limitself it$^        for obj in it$^            while self.processed  self.processahead$^                time.sleepself.sleepingfor$^            self.processed  0$^            yield obj$^$^    def imapself fun it$^        for obj in self.pool.imapfun self.limitit$^            self.processed  0$^            yield obj$^$^    def terminateself$^        self.pool.terminate$^$^    def exitself exctype excvalue traceback$^        self.pool.close$^$^$^class exceptionbox$^    helper class for passingback and reraising an exception from inside a tensorflow dataset generator.$^    used in conjunction with rememberexception.$^    def initself$^        self.exception  none$^$^    def raiseifsetself$^        if self.exception is not none$^            exception  self.exception$^            self.exception  none$^            raise exception   pylint disable  raisingbadtype$^$^$^def rememberexceptioniterable exceptionboxnone$^    wraps a tensorflow dataset generator for catching its actual exceptions$^    that would otherwise just interrupt iteration wo bubbling up.$^    def doiterate$^        try$^            yield from iterable$^        except stopiteration$^            return$^        except exception as ex   pylint disable  broadexcept$^            exceptionbox.exception  ex$^    return iterable if exceptionbox is none else doiterate$^$^$^def getvaluerangevalue targettype$^    if isinstancevalue str$^        r  targettype0$^        parts  value.split$^        if lenparts  0$^            value  parts0$^            r  targettypeparts0$^        elif lenparts  0$^            raise valueerrorcannot parse value range$^        parts  value.split$^        if lenparts  0$^            parts.appendparts0$^        elif lenparts  0$^            raise valueerrorcannot parse value range$^        return valuerangetargettypeparts0 targettypeparts0 r$^    if isinstancevalue tuple$^        if lenvalue  0$^            return valuerangetargettypevalue0 targettypevalue0 0$^        if lenvalue  0$^            return valuerangetargettypevalue0 targettypevalue0 targettypevalue0$^        raise valueerrorcannot convert to valuerange wrong tuple size$^    return valuerangetargettypevalue targettypevalue 0$^$^$^def intrangevalue$^    return getvaluerangevalue int$^$^$^def floatrangevalue$^    return getvaluerangevalue float$^$^$^def pickvaluefromrangevaluerange clocknone$^    clock  random.random if clock is none else max0.0 min0.0 floatclock$^    value  valuerange.start  clock  valuerange.end  valuerange.start$^    value  random.uniformvalue  valuerange.r value  valuerange.r$^    return roundvalue if isinstancevaluerange.start int else value$^$^$^def tfpickvaluefromrangevaluerange clocknone doubleprecisionfalse$^    import tensorflow as tf   pylint disableimportoutsidetoplevel$^    clock  tf.random.statelessuniform seed0 0 dtypetf.float00 if clock is none$^             else tf.maximumtf.constant0.0 dtypetf.float00 tf.minimumtf.constant0.0 dtypetf.float00 clock$^    value  valuerange.start  clock  valuerange.end  valuerange.start$^    value  tf.random.statelessuniform$^                                        minvalvalue  valuerange.r$^                                        maxvalvalue  valuerange.r$^                                        seedclock  tf.int00.min clock  tf.int00.max$^                                        dtypetf.float00$^    if isinstancevaluerange.start int$^        return tf.casttf.math.roundvalue tf.int00 if doubleprecision else tf.int00$^    return tf.castvalue tf.float00 if doubleprecision else tf.float00$^$^import os$^import re$^import math$^import random$^import resampy$^import numpy as np$^$^from multiprocessing import queue process$^from .audio import gaindbtoratio maxdbfs normalizeaudio audiotypenp audiotypepcm audiotypeopus$^from .helpers import limitingpool intrange floatrange pickvaluefromrange tfpickvaluefromrange megabyte$^from .samplecollections import samplesfromsource unpackmaybe$^$^buffersize  0  megabyte$^specparser  re.compilerpclsazpparams.$^$^$^class augmentation$^    def initself p0.0$^        self.probability  floatp$^$^$^class sampleaugmentationaugmentation$^    def startself bufferingbuffersize$^        pass$^$^    def applyself sample clock0.0$^        raise notimplementederror$^$^    def stopself$^        pass$^$^$^class graphaugmentationaugmentation$^    def initself p0.0 domainspectrogram$^        supergraphaugmentation self.initp$^        if domain not in signal spectrogram features$^            raise valueerrorunsupported augmentation domain .formatdomain$^        self.domain  domain$^$^    def applyself tensor transcriptnone clock0.0$^        raise notimplementederror$^$^    def applywithprobabilityself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        rv  tf.random.statelessuniform seedclock  tf.int00.min clock  tf.int00.max$^        return tf.condtf.lessrv self.probability$^                       lambda self.applytensor transcripttranscript clockclock$^                       lambda tensor$^$^    def maybeapplyself domain tensor transcriptnone clock0.0$^        if domain  self.domain$^            return self.applywithprobabilitytensor transcripttranscript clockclock$^        return tensor$^$^    def unitspermsself$^        from .flags import flags   pylint disableimportoutsidetoplevel$^        return flags.audiosamplerate  0000.0 if self.domain  signal else 0.0  flags.featurewinstep$^$^$^def parseaugmentationaugmentationspec$^    $^    parses an augmentation specification.$^$^    parameters$^    $^    augmentationspec  str$^        augmentation specification like reverbdelay00.0decay0.0.$^$^    returns$^    $^    instance of an augmentation class from util.augmentations..$^    $^    match  specparser.matchaugmentationspec$^    if not match$^        raise valueerroraugmentation specification has wrong format$^    clsname  .joinmaplambda p p0.upper  p0 match.groupcls.split$^    augmentationcls  globalsclsname if clsname in globals else none$^    if augmentationcls is none or not issubclassaugmentationcls augmentation or augmentationcls  augmentation$^        raise valueerrorunknown augmentation .formatclsname$^    parameters  match.groupparams$^    parameters   if parameters is none else parameters.split$^    args  $^    kwargs  $^    for parameter in parameters$^        pair  tuplelistmapstr.strip parameter.split$^        if lenpair  0$^            args.appendpair$^        elif lenpair  0$^            kwargspair0  pair0$^        else$^            raise valueerrorunable to parse augmentation value assignment$^    return augmentationclsargs kwargs$^$^$^def parseaugmentationsaugmentationspecs$^    $^    parses an augmentation specification.$^$^    parameters$^    $^    augmentationspecs  list of str$^        list of augmentation specifications like reverbdelay00.0decay0.0 volume.$^$^    returns$^    $^    list of augmentation class instances from util.augmentations..$^    $^    return  if augmentationspecs is none else listmapparseaugmentation augmentationspecs$^$^$^def applygraphaugmentationsdomain tensor augmentations transcriptnone clock0.0$^    $^    augments training sample tensor of a certain domain with matching augmentations of passed list.$^$^    parameters$^    $^    domain  str$^        domain of the tensor to apply augmentations to. one of signal spectrogram or features$^    tensor  tensor of type float00$^        tensor to apply augmentations to.$^    augmentations  list of augmentation class instances from util.augmentations..$^        list of augmentations of which only the spectrogram ones will get applied to the samples.$^    transcript  sparsetensor$^    clock  tensor of type float00$^        time indicator for augmentation valueranges. running from 0.0 start of training to 0.0 end of training.$^$^    returns$^    $^    tensor of type float00$^        the augmented spectrogram$^    $^    if augmentations$^        for augmentation in augmentations$^            if isinstanceaugmentation graphaugmentation$^                tensor  augmentation.maybeapplydomain tensor transcripttranscript clockclock$^    return tensor$^$^$^class augmentationcontext$^    def initself targetaudiotype augmentations$^        self.targetaudiotype  targetaudiotype$^        self.augmentations  augmentations$^$^$^augmentationcontext  none$^$^$^def initaugmentationworkerpreparationcontext$^    global augmentationcontext   pylint disableglobalstatement$^    augmentationcontext  preparationcontext$^$^$^def loadandaugmentsampletimedsample contextnone$^    sample clock  timedsample$^    realizedsample  unpackmaybesample$^    return augmentsamplerealizedsample clock context$^$^$^def augmentsampletimedsample contextnone$^    context  augmentationcontext if context is none else context$^    sample clock  timedsample$^    for augmentation in context.augmentations$^        if random.random  augmentation.probability$^            augmentation.applysample clock$^    sample.changeaudiotypenewaudiotypecontext.targetaudiotype$^    return sample$^$^$^def applysampleaugmentationssamples$^                               augmentations$^                               audiotypeaudiotypenp$^                               bufferingbuffersize$^                               processaheadnone$^                               clock0.0$^                               finalclocknone$^    $^    prepares samples for being used during training.$^    this includes parallel and buffered application of augmentations and a conversion to a specified audiotype.$^$^    parameters$^    $^    samples  sample enumeration$^        typically produced by util.samplecollections.samplesfromsources.$^    augmentations  list of augmentation class instances from util.augmentations..$^        list of augmentations of which only the signal ones will get applied to the samples.$^    audiotype  str$^        target audiotype to convert samples to. see util.audio.sample.init .$^    buffering  int$^        readbuffer size to use while reading files.$^    processahead  int$^        number of samples to preprocess ahead of time.$^    clock  float$^        start or fixed clock value between 0.0 and 0.0 for the first or all samples. has to be  than finalclock.$^    finalclock  float$^        final clock value between 0.0 and 0.0 for the last sample. has to be  than clock.$^        requires samples.len attribute.$^$^    returns$^    $^    iterable of util.samplecollections.labeledsample or util.audio.sample$^    $^    def timedsamples$^        if finalclock is none$^            for sample in samples$^                yield sample clock$^        else$^            for sampleindex sample in enumeratesamples$^                sampleclock  clock  finalclock  clock  sampleindex  lensamples$^                yield sample sampleclock$^$^    assert 0.0  clock  0.0$^    if finalclock is not none$^        assert 0.0  finalclock  0.0$^        assert clock  finalclock$^    augmentations  aug for aug in augmentations if isinstanceaug sampleaugmentation if augmentations else $^    try$^        for augmentation in augmentations$^            augmentation.startbufferingbuffering$^        context  augmentationcontextaudiotype augmentations$^        if processahead  0$^            for timedsample in timedsamples$^                yield loadandaugmentsampletimedsample contextcontext$^        else$^            with limitingpoolprocessaheadprocessahead$^                              initializerinitaugmentationworker$^                              initargscontext as pool$^                yield from pool.imaploadandaugmentsample timedsamples$^    finally$^        for augmentation in augmentations$^            augmentation.stop$^$^$^def enqueueoverlaysamplessamplesource queue bufferingbuffersize$^    $^    as the central distribution point for overlay samples this function is supposed to run in one process only.$^    this ensures that samples are not used twice if not required.$^    it loads the raw and still compressed data and provides it to the actual augmentation workers.$^    these are then doing decompression potential conversion and overlaying in parallel.$^    $^    samples  samplesfromsourcesamplesource bufferingbuffering labeledfalse$^    while true$^        for sample in samples$^            queue.putsample$^$^$^class overlaysampleaugmentation$^    see overlay augmentation in training documentation$^    def initself source p0.0 snr0.0 layers0$^        superoverlay self.initp$^        self.source  source$^        self.snr  floatrangesnr$^        self.layers  intrangelayers$^        self.currentsample  none$^        self.queue  none$^        self.enqueueprocess  none$^$^    def startself bufferingbuffersize$^        self.queue  queuemax0 math.floorself.probability  self.layers0  os.cpucount$^        self.enqueueprocess  processtargetenqueueoverlaysamples$^                                       argsself.source self.queue$^                                       kwargsbuffering buffering$^        self.enqueueprocess.start$^$^    def applyself sample clock0.0$^        sample  unpackmaybesample$^        sample.changeaudiotypenewaudiotypeaudiotypenp$^        nlayers  pickvaluefromrangeself.layers clockclock$^        audio  sample.audio$^        overlaydata  np.zeroslikeaudio$^        for  in rangenlayers$^            overlayoffset  0$^            while overlayoffset  lenaudio$^                if self.currentsample is none$^                    nextoverlaysample  self.queue.get$^                    nextoverlaysample  unpackmaybenextoverlaysample$^                    nextoverlaysample.changeaudiotypenewaudiotypeaudiotypenp$^                    self.currentsample  nextoverlaysample.audio$^                nrequired  lenaudio  overlayoffset$^                ncurrent  lenself.currentsample$^                if nrequired  ncurrent   take it completely$^                    overlaydataoverlayoffsetoverlayoffset  ncurrent  self.currentsample$^                    overlayoffset  ncurrent$^                    self.currentsample  none$^                else   take required slice from head and keep tail for next layer or sample$^                    overlaydataoverlayoffsetoverlayoffset  nrequired  self.currentsample0nrequired$^                    overlayoffset  nrequired$^                    self.currentsample  self.currentsamplenrequired$^        snrdb  pickvaluefromrangeself.snr clockclock$^        origdbfs  maxdbfsaudio$^        overlaygain  origdbfs  maxdbfsoverlaydata  snrdb$^        audio  overlaydata  gaindbtoratiooverlaygain$^        sample.audio  normalizeaudioaudio dbfsorigdbfs$^$^    def stopself$^        if self.enqueueprocess is not none$^            self.enqueueprocess.terminate$^            self.enqueueprocess  none$^        self.currentsample  none$^        self.queue  none$^$^$^class codecsampleaugmentation$^    see codec augmentation in training documentation$^    def initself p0.0 bitrate0000$^        supercodec self.initp$^        self.bitrate  intrangebitrate$^$^    def applyself sample clock0.0$^        bitrate  pickvaluefromrangeself.bitrate clockclock$^        sample.changeaudiotypenewaudiotypeaudiotypepcm   decoding to ensure it has to get encoded again$^        sample.changeaudiotypenewaudiotypeaudiotypeopus bitratebitrate   will get decoded again downstream$^$^$^class reverbsampleaugmentation$^    see reverb augmentation in training documentation$^    def initself p0.0 delay00.0 decay00.0$^        superreverb self.initp$^        self.delay  floatrangedelay$^        self.decay  floatrangedecay$^$^    def applyself sample clock0.0$^        sample.changeaudiotypenewaudiotypeaudiotypenp$^        audio  np.arraysample.audio dtypenp.float00$^        origdbfs  maxdbfsaudio$^        delay  pickvaluefromrangeself.delay clockclock$^        decay  pickvaluefromrangeself.decay clockclock$^        decay  gaindbtoratiodecay$^        result  np.copyaudio$^        primes  00 00 00 00 00$^        for delayprime in primes   primes to minimize comb filter interference$^            layer  np.copyaudio$^            ndelay  math.floordelay  delayprime  primes0  sample.audioformat.rate  0000.0$^            ndelay  max00 ndelay   00 samples minimum to avoid performance trap and risk of division by zero$^            for windex in range0 math.floorlenaudio  ndelay$^                w0  windex  ndelay$^                w0  windex  0  ndelay$^                width  minlenaudio  w0 ndelay   last window could be smaller$^                layerw0w0  width  decay  layerw0w0  width$^            result  layer$^        audio  normalizeaudioresult dbfsorigdbfs$^        sample.audio  np.arrayaudio dtypenp.float00$^$^$^class resamplesampleaugmentation$^    see resample augmentation in training documentation$^    def initself p0.0 rate0000$^        superresample self.initp$^        self.rate  intrangerate$^$^    def applyself sample clock0.0$^        sample.changeaudiotypenewaudiotypeaudiotypenp$^        rate  pickvaluefromrangeself.rate clockclock$^        origlen  lensample.audio$^        resampled  resampy.resamplesample.audio sample.audioformat.rate rate axis0 filterkaiserfast$^        sample.audio  resampy.resampleresampled rate sample.audioformat.rate axis0 filterkaiserfastoriglen$^$^$^class normalizesampleratesampleaugmentation$^    def initself rate$^        super.initp0.0$^        self.rate  rate$^$^    def applyself sample clock0.0$^        if sample.audioformat.rate  self.rate$^            return$^$^        sample.changeaudiotypenewaudiotypeaudiotypenp$^        sample.audio  resampy.resamplesample.audio sample.audioformat.rate self.rate axis0 filterkaiserfast$^        sample.audioformat  sample.audioformat.replacerateself.rate$^$^$^class volumesampleaugmentation$^    see volume augmentation in training documentation$^    def initself p0.0 dbfs0.0000$^        supervolume self.initp$^        self.targetdbfs  floatrangedbfs$^$^    def applyself sample clock0.0$^        sample.changeaudiotypenewaudiotypeaudiotypenp$^        targetdbfs  pickvaluefromrangeself.targetdbfs clockclock$^        sample.audio  normalizeaudiosample.audio dbfstargetdbfs$^$^$^class pitchgraphaugmentation$^    see pitch augmentation in training documentation$^    def initself p0.0 pitch0.000 0.000 0.000$^        superpitch self.initp domainspectrogram$^        self.pitch  floatrangepitch$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        originalshape  tf.shapetensor$^        pitch  tfpickvaluefromrangeself.pitch clockclock$^        newfreqsize  tf.casttf.castoriginalshape0 tf.float00  pitch tf.int00$^        spectrogramaug  tf.image.resizebilineartf.expanddimstensor 0 originalshape0 newfreqsize$^        spectrogramaug  tf.image.croptoboundingboxspectrogramaug$^                                                        offsetheight0$^                                                        offsetwidth0$^                                                        targetheightoriginalshape0$^                                                        targetwidthtf.math.minimumoriginalshape0 newfreqsize$^        spectrogramaug  tf.condpitch  0$^                                  lambda tf.image.padtoboundingboxspectrogramaug$^                                                                       offsetheight0$^                                                                       offsetwidth0$^                                                                       targetheighttf.shapespectrogramaug0$^                                                                       targetwidthoriginalshape0$^                                  lambda spectrogramaug$^        return spectrogramaug   0$^$^$^class tempographaugmentation$^    see tempo augmentation in training documentation$^    def initself p0.0 factor0.0 maxtime0$^        supertempo self.initp domainspectrogram$^        self.factor  floatrangefactor$^        self.maxtime  floatmaxtime$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        factor  tfpickvaluefromrangeself.factor clockclock$^        originalshape  tf.shapetensor$^        newtimesize  tf.casttf.castoriginalshape0 tf.float00  factor tf.int00$^        if transcript is not none$^            newtimesize  tf.math.maximumnewtimesize tf.shapetranscript0$^        if self.maxtime  0$^            newtimesize  tf.math.minimumnewtimesize tf.castself.maxtime  self.unitsperms tf.int00$^        spectrogramaug  tf.image.resizebilineartf.expanddimstensor 0 newtimesize originalshape0$^        return spectrogramaug   0$^$^$^class warpgraphaugmentation$^    see warp augmentation in training documentation$^    def initself p0.0 nt0 nf0 wt0.0 wf0.0$^        superwarp self.initp domainspectrogram$^        self.numt  intrangent$^        self.numf  intrangenf$^        self.warpt  floatrangewt$^        self.warpf  floatrangewf$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        originalshape  tf.shapetensor$^        sizet sizef  originalshape0 originalshape0$^        seed  clock  tf.int00.min clock  tf.int00.max$^        numt  tfpickvaluefromrangeself.numt clockclock$^        numf  tfpickvaluefromrangeself.numf clockclock$^$^        def getflowsn size warp$^            warp  tfpickvaluefromrangewarp clockclock$^            warp  warp  tf.castsize dtypetf.float00  tf.cast0  n  0 dtypetf.float00$^            f  tf.random.statelessnormalnumt numf seed mean0.0 stddevwarp dtypetf.float00$^            return tf.padf tf.constant0 0 0 0 constant   zero flow at all edges$^$^        flows  tf.stackgetflowsnumt sizet self.warpt getflowsnumf sizef self.warpf axis0$^        flows  tf.image.resizebicubictf.expanddimsflows 0 sizet sizef$^        spectrogramaug  tf.contrib.image.denseimagewarptf.expanddimstensor 0 flows$^        return tf.reshapespectrogramaug shape0 0 sizef$^$^$^class frequencymaskgraphaugmentation$^    see frequency mask augmentation in training documentation$^    def initself p0.0 n0 size0$^        superfrequencymask self.initp domainspectrogram$^        self.n  intrangen   pylint disableinvalidname$^        self.size  intrangesize$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        timemax  tf.shapetensor0$^        freqmax  tf.shapetensor0$^        n  tfpickvaluefromrangeself.n clockclock$^$^        def bodyi spectrogramaug$^            size  tfpickvaluefromrangeself.size clockclock$^            size  tf.math.maximum0 tf.math.minimumfreqmax  0 size$^            seed  tf.castclock  tf.int00.max tf.int00  i$^            f0  tf.random.statelessuniform seed seed minval0 maxvalfreqmax  size dtypetf.dtypes.int00$^            freqmask  tf.concattf.ones0 timemax f0$^                                   tf.zeros0 timemax size$^                                   tf.ones0 timemax freqmax  f0  size axis0$^            return i  0 spectrogramaug  freqmask$^$^        return tf.whilelooplambda i spectrogramaug i  n body 0 tensor0$^$^$^class timemaskgraphaugmentation$^    see time mask augmentation in training documentation$^    def initself p0.0 domainspectrogram n0 size00.0$^        supertimemask self.initp domaindomain$^        self.n  intrangen   pylint disableinvalidname$^        self.size  floatrangesize$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        timemax  tf.shapetensor0 if self.domain  signal else 0$^        n  tfpickvaluefromrangeself.n clockclock$^$^        def bodyi augmented$^            size  tf.casttfpickvaluefromrangeself.size clockclock  self.unitsperms dtypetf.int00$^            size  tf.math.maximum0 tf.math.minimumtimemax  0 size$^            seed  tf.castclock  tf.int00.max tf.int00  i$^            t0  tf.random.statelessuniform seed seed minval0 maxvaltimemax  size dtypetf.dtypes.int00$^            rest  timemax  t0  size$^            if self.domain  spectrogram$^                fm  tf.shapetensor0$^                timemask  tf.concattf.ones0 t0 fm tf.zeros0 size fm tf.ones0 rest fm axis0$^            elif self.domain  signal$^                timemask  tf.concattf.onest0 0 tf.zerossize 0 tf.onesrest 0 axis0$^            else$^                timemask  tf.concattf.ones0 t0 tf.zeros0 size tf.ones0 rest axis0$^            return i  0 augmented  timemask$^$^        return tf.whilelooplambda i augmented i  n body 0 tensor0$^$^$^class dropoutgraphaugmentation$^    see dropout augmentation in training documentation$^    def initself p0.0 domainspectrogram rate0.00$^        superdropout self.initp domaindomain$^        self.rate  floatrangerate$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        rate  tfpickvaluefromrangeself.rate clockclock$^        rate  tf.math.maximum0.0 rate$^        factors  tf.random.statelessuniformtf.shapetensor$^                                              clock  tf.int00.min clock  tf.int00.max$^                                              minval0.0$^                                              maxval0.0$^                                              dtypetf.float00$^        return tensor  tf.math.signtf.math.floorfactors  rate$^$^$^class addgraphaugmentation$^    see add augmentation in training documentation$^    def initself p0.0 domainfeatures stddev0$^        superadd self.initp domaindomain$^        self.stddev  floatrangestddev$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        stddev  tfpickvaluefromrangeself.stddev clockclock$^        seed  clock  tf.int00.min clock  tf.int00.max$^        return tensor  tf.random.statelessnormaltf.shapetensor seed mean0.0 stddevstddev$^$^$^class multiplygraphaugmentation$^    see multiply augmentation in training documentation$^    def initself p0.0 domainfeatures stddev0$^        supermultiply self.initp domaindomain$^        self.stddev  floatrangestddev$^$^    def applyself tensor transcriptnone clock0.0$^        import tensorflow as tf   pylint disableimportoutsidetoplevel$^        stddev  tfpickvaluefromrangeself.stddev clockclock$^        seed  clock  tf.int00.min clock  tf.int00.max$^        return tensor  tf.random.statelessnormaltf.shapetensor seed mean0.0 stddevstddev$^usrbinenv python$^  coding utf0 $^from future import absoluteimport division printfunction$^$^import os$^import sys$^$^loglevelindex  sys.argv.indexloglevel  0 if loglevel in sys.argv else 0$^desiredloglevel  sys.argvloglevelindex if 0  loglevelindex  lensys.argv else 0$^os.environtfcppminloglevel  desiredloglevel$^$^import absl.app$^import numpy as np$^import progressbar$^import shutil$^import tensorflow as tf$^import tensorflow.compat.v0 as tfv0$^import time$^$^tfv0.logging.setverbosity$^    0 tfv0.logging.debug$^    0 tfv0.logging.info$^    0 tfv0.logging.warn$^    0 tfv0.logging.error$^.getdesiredloglevel$^$^from datetime import datetime$^from dsctcdecoder import ctcbeamsearchdecoder scorer$^from .evaluate import evaluate$^from six.moves import zip range$^from .util.config import config initializeglobals$^from .util.checkpoints import loadorinitgraphfortraining loadgraphforevaluation reloadbestcheckpoint$^from .util.evaluatetools import savesamplesjson$^from .util.feeding import createdataset audiotofeatures audiofiletofeatures$^from .util.flags import createflags flags$^from .util.helpers import checkctcdecoderversion exceptionbox$^from .util.logging import createprogressbar logdebug logerror loginfo logprogress logwarn$^from .util.io import openremote removeremote listdirremote isremotepath isdirremote$^$^checkctcdecoderversion$^$^ graph creation$^ $^$^def variableoncpuname shape initializer$^    r$^    next we concern ourselves with graph creation.$^    however before we do so we must introduce a utility function variableoncpu$^    used to create a variable in cpu memory.$^    $^     use the cpu0 device for scoped operations$^    with tf.deviceconfig.cpudevice$^         create or get apropos variable$^        var  tfv0.getvariablenamename shapeshape initializerinitializer$^    return var$^$^$^def createoverlappingwindowsbatchx$^    batchsize  tf.shapeinputbatchx0$^    windowwidth  0  config.ncontext  0$^    numchannels  config.ninput$^$^     create a constant convolution filter using an identity matrix so that the$^     convolution returns patches of the input tensor as is and we can create$^     overlapping windows over the mfccs.$^    eyefilter  tf.constantnp.eyewindowwidth  numchannels$^                               .reshapewindowwidth numchannels windowwidth  numchannels tf.float00  pylint disablebadcontinuation$^$^     create overlapping windows$^    batchx  tf.nn.conv0dinputbatchx filterseyefilter stride0 paddingsame$^$^     remove dummy depth dimension and reshape into batchsize nwindows windowwidth ninput$^    batchx  tf.reshapebatchx batchsize 0 windowwidth numchannels$^$^    return batchx$^$^$^def densename x units dropoutratenone relutrue layernormfalse$^    with tfv0.variablescopename$^        bias  variableoncpubias units tfv0.zerosinitializer$^        weights  variableoncpuweights x.shape0 units tfv0.keras.initializers.variancescalingscale0.0 modefanavg distributionuniform$^$^    output  tf.nn.biasaddtf.matmulx weights bias$^$^    if relu$^        output  tf.minimumtf.nn.reluoutput flags.reluclip$^$^    if layernorm$^        with tfv0.variablescopename$^            output  tf.contrib.layers.layernormoutput$^$^    if dropoutrate is not none$^        output  tf.nn.dropoutoutput ratedropoutrate$^$^    return output$^$^$^def rnnimpllstmblockfusedcellx seqlength previousstate reuse$^    with tfv0.variablescopecudnnlstmrnnmultirnncellcell0$^        fwcell  tf.contrib.rnn.lstmblockfusedcellconfig.ncelldim$^                                                    forgetbias0$^                                                    reusereuse$^                                                    namecudnncompatiblelstmcell$^$^        output outputstate  fwcellinputsx$^                                       dtypetf.float00$^                                       sequencelengthseqlength$^                                       initialstatepreviousstate$^$^    return output outputstate$^$^$^def rnnimplcudnnrnnx seqlength previousstate $^    assert previousstate is none  passing previous state not supported with cudnn backend$^$^     hack cudnnlstm works similarly to keras layers in that when you instantiate$^     the object it creates the variables and then you just call it several times$^     to enable variable reuse. because all of our code is structure in an old$^     school tensorflow structure where you can just call tf.getvariable again with$^     reusetrue to reuse variables we cant easily make use of the object oriented$^     way cudnnlstm is implemented so we save a singleton instance in the function$^     emulating a static function variable.$^    if not rnnimplcudnnrnn.cell$^         forward direction cell$^        fwcell  tf.contrib.cudnnrnn.cudnnlstmnumlayers0$^                                                 numunitsconfig.ncelldim$^                                                 inputmodelinearinput$^                                                 directionunidirectional$^                                                 dtypetf.float00$^        rnnimplcudnnrnn.cell  fwcell$^$^    output outputstate  rnnimplcudnnrnn.cellinputsx$^                                                   sequencelengthsseqlength$^$^    return output outputstate$^$^rnnimplcudnnrnn.cell  none$^$^$^def rnnimplstaticrnnx seqlength previousstate reuse$^    with tfv0.variablescopecudnnlstmrnnmultirnncell$^         forward direction cell$^        fwcell  tfv0.nn.rnncell.lstmcellconfig.ncelldim$^                                            forgetbias0$^                                            reusereuse$^                                            namecudnncompatiblelstmcell$^$^         split rank n tensor into list of rank n0 tensors$^        x  xl for l in rangex.shape0$^$^        output outputstate  tfv0.nn.staticrnncellfwcell$^                                                  inputsx$^                                                  sequencelengthseqlength$^                                                  initialstatepreviousstate$^                                                  dtypetf.float00$^                                                  scopecell0$^$^        output  tf.concatoutput 0$^$^    return output outputstate$^$^$^def createmodelbatchx seqlength dropout reusefalse batchsizenone previousstatenone overlaptrue rnnimplrnnimpllstmblockfusedcell$^    layers  $^$^     input shape batchsize nsteps ninput  0ninputncontext$^    if not batchsize$^        batchsize  tf.shapeinputbatchx0$^$^     create overlapping feature windows if needed$^    if overlap$^        batchx  createoverlappingwindowsbatchx$^$^     reshaping batchx to a tensor with shape nstepsbatchsize ninput  0ninputncontext.$^     this is done to prepare the batch for input into the first layer which expects a tensor of rank 0.$^$^     permute nsteps and batchsize$^    batchx  tf.transposeabatchx perm0 0 0 0$^     reshape to prepare input for first layer$^    batchx  tf.reshapebatchx 0 config.ninput  0config.ninputconfig.ncontext  nstepsbatchsize ninput  0ninputncontext$^    layersinputreshaped  batchx$^$^     the next three blocks will pass batchx through three hidden layers with$^     clipped relu activation and dropout.$^    layerslayer0  layer0  denselayer0 batchx config.nhidden0 dropoutratedropout0 layernormflags.layernorm$^    layerslayer0  layer0  denselayer0 layer0 config.nhidden0 dropoutratedropout0 layernormflags.layernorm$^    layerslayer0  layer0  denselayer0 layer0 config.nhidden0 dropoutratedropout0 layernormflags.layernorm$^$^     layer0 is now reshaped into nsteps batchsize 0ncelldim$^     as the lstm rnn expects its input to be of shape maxtime batchsize inputsize.$^    layer0  tf.reshapelayer0 0 batchsize config.nhidden0$^$^     run through parametrized rnn implementation as we use different rnns$^     for training and inference$^    output outputstate  rnnimpllayer0 seqlength previousstate reuse$^$^     reshape output from a tensor of shape nsteps batchsize ncelldim$^     to a tensor of shape nstepsbatchsize ncelldim$^    output  tf.reshapeoutput 0 config.ncelldim$^    layersrnnoutput  output$^    layersrnnoutputstate  outputstate$^$^     now we feed output to the fifth hidden layer with clipped relu activation$^    layerslayer0  layer0  denselayer0 output config.nhidden0 dropoutratedropout0 layernormflags.layernorm$^$^     now we apply a final linear layer creating nclasses dimensional vectors the logits.$^    layerslayer0  layer0  denselayer0 layer0 config.nhidden0 relufalse$^$^     finally we reshape layer0 from a tensor of shape nstepsbatchsize nhidden0$^     to the slightly more useful shape nsteps batchsize nhidden0.$^     note that this differs from the input in that it is timemajor.$^    layer0  tf.reshapelayer0 0 batchsize config.nhidden0 namerawlogits$^    layersrawlogits  layer0$^$^     output shape nsteps batchsize nhidden0$^    return layer0 layers$^$^$^ accuracy and loss$^ $^$^ in accord with deep speech scaling up endtoend speech recognition$^ httparxiv.orgabs0000.0000$^ the loss function used by our network should be the ctc loss function$^ httpwww.cs.toronto.edugravespreprint.pdf.$^ conveniently this loss function is implemented in tensorflow.$^ thus we can simply make use of this implementation to define our loss.$^$^def calculatemeaneditdistanceandlossiterator dropout reuse$^    r$^    this routine beam search decodes a minibatch and calculates the loss and mean edit distance.$^    next to total and average loss it returns the mean edit distance$^    the decoded result and the batchs original y.$^    $^     obtain the next batch of data$^    batchfilenames batchx batchseqlen batchy  iterator.getnext$^$^    if flags.traincudnn$^        rnnimpl  rnnimplcudnnrnn$^    else$^        rnnimpl  rnnimpllstmblockfusedcell$^$^     calculate the logits of the batch$^    logits   createmodelbatchx batchseqlen dropout reusereuse rnnimplrnnimpl$^$^     compute the ctc loss using tensorflows ctcloss$^    totalloss  tfv0.nn.ctclosslabelsbatchy inputslogits sequencelengthbatchseqlen$^$^     check if any files lead to non finite loss$^    nonfinitefiles  tf.gatherbatchfilenames tfv0.wheretf.math.isfinitetotalloss$^$^     calculate the average loss across the batch$^    avgloss  tf.reducemeaninputtensortotalloss$^$^     finally we return the average loss$^    return avgloss nonfinitefiles$^$^$^ adam optimization$^ $^$^ in contrast to deep speech scaling up endtoend speech recognition$^ httparxiv.orgabs0000.0000$^ in which nesterovs accelerated gradient descent$^ www.cs.toronto.edufritzabspsmomentum.pdf was used$^ we will use the adam method for optimization httparxiv.orgabs0000.0000$^ because generally it requires less finetuning.$^def createoptimizerlearningratevar$^    optimizer  tfv0.train.adamoptimizerlearningratelearningratevar$^                                         beta0flags.beta0$^                                         beta0flags.beta0$^                                         epsilonflags.epsilon$^    return optimizer$^$^$^ towers$^ $^$^ in order to properly make use of multiple gpus one must introduce new abstractions$^ not present when using a single gpu that facilitate the multigpu use case.$^ in particular one must introduce a means to isolate the inference and gradient$^ calculations on the various gpus.$^ the abstraction we intoduce for this purpose is called a tower.$^ a tower is specified by two properties$^  scope  a scope as provided by tf.namescope$^ is a means to isolate the operations within a tower.$^ for example all operations within tower 0 could have their name prefixed with tower0.$^  device  a hardware device as provided by tf.device$^ on which all operations within the tower execute.$^ for example all operations of tower 0 could execute on the first gpu tf.devicegpu0.$^$^def gettowerresultsiterator optimizer dropoutrates$^    r$^    with this preliminary step out of the way we can for each gpu introduce a$^    tower for whichs batch we calculate and return the optimization gradients$^    and the average loss across towers.$^    $^     to calculate the mean of the losses$^    toweravglosses  $^$^     tower gradients to return$^    towergradients  $^$^     aggregate any non finite files in the batches$^    towernonfinitefiles  $^$^    with tfv0.variablescopetfv0.getvariablescope$^         loop over availabledevices$^        for i in rangelenconfig.availabledevices$^             execute operations of tower i on device i$^            device  config.availabledevicesi$^            with tf.devicedevice$^                 create a scope for all operations of tower i$^                with tf.namescopetowerd  i$^                     calculate the avgloss and meaneditdistance and retrieve the decoded$^                     batch along with the original batchs labels y of this tower$^                    avgloss nonfinitefiles  calculatemeaneditdistanceandlossiterator dropoutrates reusei  0$^$^                     allow for variables to be reused by the next tower$^                    tfv0.getvariablescope.reusevariables$^$^                     retain towers avg losses$^                    toweravglosses.appendavgloss$^$^                     compute gradients for model parameters using towers minibatch$^                    gradients  optimizer.computegradientsavgloss$^$^                     retain towers gradients$^                    towergradients.appendgradients$^$^                    towernonfinitefiles.appendnonfinitefiles$^$^    avglossacrosstowers  tf.reducemeaninputtensortoweravglosses axis0$^    tfv0.summary.scalarnamesteploss tensoravglossacrosstowers collectionsstepsummaries$^$^    allnonfinitefiles  tf.concattowernonfinitefiles axis0$^$^     return gradients and the average loss$^    return towergradients avglossacrosstowers allnonfinitefiles$^$^$^def averagegradientstowergradients$^    r$^    a routine for computing each variables average of the gradients obtained from the gpus.$^    note also that this code acts as a synchronization point as it requires all$^    gpus to be finished with their minibatch before it can run to completion.$^    $^     list of average gradients to return to the caller$^    averagegrads  $^$^     run this on cpudevice to conserve gpu memory$^    with tf.deviceconfig.cpudevice$^         loop over gradientvariable pairs from all towers$^        for gradandvars in ziptowergradients$^             introduce grads to store the gradients for the current variable$^            grads  $^$^             loop over the gradients for the current variable$^            for g  in gradandvars$^                 add 0 dimension to the gradients to represent the tower.$^                expandedg  tf.expanddimsg 0$^                 append on a tower dimension which we will average over below.$^                grads.appendexpandedg$^$^             average over the tower dimension$^            grad  tf.concatgrads 0$^            grad  tf.reducemeaninputtensorgrad axis0$^$^             create a gradientvariable tuple for the current variable with its average gradient$^            gradandvar  grad gradandvars00$^$^             add the current tuple to averagegrads$^            averagegrads.appendgradandvar$^$^     return result to caller$^    return averagegrads$^$^$^$^ logging$^ $^$^def logvariablevariable gradientnone$^    r$^    we introduce a function for logging a tensor variables current state.$^    it logs scalar values for the mean standard deviation minimum and maximum.$^    furthermore it logs a histogram of its state and if given of an optimization gradient.$^    $^    name  variable.name.replace $^    mean  tf.reducemeaninputtensorvariable$^    tfv0.summary.scalarnamesmean    name tensormean$^    tfv0.summary.scalarnamessttdev  name tensortf.sqrttf.reducemeaninputtensortf.squarevariable  mean$^    tfv0.summary.scalarnamesmax     name tensortf.reducemaxinputtensorvariable$^    tfv0.summary.scalarnamesmin     name tensortf.reducemininputtensorvariable$^    tfv0.summary.histogramnamename valuesvariable$^    if gradient is not none$^        if isinstancegradient tf.indexedslices$^            gradvalues  gradient.values$^        else$^            gradvalues  gradient$^        if gradvalues is not none$^            tfv0.summary.histogramnamesgradients  name valuesgradvalues$^$^$^def loggradsandvarsgradsandvars$^    r$^    lets also introduce a helper function for logging collections of gradientvariable tuples.$^    $^    for gradient variable in gradsandvars$^        logvariablevariable gradientgradient$^$^$^def train$^    exceptionbox  exceptionbox$^$^    if flags.horovod$^        import horovod.tensorflow as hvd$^$^     create training and validation datasets$^    splitdataset  flags.horovod$^$^    trainset  createdatasetflags.trainfiles.split$^                               batchsizeflags.trainbatchsize$^                               epochsflags.epochs$^                               augmentationsconfig.augmentations$^                               cachepathflags.featurecache$^                               trainphasetrue$^                               exceptionboxexceptionbox$^                               processaheadconfig.numdevices  flags.trainbatchsize  0$^                               reverseflags.reversetrain$^                               limitflags.limittrain$^                               bufferingflags.readbuffer$^                               splitdatasetsplitdataset$^$^    iterator  tfv0.data.iterator.fromstructuretfv0.data.getoutputtypestrainset$^                                                 tfv0.data.getoutputshapestrainset$^                                                 outputclassestfv0.data.getoutputclassestrainset$^$^     make initialization ops for switching between the two sets$^    traininitop  iterator.makeinitializertrainset$^$^    if flags.devfiles$^        devsources  flags.devfiles.split$^        devsets  createdatasetsource$^                                   batchsizeflags.devbatchsize$^                                   trainphasefalse$^                                   exceptionboxexceptionbox$^                                   processaheadconfig.numdevices  flags.devbatchsize  0$^                                   reverseflags.reversedev$^                                   limitflags.limitdev$^                                   bufferingflags.readbuffer$^                                   splitdatasetsplitdataset for source in devsources$^        devinitops  iterator.makeinitializerdevset for devset in devsets$^$^    if flags.metricsfiles$^        metricssources  flags.metricsfiles.split$^        metricssets  createdatasetsource$^                                       batchsizeflags.devbatchsize$^                                       trainphasefalse$^                                       exceptionboxexceptionbox$^                                       processaheadconfig.numdevices  flags.devbatchsize  0$^                                       reverseflags.reversedev$^                                       limitflags.limitdev$^                                       bufferingflags.readbuffer$^                                       splitdatasetsplitdataset for source in metricssources$^        metricsinitops  iterator.makeinitializermetricsset for metricsset in metricssets$^$^     dropout$^    dropoutrates  tfv0.placeholdertf.float00 namedropout.formati for i in range0$^    dropoutfeeddict  $^        dropoutrates0 flags.dropoutrate$^        dropoutrates0 flags.dropoutrate0$^        dropoutrates0 flags.dropoutrate0$^        dropoutrates0 flags.dropoutrate0$^        dropoutrates0 flags.dropoutrate0$^        dropoutrates0 flags.dropoutrate0$^    $^    nodropoutfeeddict  $^        rate 0. for rate in dropoutrates$^    $^$^     building the graph$^    learningratevar  tfv0.getvariablelearningrate initializerflags.learningrate trainablefalse$^    reducelearningrateop  learningratevar.assigntf.multiplylearningratevar flags.plateaureduction$^    if flags.horovod$^         effective batch size in synchronous distributed training is scaled by the number of workers. an increase in learning rate compensates for the increased batch size.$^        optimizer  createoptimizerlearningratevar  hvd.size$^        optimizer  hvd.distributedoptimizeroptimizer$^    else$^        optimizer  createoptimizerlearningratevar$^$^     enable mixed precision training$^    if flags.automaticmixedprecision$^        loginfoenabling automatic mixed precision training.$^        optimizer  tfv0.train.experimental.enablemixedprecisiongraphrewriteoptimizer$^$^    if flags.horovod$^        loss nonfinitefiles  calculatemeaneditdistanceandlossiterator dropoutrates reusefalse$^        gradients  optimizer.computegradientsloss$^$^        tfv0.summary.scalarnamesteploss tensorloss collectionsstepsummaries$^        loggradsandvarsgradients$^$^         globalstep is automagically incremented by the optimizer$^        globalstep  tfv0.train.getorcreateglobalstep$^        applygradientop  optimizer.applygradientsgradients globalstepglobalstep$^    else$^        gradients loss nonfinitefiles  gettowerresultsiterator optimizer dropoutrates$^$^         average tower gradients across gpus$^        avgtowergradients  averagegradientsgradients$^        loggradsandvarsavgtowergradients$^$^         globalstep is automagically incremented by the optimizer$^        globalstep  tfv0.train.getorcreateglobalstep$^        applygradientop  optimizer.applygradientsavgtowergradients globalstepglobalstep$^$^     summaries$^    stepsummariesop  tfv0.summary.mergeallstepsummaries$^    stepsummarywriters  $^        train tfv0.summary.filewriteros.path.joinflags.summarydir train maxqueue000$^        dev tfv0.summary.filewriteros.path.joinflags.summarydir dev maxqueue000$^        metrics tfv0.summary.filewriteros.path.joinflags.summarydir metrics maxqueue000$^    $^$^    humanreadablesetnames  $^        train training$^        dev validation$^        metrics metrics$^    $^$^     checkpointing$^    if config.ismasterprocess$^        checkpointsaver  tfv0.train.savermaxtokeepflags.maxtokeep$^        checkpointpath  os.path.joinflags.savecheckpointdir train$^$^        bestdevsaver  tfv0.train.savermaxtokeep0$^        bestdevpath  os.path.joinflags.savecheckpointdir bestdev$^$^         save flags next to checkpoints$^        if not isremotepathflags.savecheckpointdir$^            os.makedirsflags.savecheckpointdir existoktrue$^        flagsfile  os.path.joinflags.savecheckpointdir flags.txt$^        with openremoteflagsfile w as fout$^            fout.writeflags.flagsintostring$^$^    if flags.horovod$^        bcast  hvd.broadcastglobalvariables0$^$^    with tfv0.sessionconfigconfig.sessionconfig as session$^        logdebugsession opened.$^$^         prevent further graph changes$^        tfv0.getdefaultgraph.finalize$^$^         load checkpoint or initialize variables$^        loadorinitgraphfortrainingsession$^        if flags.horovod$^            bcast.run$^$^        def runsetsetname epoch initop datasetnone$^            istrain  setname  train$^            trainop  applygradientop if istrain else $^            feeddict  dropoutfeeddict if istrain else nodropoutfeeddict$^$^            totalloss  0.0$^            stepcount  0$^$^            stepsummarywriter  stepsummarywriters.getsetname$^            checkpointtime  time.time$^$^            if istrain and flags.cacheforepochs  0 and flags.featurecache$^                featurecacheindex  flags.featurecache  .index$^                if epoch  flags.cacheforepochs  0 and os.path.isfilefeaturecacheindex$^                    loginfoinvalidating feature cache$^                    removeremotefeaturecacheindex   this will let tf also overwrite the related cache data files$^$^             setup progress bar$^            class losswidgetprogressbar.widgets.formatlabel$^                def initself$^                    progressbar.widgets.formatlabel.initself formatloss meanlossf$^$^                def callself progress data kwargs$^                    datameanloss  totalloss  stepcount if stepcount else 0.0$^                    return progressbar.widgets.formatlabel.callself progress data kwargs$^$^            if config.ismasterprocess$^                prefix  epoch   00.formatepoch humanreadablesetnamessetname$^                widgets     progressbar.widgets.timer$^                             steps  progressbar.widgets.counter$^                              losswidget$^                suffix    dataset .formatdataset if dataset else none$^                pbar  createprogressbarprefixprefix widgetswidgets suffixsuffix.start$^$^             initialize iterator to the appropriate dataset$^            session.runinitop$^$^             batch loop$^            while true$^                try$^                     currentstep batchloss problemfiles stepsummary  $^                        session.runtrainop globalstep loss nonfinitefiles stepsummariesop$^                                    feeddictfeeddict$^                    exceptionbox.raiseifset$^                except tf.errors.outofrangeerror$^                    exceptionbox.raiseifset$^                    break$^$^                if problemfiles.size  0$^                    problemfiles  f.decodeutf0 for f in problemfiles... 0$^                    logerrorthe following files caused an infinite or nan $^                              loss .format.joinproblemfiles$^$^                totalloss  batchloss$^                stepcount  0$^$^                if config.ismasterprocess$^                    pbar.updatestepcount$^$^                    stepsummarywriter.addsummarystepsummary currentstep$^$^                    if istrain and flags.checkpointsecs  0 and time.time  checkpointtime  flags.checkpointsecs$^                        checkpointsaver.savesession checkpointpath globalstepcurrentstep$^                        checkpointtime  time.time$^$^            if config.ismasterprocess$^                pbar.finish$^            meanloss  totalloss  stepcount if stepcount  0 else 0.0$^            return meanloss stepcount$^$^        loginfostarting optimization$^        trainstarttime  datetime.utcnow$^        bestdevloss  floatinf$^        devlosses  $^        epochswithoutimprovement  0$^        try$^            for epoch in rangeflags.epochs$^                 training$^                if config.ismasterprocess$^                    logprogresstraining epoch d...  epoch$^                trainloss   runsettrain epoch traininitop$^                if config.ismasterprocess$^                    logprogressfinished training epoch d  loss f  epoch trainloss$^                    checkpointsaver.savesession checkpointpath globalstepglobalstep$^$^                if flags.devfiles$^                     validation$^                    devloss  0.0$^                    totalsteps  0$^                    for source initop in zipdevsources devinitops$^                        if config.ismasterprocess$^                            logprogressvalidating epoch d on s...  epoch source$^                        setloss steps  runsetdev epoch initop datasetsource$^                        devloss  setloss  steps$^                        totalsteps  steps$^                        if config.ismasterprocess$^                            logprogressfinished validating epoch d on s  loss f  epoch source setloss$^$^                    devloss  devloss  totalsteps$^                    devlosses.appenddevloss$^$^                     count epochs without an improvement for early stopping and reduction of learning rate on a plateau$^                     the improvement has to be greater than flags.esmindelta$^                    if devloss  bestdevloss  flags.esmindelta$^                        epochswithoutimprovement  0$^                    else$^                        epochswithoutimprovement  0$^$^                    if config.ismasterprocess$^                         save new best model$^                        if devloss  bestdevloss$^                            bestdevloss  devloss$^                            savepath  bestdevsaver.savesession bestdevpath globalstepglobalstep$^                                                            latestfilenamebestdevcheckpoint$^                            loginfosaved new best validating model with loss f to s  bestdevloss savepath$^$^                     early stopping$^                    if flags.earlystop and epochswithoutimprovement  flags.esepochs$^                        if config.ismasterprocess$^                            loginfoearly stop triggered as the loss did not improve the last  epochs.format$^                                epochswithoutimprovement$^                        break$^$^                     reduce learning rate on plateau$^                     if the learning rate was reduced and there is still no improvement$^                     wait flags.plateauepochs before the learning rate is reduced again$^                    if $^                        flags.reducelronplateau$^                        and epochswithoutimprovement  0$^                        and epochswithoutimprovement  flags.plateauepochs  0$^                    $^                         reload checkpoint that we use the bestdev weights again$^                        reloadbestcheckpointsession$^$^                         reduce learning rate$^                        session.runreducelearningrateop$^                        currentlearningrate  learningratevar.eval$^                        if config.ismasterprocess$^                            loginfoencountered a plateau reducing learning rate to .format$^                                currentlearningrate$^$^                             overwrite best checkpoint with new learning rate value$^                            savepath  bestdevsaver.savesession bestdevpath globalstepglobalstep$^                                                            latestfilenamebestdevcheckpoint$^                            loginfosaved best validating model with reduced learning rate to s  savepath$^$^                if flags.metricsfiles$^                     read only metrics not affecting best validation loss tracking$^                    for source initop in zipmetricssources metricsinitops$^                        if config.ismasterprocess$^                            logprogressmetrics for epoch d on s...  epoch source$^                        setloss   runsetmetrics epoch initop datasetsource$^                        if config.ismasterprocess$^                            logprogressmetrics for epoch d on s  loss f  epoch source setloss$^$^                print  00$^$^$^        except keyboardinterrupt$^            pass$^        if config.ismasterprocess$^            loginfofinished optimization in .formatdatetime.utcnow  trainstarttime$^    logdebugsession closed.$^$^$^def test$^    samples  evaluateflags.testfiles.split createmodel$^    if flags.testoutputfile$^        savesamplesjsonsamples flags.testoutputfile$^$^$^def createinferencegraphbatchsize0 nsteps00 tflitefalse$^    batchsize  batchsize if batchsize  0 else none$^$^     create feature computation graph$^    inputsamples  tfv0.placeholdertf.float00 config.audiowindowsamples inputsamples$^    samples  tf.expanddimsinputsamples 0$^    mfccs   audiotofeaturessamples flags.audiosamplerate$^    mfccs  tf.identitymfccs namemfccs$^$^     input tensor will be of shape batchsize nsteps 0ncontext0 ninput$^     this shape is read by the nativeclient in dscreatemodel to know the$^     value of nsteps ncontext and ninput. make sure you update the code$^     there if this shape is changed.$^    inputtensor  tfv0.placeholdertf.float00 batchsize nsteps if nsteps  0 else none 0  config.ncontext  0 config.ninput nameinputnode$^    seqlength  tfv0.placeholdertf.int00 batchsize nameinputlengths$^$^    if batchsize  0$^         no state management since nstep is expected to be dynamic too see below$^        previousstate  none$^    else$^        previousstatec  tfv0.placeholdertf.float00 batchsize config.ncelldim namepreviousstatec$^        previousstateh  tfv0.placeholdertf.float00 batchsize config.ncelldim namepreviousstateh$^$^        previousstate  tf.nn.rnncell.lstmstatetuplepreviousstatec previousstateh$^$^     one rate per layer$^    nodropout  none  0$^$^    if tflite$^        rnnimpl  rnnimplstaticrnn$^    else$^        rnnimpl  rnnimpllstmblockfusedcell$^$^    logits layers  createmodelbatchxinputtensor$^                                  batchsizebatchsize$^                                  seqlengthseqlength if not flags.exporttflite else none$^                                  dropoutnodropout$^                                  previousstatepreviousstate$^                                  overlapfalse$^                                  rnnimplrnnimpl$^$^     tf lite runtime will check that input dimensions are 0 0 or 0$^     by default we get 0 the middle one being batchsize which is forced to$^     one on inference graph so remove that dimension$^    if tflite$^        logits  tf.squeezelogits 0$^$^     apply softmax for ctc decoder$^    probs  tf.nn.softmaxlogits namelogits$^$^    if batchsize  0$^        if tflite$^            raise notimplementederrordynamic batchsize does not support tflite nor streaming$^        if nsteps  0$^            raise notimplementederrordynamic batchsize expect nsteps to be dynamic too$^        return $^            $^                input inputtensor$^                inputlengths seqlength$^            $^            $^                outputs probs$^            $^            layers$^        $^$^    newstatec newstateh  layersrnnoutputstate$^    newstatec  tf.identitynewstatec namenewstatec$^    newstateh  tf.identitynewstateh namenewstateh$^$^    inputs  $^        input inputtensor$^        previousstatec previousstatec$^        previousstateh previousstateh$^        inputsamples inputsamples$^    $^$^    if not flags.exporttflite$^        inputsinputlengths  seqlength$^$^    outputs  $^        outputs probs$^        newstatec newstatec$^        newstateh newstateh$^        mfccs mfccs$^$^         expose internal layers for downstream applications$^        layer0 layerslayer0$^        layer0 layerslayer0$^    $^$^    return inputs outputs layers$^$^$^def filerelativereadfname$^    return openos.path.joinos.path.dirnamefile fname.read$^$^$^def export$^    r$^    restores the trained variables into a simpler graph that will be exported for serving.$^    $^    loginfoexporting the model...$^$^    inputs outputs   createinferencegraphbatchsizeflags.exportbatchsize nstepsflags.nsteps tfliteflags.exporttflite$^$^    graphversion  intfilerelativereadgraphversion.strip$^    assert graphversion  0$^$^    outputsmetadataversion  tf.constantgraphversion namemetadataversion$^    outputsmetadatasamplerate  tf.constantflags.audiosamplerate namemetadatasamplerate$^    outputsmetadatafeaturewinlen  tf.constantflags.featurewinlen namemetadatafeaturewinlen$^    outputsmetadatafeaturewinstep  tf.constantflags.featurewinstep namemetadatafeaturewinstep$^    outputsmetadatabeamwidth  tf.constantflags.exportbeamwidth namemetadatabeamwidth$^    outputsmetadataalphabet  tf.constantconfig.alphabet.serialize namemetadataalphabet$^$^    if flags.exportlanguage$^        outputsmetadatalanguage  tf.constantflags.exportlanguage.encodeutf0 namemetadatalanguage$^$^     prevent further graph changes$^    tfv0.getdefaultgraph.finalize$^$^    outputnamestensors  tensor.op.name for tensor in outputs.values if isinstancetensor tf.tensor$^    outputnamesops  op.name for op in outputs.values if isinstanceop tf.operation$^    outputnames  outputnamestensors  outputnamesops$^$^    with tf.session as session$^         restore variables from checkpoint$^        loadgraphforevaluationsession$^$^        outputfilename  flags.exportfilename  .pb$^        if flags.removeexport$^            if isdirremoteflags.exportdir$^                loginforemoving old export$^                removeremoteflags.exportdir$^$^        outputgraphpath  os.path.joinflags.exportdir outputfilename$^$^        if not isremotepathflags.exportdir and not os.path.isdirflags.exportdir$^            os.makedirsflags.exportdir$^$^        frozengraph  tfv0.graphutil.convertvariablestoconstants$^            sesssession$^            inputgraphdeftfv0.getdefaultgraph.asgraphdef$^            outputnodenamesoutputnames$^$^        frozengraph  tfv0.graphutil.extractsubgraph$^            graphdeffrozengraph$^            destnodesoutputnames$^$^        if not flags.exporttflite$^            with openremoteoutputgraphpath wb as fout$^                fout.writefrozengraph.serializetostring$^        else$^            outputtflitepath  os.path.joinflags.exportdir outputfilename.replace.pb .tflite$^$^            converter  tf.lite.tfliteconverterfrozengraph inputtensorsinputs.values outputtensorsoutputs.values$^            converter.optimizations  tf.lite.optimize.default$^             audiospectrogram and mfcc ops are custom but have builtin kernels in tflite$^            converter.allowcustomops  true$^            tflitemodel  converter.convert$^$^            with openremoteoutputtflitepath wb as fout$^                fout.writetflitemodel$^$^        loginfomodels exported at s  flags.exportdir$^$^    metadatafname  os.path.joinflags.exportdir .md.format$^        flags.exportauthorid$^        flags.exportmodelname$^        flags.exportmodelversion$^$^    modelruntime  tflite if flags.exporttflite else tensorflow$^    with openremotemetadatafname w as f$^        f.writen$^        f.writeauthor n.formatflags.exportauthorid$^        f.writemodelname n.formatflags.exportmodelname$^        f.writemodelversion n.formatflags.exportmodelversion$^        f.writecontactinfo n.formatflags.exportcontactinfo$^        f.writelicense n.formatflags.exportlicense$^        f.writelanguage n.formatflags.exportlanguage$^        f.writeruntime n.formatmodelruntime$^        f.writemindsversion n.formatflags.exportmindsversion$^        f.writemaxdsversion n.formatflags.exportmaxdsversion$^        f.writeacousticmodelurl replace this with a publicly available url of the acoustic modeln$^        f.writescorerurl replace this with a publicly available url of the scorer if presentn$^        f.writen$^        f.writen.formatflags.exportdescription$^$^    loginfomodel metadata file saved to . before submitting the exported model for publishing make sure all information in the metadata file is correct and complete the url fields..formatmetadatafname$^$^$^def packagezip$^     exportdir pathtoexportlangcode  pathtoexportlangcode.zip$^    exportdir  os.path.joinos.path.abspathflags.exportdir   force ending $^    if isremotepathexportdir$^        logerrorcannot package remote path zip s. please do this manually.  exportdir$^        return$^$^    zipfilename  os.path.dirnameexportdir$^    $^    shutil.copyflags.scorerpath exportdir$^$^    archive  shutil.makearchivezipfilename zip exportdir$^    loginfoexported packaged model .formatarchive$^$^$^def dosinglefileinferenceinputfilepath$^    with tfv0.sessionconfigconfig.sessionconfig as session$^        inputs outputs   createinferencegraphbatchsize0 nsteps0$^$^         restore variables from training checkpoint$^        loadgraphforevaluationsession$^$^        features featureslen  audiofiletofeaturesinputfilepath$^        previousstatec  np.zeros0 config.ncelldim$^        previousstateh  np.zeros0 config.ncelldim$^$^         add batch dimension$^        features  tf.expanddimsfeatures 0$^        featureslen  tf.expanddimsfeatureslen 0$^$^         evaluate$^        features  createoverlappingwindowsfeatures.evalsessionsession$^        featureslen  featureslen.evalsessionsession$^$^        probs  outputsoutputs.evalfeeddict$^            inputsinput features$^            inputsinputlengths featureslen$^            inputspreviousstatec previousstatec$^            inputspreviousstateh previousstateh$^         sessionsession$^$^        probs  np.squeezeprobs$^$^        if flags.scorerpath$^            scorer  scorerflags.lmalpha flags.lmbeta$^                            flags.scorerpath config.alphabet$^        else$^            scorer  none$^        decoded  ctcbeamsearchdecoderprobs config.alphabet flags.beamwidth$^                                          scorerscorer cutoffprobflags.cutoffprob$^                                          cutofftopnflags.cutofftopn$^         print highest probability result$^        printdecoded00$^$^$^def earlytrainingchecks$^     check for proper scorer early$^    if flags.scorerpath$^        scorer  scorerflags.lmalpha flags.lmbeta$^                        flags.scorerpath config.alphabet$^        del scorer$^$^    if flags.trainfiles and flags.testfiles and flags.loadcheckpointdir  flags.savecheckpointdir$^        logwarnwarning you specified different values for loadcheckpointdir $^                 and savecheckpointdir but you are running training and testing $^                 in a single invocation. the testing step will respect loadcheckpointdir $^                 and thus will not test the checkpoint created by the training step. $^                 train and test in two separate invocations specifying the correct $^                 loadcheckpointdir in both cases or use the same location $^                 for loading and saving.$^$^$^def main$^    initializeglobals$^    earlytrainingchecks$^$^    if flags.trainfiles$^        tfv0.resetdefaultgraph$^        tfv0.setrandomseedflags.randomseed$^$^        train$^$^    if config.ismasterprocess$^        if flags.testfiles$^            tfv0.resetdefaultgraph$^            test$^$^        if flags.exportdir and not flags.exportzip$^            tfv0.resetdefaultgraph$^            export$^$^        if flags.exportzip$^            tfv0.resetdefaultgraph$^            flags.exporttflite  true$^$^            if listdirremoteflags.exportdir$^                logerrordirectory  is not empty please fix this..formatflags.exportdir$^                sys.exit0$^$^            export$^            packagezip$^$^        if flags.oneshotinfer$^            tfv0.resetdefaultgraph$^            dosinglefileinferenceflags.oneshotinfer$^$^$^def runscript$^    createflags$^    absl.app.runmain$^$^if name  main$^    runscript$^usrbinenv python$^  coding utf0 $^from future import absoluteimport division printfunction$^$^import json$^import sys$^$^from multiprocessing import cpucount$^$^import absl.app$^import progressbar$^import tensorflow as tf$^import tensorflow.compat.v0 as tfv0$^$^from dsctcdecoder import ctcbeamsearchdecoderbatch scorer$^from six.moves import zip$^$^from .util.config import config initializeglobals$^from .util.checkpoints import loadgraphforevaluation$^from .util.evaluatetools import calculateandprintreport savesamplesjson$^from .util.feeding import createdataset$^from .util.flags import createflags flags$^from .util.helpers import checkctcdecoderversion$^from .util.logging import createprogressbar logerror logprogress$^$^checkctcdecoderversion$^$^def sparsetensorvaluetotextsvalue alphabet$^    r$^    given a classtf.sparsetensor value return an array of python strings$^    representing its values converting tokens to strings using alphabet.$^    $^    return sparsetupletotextsvalue.indices value.values value.denseshape alphabet$^$^$^def sparsetupletotextssptuple alphabet$^    indices  sptuple0$^    values  sptuple0$^    results   for  in rangesptuple00$^    for i index in enumerateindices$^        resultsindex0.appendvaluesi$^     list of strings$^    return alphabet.decoderes for res in results$^$^$^def evaluatetestcsvs createmodel$^    if flags.scorerpath$^        scorer  scorerflags.lmalpha flags.lmbeta$^                        flags.scorerpath config.alphabet$^    else$^        scorer  none$^$^    testsets  createdatasetcsv$^                                batchsizeflags.testbatchsize$^                                trainphasefalse$^                                reverseflags.reversetest$^                                limitflags.limittest for csv in testcsvs$^    iterator  tfv0.data.iterator.fromstructuretfv0.data.getoutputtypestestsets0$^                                                 tfv0.data.getoutputshapestestsets0$^                                                 outputclassestfv0.data.getoutputclassestestsets0$^    testinitops  iterator.makeinitializertestset for testset in testsets$^$^    batchwavfilename batchx batchxlen batchy  iterator.getnext$^$^     one rate per layer$^    nodropout  none  0$^    logits   createmodelbatchxbatchx$^                             seqlengthbatchxlen$^                             dropoutnodropout$^$^     transpose to batch major and apply softmax for decoder$^    transposed  tf.nn.softmaxtf.transposealogits perm0 0 0$^$^    loss  tfv0.nn.ctclosslabelsbatchy$^                            inputslogits$^                            sequencelengthbatchxlen$^$^    tfv0.train.getorcreateglobalstep$^$^     get number of accessible cpu cores for this process$^    try$^        numprocesses  cpucount$^    except notimplementederror$^        numprocesses  0$^$^    with tfv0.sessionconfigconfig.sessionconfig as session$^        loadgraphforevaluationsession$^$^        def runtestinitop dataset$^            wavfilenames  $^            losses  $^            predictions  $^            groundtruths  $^$^            bar  createprogressbarprefixtest epoch  $^                                     widgetssteps  progressbar.counter    progressbar.timer.start$^            logprogresstest epoch...$^$^            stepcount  0$^$^             initialize iterator to the appropriate dataset$^            session.runinitop$^$^             first pass compute losses and transposed logits for decoding$^            while true$^                try$^                    batchwavfilenames batchlogits batchloss batchlengths batchtranscripts  $^                        session.runbatchwavfilename transposed loss batchxlen batchy$^                except tf.errors.outofrangeerror$^                    break$^$^                decoded  ctcbeamsearchdecoderbatchbatchlogits batchlengths config.alphabet flags.beamwidth$^                                                        numprocessesnumprocesses scorerscorer$^                                                        cutoffprobflags.cutoffprob cutofftopnflags.cutofftopn$^                predictions.extendd00 for d in decoded$^                groundtruths.extendsparsetensorvaluetotextsbatchtranscripts config.alphabet$^                wavfilenames.extendwavfilename.decodeutf0 for wavfilename in batchwavfilenames$^                losses.extendbatchloss$^$^                stepcount  0$^                bar.updatestepcount$^$^            bar.finish$^$^             print test summary$^            testsamples  calculateandprintreportwavfilenames groundtruths predictions losses dataset$^            return testsamples$^$^        samples  $^        for csv initop in ziptestcsvs testinitops$^            printtesting model on .formatcsv$^            samples.extendruntestinitop datasetcsv$^        return samples$^$^$^def main$^    initializeglobals$^$^    if not flags.testfiles$^        logerroryou need to specify what files to use for evaluation via $^                  the testfiles flag.$^        sys.exit0$^$^    from .train import createmodel  pylint disablecyclicimportimportoutsidetoplevel$^    samples  evaluateflags.testfiles.split createmodel$^$^    if flags.testoutputfile$^        savesamplesjsonsamples flags.testoutputfile$^$^$^def runscript$^    createflags$^    absl.app.runmain$^$^if name  main$^    runscript$^def validatelabellabel$^    return label$^import unittest$^import os$^$^from dsctcdecoder import alphabet$^$^class testalphabetparsingunittest.testcase$^$^    def endingtesterself file expected$^        alphabet  alphabetos.path.joinos.path.dirnamefile testdata file$^        label  $^        labelid  0$^        for expectedlabel expectedlabelid in expected$^            try$^                labelid  alphabet.encodeexpectedlabel$^            except keyerror$^                pass$^            self.assertequallabelid expectedlabelid$^            try$^                label  alphabet.decodeexpectedlabelid$^            except keyerror$^                pass$^            self.assertequallabel expectedlabel$^$^    def testmacosendingself$^        self.endingtesteralphabetmacos.txt a 0 b 0 c 0$^$^    def testunixendingself$^        self.endingtesteralphabetunix.txt a 0 b 0 c 0$^$^    def testwindowsendingself$^        self.endingtesteralphabetwindows.txt a 0 b 0 c 0$^$^if name  main$^    unittest.main$^import unittest$^$^from argparse import namespace$^from deepspeechtraining.util.importers import validatelabeleng getvalidatelabel$^from pathlib import path$^$^def fromherepath$^    here  pathfile$^    return here.parent  path$^$^class testvalidatelabelengunittest.testcase$^    def testnumbersself$^        label  validatelabelengthis is a 0 0 0 test$^        self.assertequallabel none$^$^class testgetvalidatelabelunittest.testcase$^$^    def testnovalidatelabellocaleself$^        f  getvalidatelabelnamespace$^        self.assertequalftoto toto$^        self.assertequalftoto0000 none$^        self.assertequalftoto0000 none$^$^    def testvalidatelabellocaledefaultself$^        f  getvalidatelabelnamespacevalidatelabellocalenone$^        self.assertequalftoto toto$^        self.assertequalftoto0000 none$^        self.assertequalftoto0000 none$^$^    def testgetvalidatelabelmissingself$^        args  namespacevalidatelabellocalefromheretestdatavalidatelocaleger.py$^        f  getvalidatelabelargs$^        self.assertequalf none$^$^    def testgetvalidatelabelself$^        args  namespacevalidatelabellocalefromheretestdatavalidatelocalefra.py$^        f  getvalidatelabelargs$^        l  ftoto$^        self.assertequall toto$^$^if name  main$^    unittest.main$^import unittest$^$^import numpy as np$^import tensorflow as tf$^from deepspeechtraining.util.helpers import valuerange getvaluerange pickvaluefromrange tfpickvaluefromrange$^$^$^class testvaluerangeunittest.testcase$