### Notes - 11 5 2025

Found a bug in my vocabulary code, which was not taking into account the space, tab and newline characters in the trigrams. Fixed it, and generated new models, which gave the following perplexity scores:

Performance on ChatGPT validation file
Perplexity of ChatGPT: 9.120985526451351
Perplexity of Gemini: 10.279815254486964
Perplexity of Grok: 10.346589344051093
Perplexity of Claude: 10.199665737641741
Performance on Claude validation file
Perplexity of ChatGPT: 10.065476812035575
Perplexity of Gemini: 10.569816717839359
Perplexity of Grok: 10.85314378677784
Perplexity of Claude: 9.822037464644081
Performance on Gemini validation file
Perplexity of ChatGPT: 9.687669980998237
Perplexity of Gemini: 8.590576147674355
Perplexity of Grok: 10.068534720617933
Perplexity of Claude: 9.822037464644081
Performance on Grok validation file
Perplexity of ChatGPT: 10.19774387909299
Perplexity of Gemini: 10.675324820680549
Perplexity of Grok: 9.504675492711842
Perplexity of Claude: 10.806285485830694
Performance on python validation file
Perplexity of python: 8.698546416948448
Perplexity of c++: 13.609083666629587
Performance on c++ validation file
Perplexity of python: 34.07022638299916
Perplexity of c++: 3.16357106643783

Improves it all across the board. In terms of the code models, they become more discriminatory.

### Notes - 8 5 2025

Updated the code to work for English generated by different kinds of LLMs. I suspected that the character based distributions would be quite minor given that they are all English, and this was indeed the case. Below is the validation statistics of the different models tested against english written cannonically:

python generate_writing_models.py
Performance on ChatGPT validation file
Perplexity of ChatGPT: 11.055560598919794
Perplexity of Gemini: 12.924300277891188
Perplexity of Grok: 12.709118292256283
Perplexity of Claude: 12.661590929175004
Performance on Claude validation file
Perplexity of ChatGPT: 12.705266902906303
Perplexity of Gemini: 13.391505257812353
Perplexity of Grok: 13.837269362392481
Perplexity of Claude: 11.962432964838104
Performance on Gemini validation file
Perplexity of ChatGPT: 11.526522064555568
Perplexity of Gemini: 9.874294986541909
Perplexity of Grok: 12.243267723311657
Perplexity of Claude: 11.962432964838104
Performance on Grok validation file
Perplexity of ChatGPT: 13.385726072663338
Perplexity of Gemini: 13.584089850754841
Perplexity of Grok: 11.704773496991669
Perplexity of Claude: 13.845430428137103

It seems like Gemini is the most distinguishable, but unclear why this is the case just yet. I'd have to also compare the actual distributions of the characters to get a better view of what's going on.

### Notes - 24 12 2024

Removing the preprocessing lines (which currently strip out non-alphanumerics) should help improve the recognition of languages, since there's more patterns learned by the model.

Testing the perplexity of the python and c++ models on python validation file <br>
Performance on python validation file<br>
Perplexity of python: 6.14593899640423<br>
Perplexity of c++: 8.967401846037356<br>

Performance on c++ validation file<br>
Perplexity of python: 11.975193045555073<br>
Perplexity of c++: 3.287188133639928<br>

Compared to the previous method of tokenization, it seems that the discriminatory power of these models has worsened (as the scores are closer together now). This method is obviously poor, since it doesn't include the correct adjustments to the smoothing techniques included.

### Notes - 05 12 2024

Today I updated the code to train, preprocess and build models based on new training data. The first project I used was DeepSpeech repo from Mozilla opensource, since it had a large portion of c++ and python code of high quality (it had > 2K github stars, which as a proxy is at best one of quality, and at worst the number of eyes on this project). <br>

Without modifying the logic of preprocessing, I got the following results for perplexity:<br>

Testing the perplexity of the python and c++ models on python validation file <br>
(base) ➜  kite git:(main) ✗ python custom_model.py <br>
Perplexity of python: 7.346046011249762<br>
Perplexity of c++: 10.286613095191875<br>

Testing the perplexity of the python and c++ models on cpp validation file<br>
(base) ➜  kite git:(main) ✗ python custom_model.py<br>
Perplexity of python: 15.02144925139808<br>
Perplexity of c++: 3.8882049085060895<br>

The models managed to get the predictions right, despite the preprocessing being quite simple and suboptimal. The discriminativeness of the C++ model was impressive, and in comparison to the python model could be due to the larger amount of training data